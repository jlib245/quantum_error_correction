Path to model/logs: Final_Results_QECCT/surface/FFNN_Code_L_3/noise_model_independent/repetition_1/11_11_2025_23_02_16
Namespace(epochs=50, workers=4, lr=0.01, gpus='0', batch_size=128, test_batch_size=512, seed=42, code_type='surface', code_L=3, repetitions=1, noise_type='independent', y_ratio=0.3, N_dec=6, d_model=128, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x7ec6e75de9b0>, path='Final_Results_QECCT/surface/FFNN_Code_L_3/noise_model_independent/repetition_1/11_11_2025_23_02_16')
사용 가능한 GPU가 없어 CPU를 사용합니다.
PC matrix shape torch.Size([8, 18])
ECC_FFNN(
  (fc1): Linear(in_features=8, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=4, bias=True)
  (activation): Sigmoid()
  (criterion): CrossEntropyLoss()
)
# of Parameters: 1668
Training epoch 1, Batch 500/1000: LR=1.00e-02, Loss=6.87037e-01 LER=2.111e-01
***Loss=6.87037e-01
Training epoch 1, Batch 1000/1000: LR=1.00e-02, Loss=5.99168e-01 LER=1.936e-01
***Loss=5.99168e-01
Epoch 1 Train Time 9.333811521530151s

Model Saved
Training epoch 2, Batch 500/1000: LR=9.99e-03, Loss=4.65611e-01 LER=1.681e-01
***Loss=4.65611e-01
Training epoch 2, Batch 1000/1000: LR=9.99e-03, Loss=4.56617e-01 LER=1.627e-01
***Loss=4.56617e-01
Epoch 2 Train Time 7.325969219207764s

Model Saved
Training epoch 3, Batch 500/1000: LR=9.96e-03, Loss=4.39856e-01 LER=1.541e-01
***Loss=4.39856e-01
Training epoch 3, Batch 1000/1000: LR=9.96e-03, Loss=4.38779e-01 LER=1.527e-01
***Loss=4.38779e-01
Epoch 3 Train Time 7.568370819091797s

Model Saved
Training epoch 4, Batch 500/1000: LR=9.91e-03, Loss=4.30966e-01 LER=1.498e-01
***Loss=4.30966e-01
Training epoch 4, Batch 1000/1000: LR=9.91e-03, Loss=4.29486e-01 LER=1.501e-01
***Loss=4.29486e-01
Epoch 4 Train Time 8.100754976272583s

Model Saved
Training epoch 5, Batch 500/1000: LR=9.84e-03, Loss=4.32384e-01 LER=1.516e-01
***Loss=4.32384e-01
Training epoch 5, Batch 1000/1000: LR=9.84e-03, Loss=4.31037e-01 LER=1.512e-01
***Loss=4.31037e-01
Epoch 5 Train Time 8.219572067260742s

Training epoch 6, Batch 500/1000: LR=9.76e-03, Loss=4.31185e-01 LER=1.502e-01
***Loss=4.31185e-01
Training epoch 6, Batch 1000/1000: LR=9.76e-03, Loss=4.31834e-01 LER=1.508e-01
***Loss=4.31834e-01
Epoch 6 Train Time 8.503048658370972s

Training epoch 7, Batch 500/1000: LR=9.65e-03, Loss=4.23302e-01 LER=1.483e-01
***Loss=4.23302e-01
Training epoch 7, Batch 1000/1000: LR=9.65e-03, Loss=4.28770e-01 LER=1.505e-01
***Loss=4.28770e-01
Epoch 7 Train Time 9.202320575714111s

Model Saved
Training epoch 8, Batch 500/1000: LR=9.52e-03, Loss=4.24874e-01 LER=1.484e-01
***Loss=4.24874e-01
Training epoch 8, Batch 1000/1000: LR=9.52e-03, Loss=4.25693e-01 LER=1.482e-01
***Loss=4.25693e-01
Epoch 8 Train Time 11.221519231796265s

Model Saved
Training epoch 9, Batch 500/1000: LR=9.38e-03, Loss=4.24304e-01 LER=1.482e-01
***Loss=4.24304e-01
Training epoch 9, Batch 1000/1000: LR=9.38e-03, Loss=4.23421e-01 LER=1.474e-01
***Loss=4.23421e-01
Epoch 9 Train Time 10.755107879638672s

Model Saved
Training epoch 10, Batch 500/1000: LR=9.22e-03, Loss=4.23721e-01 LER=1.469e-01
***Loss=4.23721e-01
Training epoch 10, Batch 1000/1000: LR=9.22e-03, Loss=4.21495e-01 LER=1.466e-01
***Loss=4.21495e-01
Epoch 10 Train Time 12.480277299880981s

Model Saved
Test LER  p=7.00e-02: 1.13e-01 p=8.00e-02: 1.31e-01 p=9.00e-02: 1.48e-01 p=1.00e-01: 1.67e-01 p=1.10e-01: 1.86e-01
Mean LER = 1.489e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 105.02178907394409 s

Training epoch 11, Batch 500/1000: LR=9.05e-03, Loss=4.23319e-01 LER=1.478e-01
***Loss=4.23319e-01
Training epoch 11, Batch 1000/1000: LR=9.05e-03, Loss=4.28621e-01 LER=1.501e-01
***Loss=4.28621e-01
Epoch 11 Train Time 8.50406265258789s

Training epoch 12, Batch 500/1000: LR=8.85e-03, Loss=4.24009e-01 LER=1.485e-01
***Loss=4.24009e-01
Training epoch 12, Batch 1000/1000: LR=8.85e-03, Loss=4.21635e-01 LER=1.483e-01
***Loss=4.21635e-01
Epoch 12 Train Time 11.093557834625244s

Training epoch 13, Batch 500/1000: LR=8.64e-03, Loss=4.20636e-01 LER=1.482e-01
***Loss=4.20636e-01
Training epoch 13, Batch 1000/1000: LR=8.64e-03, Loss=4.21501e-01 LER=1.476e-01
***Loss=4.21501e-01
Epoch 13 Train Time 11.57279658317566s

Training epoch 14, Batch 500/1000: LR=8.42e-03, Loss=4.23142e-01 LER=1.480e-01
***Loss=4.23142e-01
Training epoch 14, Batch 1000/1000: LR=8.42e-03, Loss=4.20715e-01 LER=1.471e-01
***Loss=4.20715e-01
Epoch 14 Train Time 9.815873861312866s

Model Saved
Training epoch 15, Batch 500/1000: LR=8.19e-03, Loss=4.22565e-01 LER=1.475e-01
***Loss=4.22565e-01
Training epoch 15, Batch 1000/1000: LR=8.19e-03, Loss=4.23807e-01 LER=1.484e-01
***Loss=4.23807e-01
Epoch 15 Train Time 11.554335594177246s

Training epoch 16, Batch 500/1000: LR=7.94e-03, Loss=4.29918e-01 LER=1.497e-01
***Loss=4.29918e-01
Training epoch 16, Batch 1000/1000: LR=7.94e-03, Loss=4.25810e-01 LER=1.498e-01
***Loss=4.25810e-01
Epoch 16 Train Time 9.664429903030396s

Training epoch 17, Batch 500/1000: LR=7.68e-03, Loss=4.26639e-01 LER=1.475e-01
***Loss=4.26639e-01
Training epoch 17, Batch 1000/1000: LR=7.68e-03, Loss=4.23860e-01 LER=1.469e-01
***Loss=4.23860e-01
Epoch 17 Train Time 9.082536935806274s

Training epoch 18, Batch 500/1000: LR=7.41e-03, Loss=4.20817e-01 LER=1.469e-01
***Loss=4.20817e-01
Training epoch 18, Batch 1000/1000: LR=7.41e-03, Loss=4.22259e-01 LER=1.475e-01
***Loss=4.22259e-01
Epoch 18 Train Time 9.508402585983276s

Training epoch 19, Batch 500/1000: LR=7.13e-03, Loss=4.20822e-01 LER=1.469e-01
***Loss=4.20822e-01
Training epoch 19, Batch 1000/1000: LR=7.13e-03, Loss=4.20969e-01 LER=1.465e-01
***Loss=4.20969e-01
Epoch 19 Train Time 10.296874761581421s

Training epoch 20, Batch 500/1000: LR=6.84e-03, Loss=4.21523e-01 LER=1.471e-01
***Loss=4.21523e-01
Training epoch 20, Batch 1000/1000: LR=6.84e-03, Loss=4.22503e-01 LER=1.480e-01
***Loss=4.22503e-01
Epoch 20 Train Time 9.073855638504028s

Test LER  p=7.00e-02: 1.13e-01 p=8.00e-02: 1.28e-01 p=9.00e-02: 1.50e-01 p=1.00e-01: 1.66e-01 p=1.10e-01: 1.82e-01
Mean LER = 1.478e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 105.50310826301575 s

Training epoch 21, Batch 500/1000: LR=6.55e-03, Loss=4.20567e-01 LER=1.480e-01
***Loss=4.20567e-01
Training epoch 21, Batch 1000/1000: LR=6.55e-03, Loss=4.19737e-01 LER=1.476e-01
***Loss=4.19737e-01
Epoch 21 Train Time 9.584258317947388s

Model Saved
Training epoch 22, Batch 500/1000: LR=6.24e-03, Loss=4.22796e-01 LER=1.473e-01
***Loss=4.22796e-01
Training epoch 22, Batch 1000/1000: LR=6.24e-03, Loss=4.23251e-01 LER=1.480e-01
***Loss=4.23251e-01
Epoch 22 Train Time 12.711978912353516s

Training epoch 23, Batch 500/1000: LR=5.94e-03, Loss=4.18830e-01 LER=1.462e-01
***Loss=4.18830e-01
Training epoch 23, Batch 1000/1000: LR=5.94e-03, Loss=4.17765e-01 LER=1.461e-01
***Loss=4.17765e-01
Epoch 23 Train Time 9.930797100067139s

Model Saved
Training epoch 24, Batch 500/1000: LR=5.63e-03, Loss=4.17520e-01 LER=1.460e-01
***Loss=4.17520e-01
Training epoch 24, Batch 1000/1000: LR=5.63e-03, Loss=4.17733e-01 LER=1.458e-01
***Loss=4.17733e-01
Epoch 24 Train Time 9.014538764953613s

Model Saved
Training epoch 25, Batch 500/1000: LR=5.31e-03, Loss=4.26021e-01 LER=1.484e-01
***Loss=4.26021e-01
Training epoch 25, Batch 1000/1000: LR=5.31e-03, Loss=4.22576e-01 LER=1.470e-01
***Loss=4.22576e-01
Epoch 25 Train Time 9.61708378791809s

Training epoch 26, Batch 500/1000: LR=5.00e-03, Loss=4.21661e-01 LER=1.480e-01
***Loss=4.21661e-01
Training epoch 26, Batch 1000/1000: LR=5.00e-03, Loss=4.21086e-01 LER=1.482e-01
***Loss=4.21086e-01
Epoch 26 Train Time 10.410109758377075s

Training epoch 27, Batch 500/1000: LR=4.69e-03, Loss=4.15738e-01 LER=1.464e-01
***Loss=4.15738e-01
Training epoch 27, Batch 1000/1000: LR=4.69e-03, Loss=4.16551e-01 LER=1.460e-01
***Loss=4.16551e-01
Epoch 27 Train Time 9.20374059677124s

Model Saved
Training epoch 28, Batch 500/1000: LR=4.37e-03, Loss=4.22542e-01 LER=1.474e-01
***Loss=4.22542e-01
Training epoch 28, Batch 1000/1000: LR=4.37e-03, Loss=4.22006e-01 LER=1.478e-01
***Loss=4.22006e-01
Epoch 28 Train Time 8.983741998672485s

Training epoch 29, Batch 500/1000: LR=4.06e-03, Loss=4.15876e-01 LER=1.449e-01
***Loss=4.15876e-01
Training epoch 29, Batch 1000/1000: LR=4.06e-03, Loss=4.15901e-01 LER=1.451e-01
***Loss=4.15901e-01
Epoch 29 Train Time 8.988193035125732s

Model Saved
Training epoch 30, Batch 500/1000: LR=3.76e-03, Loss=4.22807e-01 LER=1.479e-01
***Loss=4.22807e-01
Training epoch 30, Batch 1000/1000: LR=3.76e-03, Loss=4.22935e-01 LER=1.480e-01
***Loss=4.22935e-01
Epoch 30 Train Time 9.305567264556885s

Test LER  p=7.00e-02: 1.13e-01 p=8.00e-02: 1.29e-01 p=9.00e-02: 1.48e-01 p=1.00e-01: 1.67e-01 p=1.10e-01: 1.85e-01
Mean LER = 1.485e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 112.11600017547607 s

Training epoch 31, Batch 500/1000: LR=3.46e-03, Loss=4.16137e-01 LER=1.452e-01
***Loss=4.16137e-01
Training epoch 31, Batch 1000/1000: LR=3.46e-03, Loss=4.17623e-01 LER=1.468e-01
***Loss=4.17623e-01
Epoch 31 Train Time 12.554460048675537s

Training epoch 32, Batch 500/1000: LR=3.16e-03, Loss=4.17549e-01 LER=1.474e-01
***Loss=4.17549e-01
Training epoch 32, Batch 1000/1000: LR=3.16e-03, Loss=4.17987e-01 LER=1.468e-01
***Loss=4.17987e-01
Epoch 32 Train Time 10.505390405654907s

Training epoch 33, Batch 500/1000: LR=2.87e-03, Loss=4.16385e-01 LER=1.458e-01
***Loss=4.16385e-01
Training epoch 33, Batch 1000/1000: LR=2.87e-03, Loss=4.18116e-01 LER=1.462e-01
***Loss=4.18116e-01
Epoch 33 Train Time 11.592930555343628s

Training epoch 34, Batch 500/1000: LR=2.59e-03, Loss=4.17082e-01 LER=1.457e-01
***Loss=4.17082e-01
Training epoch 34, Batch 1000/1000: LR=2.59e-03, Loss=4.20485e-01 LER=1.471e-01
***Loss=4.20485e-01
Epoch 34 Train Time 9.966078519821167s

Training epoch 35, Batch 500/1000: LR=2.32e-03, Loss=4.16363e-01 LER=1.462e-01
***Loss=4.16363e-01
Training epoch 35, Batch 1000/1000: LR=2.32e-03, Loss=4.18797e-01 LER=1.472e-01
***Loss=4.18797e-01
Epoch 35 Train Time 9.142289400100708s

Training epoch 36, Batch 500/1000: LR=2.06e-03, Loss=4.16667e-01 LER=1.454e-01
***Loss=4.16667e-01
Training epoch 36, Batch 1000/1000: LR=2.06e-03, Loss=4.18445e-01 LER=1.467e-01
***Loss=4.18445e-01
Epoch 36 Train Time 9.787384271621704s

Training epoch 37, Batch 500/1000: LR=1.81e-03, Loss=4.19773e-01 LER=1.477e-01
***Loss=4.19773e-01
Training epoch 37, Batch 1000/1000: LR=1.81e-03, Loss=4.19960e-01 LER=1.473e-01
***Loss=4.19960e-01
Epoch 37 Train Time 9.68656849861145s

Training epoch 38, Batch 500/1000: LR=1.58e-03, Loss=4.15167e-01 LER=1.443e-01
***Loss=4.15167e-01
Training epoch 38, Batch 1000/1000: LR=1.58e-03, Loss=4.19367e-01 LER=1.467e-01
***Loss=4.19367e-01
Epoch 38 Train Time 9.249903440475464s

Training epoch 39, Batch 500/1000: LR=1.36e-03, Loss=4.13372e-01 LER=1.447e-01
***Loss=4.13372e-01
Training epoch 39, Batch 1000/1000: LR=1.36e-03, Loss=4.15825e-01 LER=1.465e-01
***Loss=4.15825e-01
Epoch 39 Train Time 9.375515937805176s

Model Saved
Training epoch 40, Batch 500/1000: LR=1.15e-03, Loss=4.17075e-01 LER=1.460e-01
***Loss=4.17075e-01
Training epoch 40, Batch 1000/1000: LR=1.15e-03, Loss=4.16805e-01 LER=1.465e-01
***Loss=4.16805e-01
Epoch 40 Train Time 10.643076419830322s

Test LER  p=7.00e-02: 1.11e-01 p=8.00e-02: 1.30e-01 p=9.00e-02: 1.48e-01 p=1.00e-01: 1.62e-01 p=1.10e-01: 1.83e-01
Mean LER = 1.466e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 119.39695644378662 s

Training epoch 41, Batch 500/1000: LR=9.56e-04, Loss=4.18853e-01 LER=1.463e-01
***Loss=4.18853e-01
Training epoch 41, Batch 1000/1000: LR=9.56e-04, Loss=4.19155e-01 LER=1.466e-01
***Loss=4.19155e-01
Epoch 41 Train Time 11.405107736587524s

Training epoch 42, Batch 500/1000: LR=7.79e-04, Loss=4.20190e-01 LER=1.483e-01
***Loss=4.20190e-01
Training epoch 42, Batch 1000/1000: LR=7.79e-04, Loss=4.19565e-01 LER=1.474e-01
***Loss=4.19565e-01
Epoch 42 Train Time 12.00142526626587s

Training epoch 43, Batch 500/1000: LR=6.19e-04, Loss=4.15719e-01 LER=1.461e-01
***Loss=4.15719e-01
Training epoch 43, Batch 1000/1000: LR=6.19e-04, Loss=4.17295e-01 LER=1.468e-01
***Loss=4.17295e-01
Epoch 43 Train Time 9.968583583831787s

Training epoch 44, Batch 500/1000: LR=4.77e-04, Loss=4.13512e-01 LER=1.442e-01
***Loss=4.13512e-01
Training epoch 44, Batch 1000/1000: LR=4.77e-04, Loss=4.16451e-01 LER=1.450e-01
***Loss=4.16451e-01
Epoch 44 Train Time 12.393537521362305s

Training epoch 45, Batch 500/1000: LR=3.52e-04, Loss=4.17106e-01 LER=1.457e-01
***Loss=4.17106e-01
Training epoch 45, Batch 1000/1000: LR=3.52e-04, Loss=4.14756e-01 LER=1.456e-01
***Loss=4.14756e-01
Epoch 45 Train Time 10.465368747711182s

Model Saved
Training epoch 46, Batch 500/1000: LR=2.46e-04, Loss=4.10368e-01 LER=1.443e-01
***Loss=4.10368e-01
Training epoch 46, Batch 1000/1000: LR=2.46e-04, Loss=4.14169e-01 LER=1.446e-01
***Loss=4.14169e-01
Epoch 46 Train Time 9.845561742782593s

Model Saved
Training epoch 47, Batch 500/1000: LR=1.58e-04, Loss=4.17391e-01 LER=1.470e-01
***Loss=4.17391e-01
Training epoch 47, Batch 1000/1000: LR=1.58e-04, Loss=4.18065e-01 LER=1.468e-01
***Loss=4.18065e-01
Epoch 47 Train Time 9.972182989120483s

Training epoch 48, Batch 500/1000: LR=8.96e-05, Loss=4.19187e-01 LER=1.475e-01
***Loss=4.19187e-01
Training epoch 48, Batch 1000/1000: LR=8.96e-05, Loss=4.18847e-01 LER=1.468e-01
***Loss=4.18847e-01
Epoch 48 Train Time 10.017466306686401s

Training epoch 49, Batch 500/1000: LR=4.04e-05, Loss=4.15851e-01 LER=1.465e-01
***Loss=4.15851e-01
Training epoch 49, Batch 1000/1000: LR=4.04e-05, Loss=4.14686e-01 LER=1.455e-01
***Loss=4.14686e-01
Epoch 49 Train Time 9.687735795974731s

Training epoch 50, Batch 500/1000: LR=1.09e-05, Loss=4.13031e-01 LER=1.445e-01
***Loss=4.13031e-01
Training epoch 50, Batch 1000/1000: LR=1.09e-05, Loss=4.14060e-01 LER=1.450e-01
***Loss=4.14060e-01
Epoch 50 Train Time 11.2482271194458s

Model Saved
Test LER  p=7.00e-02: 1.10e-01 p=8.00e-02: 1.27e-01 p=9.00e-02: 1.46e-01 p=1.00e-01: 1.64e-01 p=1.10e-01: 1.81e-01
Mean LER = 1.459e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 113.69259595870972 s

Best model loaded
Test LER  p=7.00e-02: 1.12e-01 p=8.00e-02: 1.29e-01 p=9.00e-02: 1.46e-01 p=1.00e-01: 1.64e-01 p=1.10e-01: 1.81e-01
Mean LER = 1.462e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 114.27618432044983 s

