Path to model/logs: Final_Results_QECCT/surface/Transformer_Code_L_3/noise_model_independent/repetition_1/11_11_2025_23_27_27
Namespace(epochs=200, workers=4, lr=0.01, gpus='0', batch_size=128, test_batch_size=512, seed=42, code_type='surface', code_L=3, repetitions=1, noise_type='independent', y_ratio=0.3, N_dec=6, d_model=128, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x7bbad2c4d570>, path='Final_Results_QECCT/surface/Transformer_Code_L_3/noise_model_independent/repetition_1/11_11_2025_23_27_27')
사용 가능한 GPU가 없어 CPU를 사용합니다.
PC matrix shape torch.Size([8, 18])
ECC_Transformer(
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder): Encoder(
    (layers): ModuleList(
      (0-5): 6 x EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0-1): 2 x SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (input_embedding): Linear(in_features=8, out_features=128, bias=True)
  (output_classifier): Linear(in_features=128, out_features=4, bias=True)
  (criterion): CrossEntropyLoss()
)
# of Parameters: 1191812
Training epoch 1, Batch 500/1000: LR=1.00e-02, Loss=1.19555e+00 LER=4.800e-01
***Loss=1.19555e+00
Training epoch 1, Batch 1000/1000: LR=1.00e-02, Loss=1.09483e+00 LER=4.378e-01
***Loss=1.09483e+00
Epoch 1 Train Time 31.501768827438354s

Model Saved
Training epoch 2, Batch 500/1000: LR=1.00e-02, Loss=6.13524e-01 LER=2.118e-01
***Loss=6.13524e-01
Training epoch 2, Batch 1000/1000: LR=1.00e-02, Loss=5.66808e-01 LER=1.883e-01
***Loss=5.66808e-01
Epoch 2 Train Time 30.232628107070923s

Model Saved
Training epoch 3, Batch 500/1000: LR=1.00e-02, Loss=4.99850e-01 LER=1.620e-01
***Loss=4.99850e-01
Training epoch 3, Batch 1000/1000: LR=1.00e-02, Loss=4.91326e-01 LER=1.603e-01
***Loss=4.91326e-01
Epoch 3 Train Time 31.82432985305786s

Model Saved
Training epoch 4, Batch 500/1000: LR=9.99e-03, Loss=4.89172e-01 LER=1.632e-01
***Loss=4.89172e-01
Training epoch 4, Batch 1000/1000: LR=9.99e-03, Loss=4.90628e-01 LER=1.628e-01
***Loss=4.90628e-01
Epoch 4 Train Time 33.099531173706055s

Model Saved
Training epoch 5, Batch 500/1000: LR=9.99e-03, Loss=4.92368e-01 LER=1.623e-01
***Loss=4.92368e-01
Training epoch 5, Batch 1000/1000: LR=9.99e-03, Loss=4.82936e-01 LER=1.605e-01
***Loss=4.82936e-01
Epoch 5 Train Time 32.23696208000183s

Model Saved
Training epoch 6, Batch 500/1000: LR=9.98e-03, Loss=4.75454e-01 LER=1.592e-01
***Loss=4.75454e-01
Training epoch 6, Batch 1000/1000: LR=9.98e-03, Loss=4.73058e-01 LER=1.601e-01
***Loss=4.73058e-01
Epoch 6 Train Time 33.359352588653564s

Model Saved
Training epoch 7, Batch 500/1000: LR=9.98e-03, Loss=4.62022e-01 LER=1.563e-01
***Loss=4.62022e-01
Training epoch 7, Batch 1000/1000: LR=9.98e-03, Loss=4.60754e-01 LER=1.569e-01
***Loss=4.60754e-01
Epoch 7 Train Time 32.35599374771118s

Model Saved
Training epoch 8, Batch 500/1000: LR=9.97e-03, Loss=4.57774e-01 LER=1.590e-01
***Loss=4.57774e-01
Training epoch 8, Batch 1000/1000: LR=9.97e-03, Loss=4.58803e-01 LER=1.585e-01
***Loss=4.58803e-01
Epoch 8 Train Time 33.53440594673157s

Model Saved
Training epoch 9, Batch 500/1000: LR=9.96e-03, Loss=4.58438e-01 LER=1.579e-01
***Loss=4.58438e-01
Training epoch 9, Batch 1000/1000: LR=9.96e-03, Loss=4.60150e-01 LER=1.588e-01
***Loss=4.60150e-01
Epoch 9 Train Time 32.577170848846436s

Training epoch 10, Batch 500/1000: LR=9.95e-03, Loss=4.56715e-01 LER=1.579e-01
***Loss=4.56715e-01
Training epoch 10, Batch 1000/1000: LR=9.95e-03, Loss=4.55002e-01 LER=1.580e-01
***Loss=4.55002e-01
Epoch 10 Train Time 32.590747356414795s

Model Saved
Test LER  p=7.00e-02: 1.18e-01 p=8.00e-02: 1.36e-01 p=9.00e-02: 1.52e-01 p=1.00e-01: 1.71e-01 p=1.10e-01: 1.90e-01
Mean LER = 1.532e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 78.7962954044342 s

Training epoch 11, Batch 500/1000: LR=9.94e-03, Loss=4.50482e-01 LER=1.544e-01
***Loss=4.50482e-01
Training epoch 11, Batch 1000/1000: LR=9.94e-03, Loss=4.50528e-01 LER=1.547e-01
***Loss=4.50528e-01
Epoch 11 Train Time 33.98369026184082s

Model Saved
Training epoch 12, Batch 500/1000: LR=9.93e-03, Loss=4.50174e-01 LER=1.555e-01
***Loss=4.50174e-01
Training epoch 12, Batch 1000/1000: LR=9.93e-03, Loss=4.47272e-01 LER=1.544e-01
***Loss=4.47272e-01
Epoch 12 Train Time 30.282775163650513s

Model Saved
Training epoch 13, Batch 500/1000: LR=9.91e-03, Loss=4.43639e-01 LER=1.516e-01
***Loss=4.43639e-01
Training epoch 13, Batch 1000/1000: LR=9.91e-03, Loss=4.41118e-01 LER=1.517e-01
***Loss=4.41118e-01
Epoch 13 Train Time 31.43477511405945s

Model Saved
Training epoch 14, Batch 500/1000: LR=9.90e-03, Loss=4.41731e-01 LER=1.515e-01
***Loss=4.41731e-01
Training epoch 14, Batch 1000/1000: LR=9.90e-03, Loss=4.38463e-01 LER=1.508e-01
***Loss=4.38463e-01
Epoch 14 Train Time 31.307103157043457s

Model Saved
Training epoch 15, Batch 500/1000: LR=9.88e-03, Loss=4.45095e-01 LER=1.538e-01
***Loss=4.45095e-01
Training epoch 15, Batch 1000/1000: LR=9.88e-03, Loss=4.40595e-01 LER=1.531e-01
***Loss=4.40595e-01
Epoch 15 Train Time 32.08313250541687s

Training epoch 16, Batch 500/1000: LR=9.86e-03, Loss=4.34619e-01 LER=1.508e-01
***Loss=4.34619e-01
Training epoch 16, Batch 1000/1000: LR=9.86e-03, Loss=4.40202e-01 LER=1.515e-01
***Loss=4.40202e-01
Epoch 16 Train Time 31.19418454170227s

Training epoch 17, Batch 500/1000: LR=9.84e-03, Loss=4.30437e-01 LER=1.504e-01
***Loss=4.30437e-01
Training epoch 17, Batch 1000/1000: LR=9.84e-03, Loss=4.30471e-01 LER=1.493e-01
***Loss=4.30471e-01
Epoch 17 Train Time 33.37403345108032s

Model Saved
Training epoch 18, Batch 500/1000: LR=9.82e-03, Loss=4.31085e-01 LER=1.503e-01
***Loss=4.31085e-01
Training epoch 18, Batch 1000/1000: LR=9.82e-03, Loss=4.33844e-01 LER=1.506e-01
***Loss=4.33844e-01
Epoch 18 Train Time 34.085949659347534s

Training epoch 19, Batch 500/1000: LR=9.80e-03, Loss=4.34596e-01 LER=1.523e-01
***Loss=4.34596e-01
Training epoch 19, Batch 1000/1000: LR=9.80e-03, Loss=4.32367e-01 LER=1.512e-01
***Loss=4.32367e-01
Epoch 19 Train Time 33.394116163253784s

Training epoch 20, Batch 500/1000: LR=9.78e-03, Loss=4.37323e-01 LER=1.525e-01
***Loss=4.37323e-01
Training epoch 20, Batch 1000/1000: LR=9.78e-03, Loss=4.34394e-01 LER=1.515e-01
***Loss=4.34394e-01
Epoch 20 Train Time 31.830413818359375s

Test LER  p=7.00e-02: 1.13e-01 p=8.00e-02: 1.33e-01 p=9.00e-02: 1.50e-01 p=1.00e-01: 1.69e-01 p=1.10e-01: 1.83e-01
Mean LER = 1.494e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 82.4546046257019 s

Training epoch 21, Batch 500/1000: LR=9.76e-03, Loss=4.30636e-01 LER=1.521e-01
***Loss=4.30636e-01
Training epoch 21, Batch 1000/1000: LR=9.76e-03, Loss=4.31847e-01 LER=1.522e-01
***Loss=4.31847e-01
Epoch 21 Train Time 32.55494952201843s

Training epoch 22, Batch 500/1000: LR=9.73e-03, Loss=4.33999e-01 LER=1.509e-01
***Loss=4.33999e-01
Training epoch 22, Batch 1000/1000: LR=9.73e-03, Loss=4.32386e-01 LER=1.501e-01
***Loss=4.32386e-01
Epoch 22 Train Time 34.03172445297241s

Training epoch 23, Batch 500/1000: LR=9.70e-03, Loss=4.27522e-01 LER=1.479e-01
***Loss=4.27522e-01
Training epoch 23, Batch 1000/1000: LR=9.70e-03, Loss=4.26394e-01 LER=1.478e-01
***Loss=4.26394e-01
Epoch 23 Train Time 34.814613819122314s

Model Saved
Training epoch 24, Batch 500/1000: LR=9.68e-03, Loss=4.31172e-01 LER=1.512e-01
***Loss=4.31172e-01
Training epoch 24, Batch 1000/1000: LR=9.68e-03, Loss=4.29620e-01 LER=1.504e-01
***Loss=4.29620e-01
Epoch 24 Train Time 33.57510757446289s

Training epoch 25, Batch 500/1000: LR=9.65e-03, Loss=4.31174e-01 LER=1.502e-01
***Loss=4.31174e-01
Training epoch 25, Batch 1000/1000: LR=9.65e-03, Loss=4.28823e-01 LER=1.493e-01
***Loss=4.28823e-01
Epoch 25 Train Time 33.691702127456665s

Training epoch 26, Batch 500/1000: LR=9.62e-03, Loss=4.29024e-01 LER=1.495e-01
***Loss=4.29024e-01
Training epoch 26, Batch 1000/1000: LR=9.62e-03, Loss=4.29347e-01 LER=1.494e-01
***Loss=4.29347e-01
Epoch 26 Train Time 35.00856804847717s

