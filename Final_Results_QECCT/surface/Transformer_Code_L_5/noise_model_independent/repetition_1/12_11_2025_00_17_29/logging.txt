Path to model/logs: Final_Results_QECCT/surface/Transformer_Code_L_5/noise_model_independent/repetition_1/12_11_2025_00_17_29
Namespace(epochs=200, workers=2, lr=0.001, gpus='0', batch_size=256, test_batch_size=512, seed=42, code_type='surface', code_L=5, repetitions=1, noise_type='independent', y_ratio=0.3, N_dec=6, d_model=128, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x75c2c9fde380>, path='Final_Results_QECCT/surface/Transformer_Code_L_5/noise_model_independent/repetition_1/12_11_2025_00_17_29')
Intel Arc GPU (XPU)를 사용합니다: Intel(R) Graphics [0x5694]
Intel Arc GPU (XPU) - 단일 GPU 모드 (권장: L>=5, batch_size>=512)
PC matrix shape torch.Size([24, 50])
ECC_Transformer(
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder): Encoder(
    (layers): ModuleList(
      (0-5): 6 x EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0-1): 2 x SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (input_embedding): Linear(in_features=24, out_features=128, bias=True)
  (output_classifier): Linear(in_features=128, out_features=4, bias=True)
  (criterion): CrossEntropyLoss()
)
# of Parameters: 1193860
Training epoch 1, Batch 500/1000: LR=1.00e-03, Loss=5.89053e-01 LER=2.551e-01
***Loss=5.89053e-01
Training epoch 1, Batch 1000/1000: LR=1.00e-03, Loss=4.42635e-01 LER=1.821e-01
***Loss=4.42635e-01
Epoch 1 Train Time 58.98998165130615s

Model Saved
Training epoch 2, Batch 500/1000: LR=1.00e-03, Loss=2.58634e-01 LER=9.368e-02
***Loss=2.58634e-01
Training epoch 2, Batch 1000/1000: LR=1.00e-03, Loss=2.49297e-01 LER=9.038e-02
***Loss=2.49297e-01
Epoch 2 Train Time 59.358614444732666s

Model Saved
Training epoch 3, Batch 500/1000: LR=1.00e-03, Loss=2.35857e-01 LER=8.481e-02
***Loss=2.35857e-01
Training epoch 3, Batch 1000/1000: LR=1.00e-03, Loss=2.32854e-01 LER=8.388e-02
***Loss=2.32854e-01
Epoch 3 Train Time 60.60224509239197s

Model Saved
Training epoch 4, Batch 500/1000: LR=9.99e-04, Loss=2.24050e-01 LER=8.127e-02
***Loss=2.24050e-01
Training epoch 4, Batch 1000/1000: LR=9.99e-04, Loss=2.20588e-01 LER=7.928e-02
***Loss=2.20588e-01
Epoch 4 Train Time 69.3401529788971s

Model Saved
Training epoch 5, Batch 500/1000: LR=9.99e-04, Loss=2.17632e-01 LER=7.912e-02
***Loss=2.17632e-01
Training epoch 5, Batch 1000/1000: LR=9.99e-04, Loss=2.15395e-01 LER=7.754e-02
***Loss=2.15395e-01
Epoch 5 Train Time 69.55183005332947s

Model Saved
Training epoch 6, Batch 500/1000: LR=9.98e-04, Loss=2.08952e-01 LER=7.524e-02
***Loss=2.08952e-01
Training epoch 6, Batch 1000/1000: LR=9.98e-04, Loss=2.08101e-01 LER=7.472e-02
***Loss=2.08101e-01
Epoch 6 Train Time 69.72787117958069s

Model Saved
Training epoch 7, Batch 500/1000: LR=9.98e-04, Loss=2.07929e-01 LER=7.468e-02
***Loss=2.07929e-01
Training epoch 7, Batch 1000/1000: LR=9.98e-04, Loss=2.06069e-01 LER=7.406e-02
***Loss=2.06069e-01
Epoch 7 Train Time 65.13001251220703s

Model Saved
Training epoch 8, Batch 500/1000: LR=9.97e-04, Loss=2.02025e-01 LER=7.251e-02
***Loss=2.02025e-01
Training epoch 8, Batch 1000/1000: LR=9.97e-04, Loss=2.02394e-01 LER=7.237e-02
***Loss=2.02394e-01
Epoch 8 Train Time 69.30532431602478s

Model Saved
Training epoch 9, Batch 500/1000: LR=9.96e-04, Loss=1.99831e-01 LER=7.153e-02
***Loss=1.99831e-01
Training epoch 9, Batch 1000/1000: LR=9.96e-04, Loss=1.98345e-01 LER=7.088e-02
***Loss=1.98345e-01
Epoch 9 Train Time 71.1331422328949s

Model Saved
Training epoch 10, Batch 500/1000: LR=9.95e-04, Loss=1.96831e-01 LER=7.051e-02
***Loss=1.96831e-01
Training epoch 10, Batch 1000/1000: LR=9.95e-04, Loss=1.97248e-01 LER=7.040e-02
***Loss=1.97248e-01
Epoch 10 Train Time 64.48891115188599s

Model Saved
Test LER  p=7.00e-02: 3.91e-02 p=8.00e-02: 5.25e-02 p=9.00e-02: 7.03e-02 p=1.00e-01: 8.92e-02 p=1.10e-01: 1.09e-01
Mean LER = 7.209e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 128.75580596923828 s

Training epoch 11, Batch 500/1000: LR=9.94e-04, Loss=1.95758e-01 LER=7.018e-02
***Loss=1.95758e-01
Training epoch 11, Batch 1000/1000: LR=9.94e-04, Loss=1.94218e-01 LER=6.936e-02
***Loss=1.94218e-01
Epoch 11 Train Time 64.01890277862549s

Model Saved
Training epoch 12, Batch 500/1000: LR=9.93e-04, Loss=1.95727e-01 LER=6.966e-02
***Loss=1.95727e-01
Training epoch 12, Batch 1000/1000: LR=9.93e-04, Loss=1.95157e-01 LER=6.906e-02
***Loss=1.95157e-01
Epoch 12 Train Time 69.4820876121521s

Training epoch 13, Batch 500/1000: LR=9.91e-04, Loss=1.91260e-01 LER=6.772e-02
***Loss=1.91260e-01
Training epoch 13, Batch 1000/1000: LR=9.91e-04, Loss=1.91416e-01 LER=6.844e-02
***Loss=1.91416e-01
Epoch 13 Train Time 78.0030608177185s

Model Saved
Training epoch 14, Batch 500/1000: LR=9.90e-04, Loss=1.90971e-01 LER=6.795e-02
***Loss=1.90971e-01
Training epoch 14, Batch 1000/1000: LR=9.90e-04, Loss=1.90062e-01 LER=6.802e-02
***Loss=1.90062e-01
Epoch 14 Train Time 69.6111261844635s

Model Saved
Training epoch 15, Batch 500/1000: LR=9.88e-04, Loss=1.88343e-01 LER=6.811e-02
***Loss=1.88343e-01
Training epoch 15, Batch 1000/1000: LR=9.88e-04, Loss=1.88038e-01 LER=6.781e-02
***Loss=1.88038e-01
Epoch 15 Train Time 67.47825980186462s

Model Saved
Training epoch 16, Batch 500/1000: LR=9.86e-04, Loss=1.86646e-01 LER=6.717e-02
***Loss=1.86646e-01
Training epoch 16, Batch 1000/1000: LR=9.86e-04, Loss=1.87699e-01 LER=6.729e-02
***Loss=1.87699e-01
Epoch 16 Train Time 67.93222618103027s

Model Saved
Training epoch 17, Batch 500/1000: LR=9.84e-04, Loss=1.86087e-01 LER=6.672e-02
***Loss=1.86087e-01
Training epoch 17, Batch 1000/1000: LR=9.84e-04, Loss=1.85945e-01 LER=6.703e-02
***Loss=1.85945e-01
Epoch 17 Train Time 70.66421723365784s

Model Saved
Training epoch 18, Batch 500/1000: LR=9.82e-04, Loss=1.85823e-01 LER=6.719e-02
***Loss=1.85823e-01
Training epoch 18, Batch 1000/1000: LR=9.82e-04, Loss=1.87220e-01 LER=6.775e-02
***Loss=1.87220e-01
Epoch 18 Train Time 68.41304540634155s

Training epoch 19, Batch 500/1000: LR=9.80e-04, Loss=1.86735e-01 LER=6.729e-02
***Loss=1.86735e-01
Training epoch 19, Batch 1000/1000: LR=9.80e-04, Loss=1.85338e-01 LER=6.650e-02
***Loss=1.85338e-01
Epoch 19 Train Time 69.12750363349915s

Model Saved
Training epoch 20, Batch 500/1000: LR=9.78e-04, Loss=1.86041e-01 LER=6.731e-02
***Loss=1.86041e-01
Training epoch 20, Batch 1000/1000: LR=9.78e-04, Loss=1.85120e-01 LER=6.686e-02
***Loss=1.85120e-01
Epoch 20 Train Time 70.85902452468872s

Model Saved
Test LER  p=7.00e-02: 3.66e-02 p=8.00e-02: 5.12e-02 p=9.00e-02: 6.65e-02 p=1.00e-01: 8.37e-02 p=1.10e-01: 1.03e-01
Mean LER = 6.824e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 130.4041199684143 s

Training epoch 21, Batch 500/1000: LR=9.76e-04, Loss=1.86439e-01 LER=6.630e-02
***Loss=1.86439e-01
Training epoch 21, Batch 1000/1000: LR=9.76e-04, Loss=1.84438e-01 LER=6.588e-02
***Loss=1.84438e-01
Epoch 21 Train Time 64.00946116447449s

Model Saved
Training epoch 22, Batch 500/1000: LR=9.73e-04, Loss=1.83811e-01 LER=6.538e-02
***Loss=1.83811e-01
Training epoch 22, Batch 1000/1000: LR=9.73e-04, Loss=1.83129e-01 LER=6.488e-02
***Loss=1.83129e-01
Epoch 22 Train Time 64.1762683391571s

Model Saved
Training epoch 23, Batch 500/1000: LR=9.70e-04, Loss=1.83385e-01 LER=6.503e-02
***Loss=1.83385e-01
Training epoch 23, Batch 1000/1000: LR=9.70e-04, Loss=1.84290e-01 LER=6.550e-02
***Loss=1.84290e-01
Epoch 23 Train Time 68.9881374835968s

Training epoch 24, Batch 500/1000: LR=9.68e-04, Loss=1.84596e-01 LER=6.679e-02
***Loss=1.84596e-01
Training epoch 24, Batch 1000/1000: LR=9.68e-04, Loss=1.84098e-01 LER=6.560e-02
***Loss=1.84098e-01
Epoch 24 Train Time 67.63785767555237s

Training epoch 25, Batch 500/1000: LR=9.65e-04, Loss=1.83866e-01 LER=6.633e-02
***Loss=1.83866e-01
Training epoch 25, Batch 1000/1000: LR=9.65e-04, Loss=1.82817e-01 LER=6.557e-02
***Loss=1.82817e-01
Epoch 25 Train Time 71.11595845222473s

Model Saved
Training epoch 26, Batch 500/1000: LR=9.62e-04, Loss=1.79446e-01 LER=6.431e-02
***Loss=1.79446e-01
Training epoch 26, Batch 1000/1000: LR=9.62e-04, Loss=1.79255e-01 LER=6.418e-02
***Loss=1.79255e-01
Epoch 26 Train Time 65.37730288505554s

Model Saved
Training epoch 27, Batch 500/1000: LR=9.59e-04, Loss=1.80796e-01 LER=6.516e-02
***Loss=1.80796e-01
Training epoch 27, Batch 1000/1000: LR=9.59e-04, Loss=1.80650e-01 LER=6.482e-02
***Loss=1.80650e-01
Epoch 27 Train Time 75.68441343307495s

Training epoch 28, Batch 500/1000: LR=9.56e-04, Loss=1.80058e-01 LER=6.462e-02
***Loss=1.80058e-01
Training epoch 28, Batch 1000/1000: LR=9.56e-04, Loss=1.80828e-01 LER=6.465e-02
***Loss=1.80828e-01
Epoch 28 Train Time 74.48250246047974s

Training epoch 29, Batch 500/1000: LR=9.52e-04, Loss=1.76824e-01 LER=6.358e-02
***Loss=1.76824e-01
Training epoch 29, Batch 1000/1000: LR=9.52e-04, Loss=1.76291e-01 LER=6.287e-02
***Loss=1.76291e-01
Epoch 29 Train Time 70.18509244918823s

Model Saved
Training epoch 30, Batch 500/1000: LR=9.49e-04, Loss=1.82138e-01 LER=6.577e-02
***Loss=1.82138e-01
Training epoch 30, Batch 1000/1000: LR=9.49e-04, Loss=1.80364e-01 LER=6.528e-02
***Loss=1.80364e-01
Epoch 30 Train Time 70.04665946960449s

Test LER  p=7.00e-02: 3.57e-02 p=8.00e-02: 4.89e-02 p=9.00e-02: 6.47e-02 p=1.00e-01: 7.95e-02 p=1.10e-01: 1.00e-01
Mean LER = 6.584e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 133.34602093696594 s

Training epoch 31, Batch 500/1000: LR=9.46e-04, Loss=1.79701e-01 LER=6.419e-02
***Loss=1.79701e-01
Training epoch 31, Batch 1000/1000: LR=9.46e-04, Loss=1.79037e-01 LER=6.382e-02
***Loss=1.79037e-01
Epoch 31 Train Time 64.85184693336487s

Training epoch 32, Batch 500/1000: LR=9.42e-04, Loss=1.80906e-01 LER=6.537e-02
***Loss=1.80906e-01
Training epoch 32, Batch 1000/1000: LR=9.42e-04, Loss=1.80136e-01 LER=6.482e-02
***Loss=1.80136e-01
Epoch 32 Train Time 67.32807350158691s

Training epoch 33, Batch 500/1000: LR=9.38e-04, Loss=1.77708e-01 LER=6.389e-02
***Loss=1.77708e-01
Training epoch 33, Batch 1000/1000: LR=9.38e-04, Loss=1.77399e-01 LER=6.358e-02
***Loss=1.77399e-01
Epoch 33 Train Time 68.46403527259827s

Training epoch 34, Batch 500/1000: LR=9.34e-04, Loss=1.76310e-01 LER=6.363e-02
***Loss=1.76310e-01
Training epoch 34, Batch 1000/1000: LR=9.34e-04, Loss=1.77322e-01 LER=6.380e-02
***Loss=1.77322e-01
Epoch 34 Train Time 69.65930700302124s

Training epoch 35, Batch 500/1000: LR=9.30e-04, Loss=1.76340e-01 LER=6.292e-02
***Loss=1.76340e-01
Training epoch 35, Batch 1000/1000: LR=9.30e-04, Loss=1.76025e-01 LER=6.315e-02
***Loss=1.76025e-01
Epoch 35 Train Time 72.5087411403656s

Model Saved
Training epoch 36, Batch 500/1000: LR=9.26e-04, Loss=1.81205e-01 LER=6.469e-02
***Loss=1.81205e-01
Training epoch 36, Batch 1000/1000: LR=9.26e-04, Loss=1.79430e-01 LER=6.406e-02
***Loss=1.79430e-01
Epoch 36 Train Time 68.07920932769775s

Training epoch 37, Batch 500/1000: LR=9.22e-04, Loss=1.77131e-01 LER=6.359e-02
***Loss=1.77131e-01
Training epoch 37, Batch 1000/1000: LR=9.22e-04, Loss=1.76867e-01 LER=6.375e-02
***Loss=1.76867e-01
Epoch 37 Train Time 67.17105460166931s

Training epoch 38, Batch 500/1000: LR=9.18e-04, Loss=1.76069e-01 LER=6.309e-02
***Loss=1.76069e-01
Training epoch 38, Batch 1000/1000: LR=9.18e-04, Loss=1.76554e-01 LER=6.316e-02
***Loss=1.76554e-01
Epoch 38 Train Time 68.41258335113525s

Training epoch 39, Batch 500/1000: LR=9.14e-04, Loss=1.76229e-01 LER=6.352e-02
***Loss=1.76229e-01
Training epoch 39, Batch 1000/1000: LR=9.14e-04, Loss=1.76097e-01 LER=6.364e-02
***Loss=1.76097e-01
Epoch 39 Train Time 71.31631660461426s

Training epoch 40, Batch 500/1000: LR=9.09e-04, Loss=1.74488e-01 LER=6.228e-02
***Loss=1.74488e-01
Training epoch 40, Batch 1000/1000: LR=9.09e-04, Loss=1.74301e-01 LER=6.269e-02
***Loss=1.74301e-01
Epoch 40 Train Time 67.93446063995361s

Model Saved
Test LER  p=7.00e-02: 3.50e-02 p=8.00e-02: 4.68e-02 p=9.00e-02: 6.25e-02 p=1.00e-01: 7.85e-02 p=1.10e-01: 9.89e-02
Mean LER = 6.434e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 127.18917107582092 s

Training epoch 41, Batch 500/1000: LR=9.05e-04, Loss=1.75266e-01 LER=6.295e-02
***Loss=1.75266e-01
Training epoch 41, Batch 1000/1000: LR=9.05e-04, Loss=1.75176e-01 LER=6.284e-02
***Loss=1.75176e-01
Epoch 41 Train Time 65.13728594779968s

Training epoch 42, Batch 500/1000: LR=9.00e-04, Loss=1.74998e-01 LER=6.283e-02
***Loss=1.74998e-01
Training epoch 42, Batch 1000/1000: LR=9.00e-04, Loss=1.74698e-01 LER=6.268e-02
***Loss=1.74698e-01
Epoch 42 Train Time 65.97823190689087s

Training epoch 43, Batch 500/1000: LR=8.95e-04, Loss=1.74651e-01 LER=6.309e-02
***Loss=1.74651e-01
Training epoch 43, Batch 1000/1000: LR=8.95e-04, Loss=1.75380e-01 LER=6.337e-02
***Loss=1.75380e-01
Epoch 43 Train Time 68.64389824867249s

Training epoch 44, Batch 500/1000: LR=8.90e-04, Loss=1.73463e-01 LER=6.272e-02
***Loss=1.73463e-01
Training epoch 44, Batch 1000/1000: LR=8.90e-04, Loss=1.72808e-01 LER=6.212e-02
***Loss=1.72808e-01
Epoch 44 Train Time 72.29803609848022s

Model Saved
Training epoch 45, Batch 500/1000: LR=8.85e-04, Loss=1.73193e-01 LER=6.208e-02
***Loss=1.73193e-01
