Path to model/logs: Final_Results_QECCT/surface/Transformer_Code_L_5/noise_model_independent/repetition_1/12_11_2025_00_17_29
Namespace(epochs=200, workers=2, lr=0.001, gpus='0', batch_size=256, test_batch_size=512, seed=42, code_type='surface', code_L=5, repetitions=1, noise_type='independent', y_ratio=0.3, N_dec=6, d_model=128, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x75c2c9fde380>, path='Final_Results_QECCT/surface/Transformer_Code_L_5/noise_model_independent/repetition_1/12_11_2025_00_17_29')
Intel Arc GPU (XPU)를 사용합니다: Intel(R) Graphics [0x5694]
Intel Arc GPU (XPU) - 단일 GPU 모드 (권장: L>=5, batch_size>=512)
PC matrix shape torch.Size([24, 50])
ECC_Transformer(
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder): Encoder(
    (layers): ModuleList(
      (0-5): 6 x EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0-1): 2 x SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (input_embedding): Linear(in_features=24, out_features=128, bias=True)
  (output_classifier): Linear(in_features=128, out_features=4, bias=True)
  (criterion): CrossEntropyLoss()
)
# of Parameters: 1193860
Training epoch 1, Batch 500/1000: LR=1.00e-03, Loss=5.89053e-01 LER=2.551e-01
***Loss=5.89053e-01
Training epoch 1, Batch 1000/1000: LR=1.00e-03, Loss=4.42635e-01 LER=1.821e-01
***Loss=4.42635e-01
Epoch 1 Train Time 58.98998165130615s

Model Saved
Training epoch 2, Batch 500/1000: LR=1.00e-03, Loss=2.58634e-01 LER=9.368e-02
***Loss=2.58634e-01
Training epoch 2, Batch 1000/1000: LR=1.00e-03, Loss=2.49297e-01 LER=9.038e-02
***Loss=2.49297e-01
Epoch 2 Train Time 59.358614444732666s

Model Saved
Training epoch 3, Batch 500/1000: LR=1.00e-03, Loss=2.35857e-01 LER=8.481e-02
***Loss=2.35857e-01
Training epoch 3, Batch 1000/1000: LR=1.00e-03, Loss=2.32854e-01 LER=8.388e-02
***Loss=2.32854e-01
Epoch 3 Train Time 60.60224509239197s

Model Saved
Training epoch 4, Batch 500/1000: LR=9.99e-04, Loss=2.24050e-01 LER=8.127e-02
***Loss=2.24050e-01
Training epoch 4, Batch 1000/1000: LR=9.99e-04, Loss=2.20588e-01 LER=7.928e-02
***Loss=2.20588e-01
Epoch 4 Train Time 69.3401529788971s

Model Saved
Training epoch 5, Batch 500/1000: LR=9.99e-04, Loss=2.17632e-01 LER=7.912e-02
***Loss=2.17632e-01
Training epoch 5, Batch 1000/1000: LR=9.99e-04, Loss=2.15395e-01 LER=7.754e-02
***Loss=2.15395e-01
Epoch 5 Train Time 69.55183005332947s

Model Saved
Training epoch 6, Batch 500/1000: LR=9.98e-04, Loss=2.08952e-01 LER=7.524e-02
***Loss=2.08952e-01
Training epoch 6, Batch 1000/1000: LR=9.98e-04, Loss=2.08101e-01 LER=7.472e-02
***Loss=2.08101e-01
Epoch 6 Train Time 69.72787117958069s

Model Saved
Training epoch 7, Batch 500/1000: LR=9.98e-04, Loss=2.07929e-01 LER=7.468e-02
***Loss=2.07929e-01
Training epoch 7, Batch 1000/1000: LR=9.98e-04, Loss=2.06069e-01 LER=7.406e-02
***Loss=2.06069e-01
Epoch 7 Train Time 65.13001251220703s

Model Saved
Training epoch 8, Batch 500/1000: LR=9.97e-04, Loss=2.02025e-01 LER=7.251e-02
***Loss=2.02025e-01
Training epoch 8, Batch 1000/1000: LR=9.97e-04, Loss=2.02394e-01 LER=7.237e-02
***Loss=2.02394e-01
Epoch 8 Train Time 69.30532431602478s

Model Saved
Training epoch 9, Batch 500/1000: LR=9.96e-04, Loss=1.99831e-01 LER=7.153e-02
***Loss=1.99831e-01
Training epoch 9, Batch 1000/1000: LR=9.96e-04, Loss=1.98345e-01 LER=7.088e-02
***Loss=1.98345e-01
Epoch 9 Train Time 71.1331422328949s

Model Saved
Training epoch 10, Batch 500/1000: LR=9.95e-04, Loss=1.96831e-01 LER=7.051e-02
***Loss=1.96831e-01
Training epoch 10, Batch 1000/1000: LR=9.95e-04, Loss=1.97248e-01 LER=7.040e-02
***Loss=1.97248e-01
Epoch 10 Train Time 64.48891115188599s

Model Saved
Test LER  p=7.00e-02: 3.91e-02 p=8.00e-02: 5.25e-02 p=9.00e-02: 7.03e-02 p=1.00e-01: 8.92e-02 p=1.10e-01: 1.09e-01
Mean LER = 7.209e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 128.75580596923828 s

Training epoch 11, Batch 500/1000: LR=9.94e-04, Loss=1.95758e-01 LER=7.018e-02
***Loss=1.95758e-01
Training epoch 11, Batch 1000/1000: LR=9.94e-04, Loss=1.94218e-01 LER=6.936e-02
***Loss=1.94218e-01
Epoch 11 Train Time 64.01890277862549s

Model Saved
Training epoch 12, Batch 500/1000: LR=9.93e-04, Loss=1.95727e-01 LER=6.966e-02
***Loss=1.95727e-01
Training epoch 12, Batch 1000/1000: LR=9.93e-04, Loss=1.95157e-01 LER=6.906e-02
***Loss=1.95157e-01
Epoch 12 Train Time 69.4820876121521s

Training epoch 13, Batch 500/1000: LR=9.91e-04, Loss=1.91260e-01 LER=6.772e-02
***Loss=1.91260e-01
Training epoch 13, Batch 1000/1000: LR=9.91e-04, Loss=1.91416e-01 LER=6.844e-02
***Loss=1.91416e-01
Epoch 13 Train Time 78.0030608177185s

Model Saved
Training epoch 14, Batch 500/1000: LR=9.90e-04, Loss=1.90971e-01 LER=6.795e-02
***Loss=1.90971e-01
Training epoch 14, Batch 1000/1000: LR=9.90e-04, Loss=1.90062e-01 LER=6.802e-02
***Loss=1.90062e-01
Epoch 14 Train Time 69.6111261844635s

Model Saved
Training epoch 15, Batch 500/1000: LR=9.88e-04, Loss=1.88343e-01 LER=6.811e-02
***Loss=1.88343e-01
Training epoch 15, Batch 1000/1000: LR=9.88e-04, Loss=1.88038e-01 LER=6.781e-02
***Loss=1.88038e-01
Epoch 15 Train Time 67.47825980186462s

Model Saved
Training epoch 16, Batch 500/1000: LR=9.86e-04, Loss=1.86646e-01 LER=6.717e-02
***Loss=1.86646e-01
Training epoch 16, Batch 1000/1000: LR=9.86e-04, Loss=1.87699e-01 LER=6.729e-02
***Loss=1.87699e-01
Epoch 16 Train Time 67.93222618103027s

Model Saved
Training epoch 17, Batch 500/1000: LR=9.84e-04, Loss=1.86087e-01 LER=6.672e-02
***Loss=1.86087e-01
