Path to model/logs: Final_Results_QECCT/surface/Transformer_Code_L_5/noise_model_independent/repetition_1/12_11_2025_00_17_29
Namespace(epochs=200, workers=2, lr=0.001, gpus='0', batch_size=256, test_batch_size=512, seed=42, code_type='surface', code_L=5, repetitions=1, noise_type='independent', y_ratio=0.3, N_dec=6, d_model=128, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x75c2c9fde380>, path='Final_Results_QECCT/surface/Transformer_Code_L_5/noise_model_independent/repetition_1/12_11_2025_00_17_29')
Intel Arc GPU (XPU)를 사용합니다: Intel(R) Graphics [0x5694]
Intel Arc GPU (XPU) - 단일 GPU 모드 (권장: L>=5, batch_size>=512)
PC matrix shape torch.Size([24, 50])
ECC_Transformer(
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder): Encoder(
    (layers): ModuleList(
      (0-5): 6 x EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0-1): 2 x SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (input_embedding): Linear(in_features=24, out_features=128, bias=True)
  (output_classifier): Linear(in_features=128, out_features=4, bias=True)
  (criterion): CrossEntropyLoss()
)
# of Parameters: 1193860
Training epoch 1, Batch 500/1000: LR=1.00e-03, Loss=5.89053e-01 LER=2.551e-01
***Loss=5.89053e-01
Training epoch 1, Batch 1000/1000: LR=1.00e-03, Loss=4.42635e-01 LER=1.821e-01
***Loss=4.42635e-01
Epoch 1 Train Time 58.98998165130615s

Model Saved
Training epoch 2, Batch 500/1000: LR=1.00e-03, Loss=2.58634e-01 LER=9.368e-02
***Loss=2.58634e-01
Training epoch 2, Batch 1000/1000: LR=1.00e-03, Loss=2.49297e-01 LER=9.038e-02
***Loss=2.49297e-01
Epoch 2 Train Time 59.358614444732666s

Model Saved
Training epoch 3, Batch 500/1000: LR=1.00e-03, Loss=2.35857e-01 LER=8.481e-02
***Loss=2.35857e-01
Training epoch 3, Batch 1000/1000: LR=1.00e-03, Loss=2.32854e-01 LER=8.388e-02
***Loss=2.32854e-01
Epoch 3 Train Time 60.60224509239197s

Model Saved
Training epoch 4, Batch 500/1000: LR=9.99e-04, Loss=2.24050e-01 LER=8.127e-02
***Loss=2.24050e-01
Training epoch 4, Batch 1000/1000: LR=9.99e-04, Loss=2.20588e-01 LER=7.928e-02
***Loss=2.20588e-01
Epoch 4 Train Time 69.3401529788971s

Model Saved
Training epoch 5, Batch 500/1000: LR=9.99e-04, Loss=2.17632e-01 LER=7.912e-02
***Loss=2.17632e-01
Training epoch 5, Batch 1000/1000: LR=9.99e-04, Loss=2.15395e-01 LER=7.754e-02
***Loss=2.15395e-01
Epoch 5 Train Time 69.55183005332947s

Model Saved
Training epoch 6, Batch 500/1000: LR=9.98e-04, Loss=2.08952e-01 LER=7.524e-02
***Loss=2.08952e-01
Training epoch 6, Batch 1000/1000: LR=9.98e-04, Loss=2.08101e-01 LER=7.472e-02
***Loss=2.08101e-01
Epoch 6 Train Time 69.72787117958069s

Model Saved
Training epoch 7, Batch 500/1000: LR=9.98e-04, Loss=2.07929e-01 LER=7.468e-02
***Loss=2.07929e-01
Training epoch 7, Batch 1000/1000: LR=9.98e-04, Loss=2.06069e-01 LER=7.406e-02
***Loss=2.06069e-01
Epoch 7 Train Time 65.13001251220703s

Model Saved
Training epoch 8, Batch 500/1000: LR=9.97e-04, Loss=2.02025e-01 LER=7.251e-02
***Loss=2.02025e-01
Training epoch 8, Batch 1000/1000: LR=9.97e-04, Loss=2.02394e-01 LER=7.237e-02
***Loss=2.02394e-01
Epoch 8 Train Time 69.30532431602478s

Model Saved
Training epoch 9, Batch 500/1000: LR=9.96e-04, Loss=1.99831e-01 LER=7.153e-02
***Loss=1.99831e-01
Training epoch 9, Batch 1000/1000: LR=9.96e-04, Loss=1.98345e-01 LER=7.088e-02
***Loss=1.98345e-01
Epoch 9 Train Time 71.1331422328949s

Model Saved
Training epoch 10, Batch 500/1000: LR=9.95e-04, Loss=1.96831e-01 LER=7.051e-02
***Loss=1.96831e-01
Training epoch 10, Batch 1000/1000: LR=9.95e-04, Loss=1.97248e-01 LER=7.040e-02
***Loss=1.97248e-01
Epoch 10 Train Time 64.48891115188599s

Model Saved
Test LER  p=7.00e-02: 3.91e-02 p=8.00e-02: 5.25e-02 p=9.00e-02: 7.03e-02 p=1.00e-01: 8.92e-02 p=1.10e-01: 1.09e-01
Mean LER = 7.209e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 128.75580596923828 s

Training epoch 11, Batch 500/1000: LR=9.94e-04, Loss=1.95758e-01 LER=7.018e-02
***Loss=1.95758e-01
Training epoch 11, Batch 1000/1000: LR=9.94e-04, Loss=1.94218e-01 LER=6.936e-02
***Loss=1.94218e-01
Epoch 11 Train Time 64.01890277862549s

Model Saved
Training epoch 12, Batch 500/1000: LR=9.93e-04, Loss=1.95727e-01 LER=6.966e-02
***Loss=1.95727e-01
Training epoch 12, Batch 1000/1000: LR=9.93e-04, Loss=1.95157e-01 LER=6.906e-02
***Loss=1.95157e-01
Epoch 12 Train Time 69.4820876121521s

Training epoch 13, Batch 500/1000: LR=9.91e-04, Loss=1.91260e-01 LER=6.772e-02
***Loss=1.91260e-01
Training epoch 13, Batch 1000/1000: LR=9.91e-04, Loss=1.91416e-01 LER=6.844e-02
***Loss=1.91416e-01
Epoch 13 Train Time 78.0030608177185s

Model Saved
Training epoch 14, Batch 500/1000: LR=9.90e-04, Loss=1.90971e-01 LER=6.795e-02
***Loss=1.90971e-01
Training epoch 14, Batch 1000/1000: LR=9.90e-04, Loss=1.90062e-01 LER=6.802e-02
***Loss=1.90062e-01
Epoch 14 Train Time 69.6111261844635s

Model Saved
Training epoch 15, Batch 500/1000: LR=9.88e-04, Loss=1.88343e-01 LER=6.811e-02
***Loss=1.88343e-01
Training epoch 15, Batch 1000/1000: LR=9.88e-04, Loss=1.88038e-01 LER=6.781e-02
***Loss=1.88038e-01
Epoch 15 Train Time 67.47825980186462s

Model Saved
Training epoch 16, Batch 500/1000: LR=9.86e-04, Loss=1.86646e-01 LER=6.717e-02
***Loss=1.86646e-01
Training epoch 16, Batch 1000/1000: LR=9.86e-04, Loss=1.87699e-01 LER=6.729e-02
***Loss=1.87699e-01
Epoch 16 Train Time 67.93222618103027s

Model Saved
Training epoch 17, Batch 500/1000: LR=9.84e-04, Loss=1.86087e-01 LER=6.672e-02
***Loss=1.86087e-01
Training epoch 17, Batch 1000/1000: LR=9.84e-04, Loss=1.85945e-01 LER=6.703e-02
***Loss=1.85945e-01
Epoch 17 Train Time 70.66421723365784s

Model Saved
Training epoch 18, Batch 500/1000: LR=9.82e-04, Loss=1.85823e-01 LER=6.719e-02
***Loss=1.85823e-01
Training epoch 18, Batch 1000/1000: LR=9.82e-04, Loss=1.87220e-01 LER=6.775e-02
***Loss=1.87220e-01
Epoch 18 Train Time 68.41304540634155s

Training epoch 19, Batch 500/1000: LR=9.80e-04, Loss=1.86735e-01 LER=6.729e-02
***Loss=1.86735e-01
Training epoch 19, Batch 1000/1000: LR=9.80e-04, Loss=1.85338e-01 LER=6.650e-02
***Loss=1.85338e-01
Epoch 19 Train Time 69.12750363349915s

Model Saved
Training epoch 20, Batch 500/1000: LR=9.78e-04, Loss=1.86041e-01 LER=6.731e-02
***Loss=1.86041e-01
Training epoch 20, Batch 1000/1000: LR=9.78e-04, Loss=1.85120e-01 LER=6.686e-02
***Loss=1.85120e-01
Epoch 20 Train Time 70.85902452468872s

Model Saved
Test LER  p=7.00e-02: 3.66e-02 p=8.00e-02: 5.12e-02 p=9.00e-02: 6.65e-02 p=1.00e-01: 8.37e-02 p=1.10e-01: 1.03e-01
Mean LER = 6.824e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 130.4041199684143 s

Training epoch 21, Batch 500/1000: LR=9.76e-04, Loss=1.86439e-01 LER=6.630e-02
***Loss=1.86439e-01
Training epoch 21, Batch 1000/1000: LR=9.76e-04, Loss=1.84438e-01 LER=6.588e-02
***Loss=1.84438e-01
Epoch 21 Train Time 64.00946116447449s

Model Saved
Training epoch 22, Batch 500/1000: LR=9.73e-04, Loss=1.83811e-01 LER=6.538e-02
***Loss=1.83811e-01
Training epoch 22, Batch 1000/1000: LR=9.73e-04, Loss=1.83129e-01 LER=6.488e-02
***Loss=1.83129e-01
Epoch 22 Train Time 64.1762683391571s

Model Saved
Training epoch 23, Batch 500/1000: LR=9.70e-04, Loss=1.83385e-01 LER=6.503e-02
***Loss=1.83385e-01
Training epoch 23, Batch 1000/1000: LR=9.70e-04, Loss=1.84290e-01 LER=6.550e-02
***Loss=1.84290e-01
Epoch 23 Train Time 68.9881374835968s

Training epoch 24, Batch 500/1000: LR=9.68e-04, Loss=1.84596e-01 LER=6.679e-02
***Loss=1.84596e-01
Training epoch 24, Batch 1000/1000: LR=9.68e-04, Loss=1.84098e-01 LER=6.560e-02
***Loss=1.84098e-01
Epoch 24 Train Time 67.63785767555237s

Training epoch 25, Batch 500/1000: LR=9.65e-04, Loss=1.83866e-01 LER=6.633e-02
***Loss=1.83866e-01
Training epoch 25, Batch 1000/1000: LR=9.65e-04, Loss=1.82817e-01 LER=6.557e-02
***Loss=1.82817e-01
Epoch 25 Train Time 71.11595845222473s

Model Saved
Training epoch 26, Batch 500/1000: LR=9.62e-04, Loss=1.79446e-01 LER=6.431e-02
***Loss=1.79446e-01
Training epoch 26, Batch 1000/1000: LR=9.62e-04, Loss=1.79255e-01 LER=6.418e-02
***Loss=1.79255e-01
Epoch 26 Train Time 65.37730288505554s

Model Saved
Training epoch 27, Batch 500/1000: LR=9.59e-04, Loss=1.80796e-01 LER=6.516e-02
***Loss=1.80796e-01
Training epoch 27, Batch 1000/1000: LR=9.59e-04, Loss=1.80650e-01 LER=6.482e-02
***Loss=1.80650e-01
Epoch 27 Train Time 75.68441343307495s

Training epoch 28, Batch 500/1000: LR=9.56e-04, Loss=1.80058e-01 LER=6.462e-02
***Loss=1.80058e-01
Training epoch 28, Batch 1000/1000: LR=9.56e-04, Loss=1.80828e-01 LER=6.465e-02
***Loss=1.80828e-01
Epoch 28 Train Time 74.48250246047974s

Training epoch 29, Batch 500/1000: LR=9.52e-04, Loss=1.76824e-01 LER=6.358e-02
***Loss=1.76824e-01
Training epoch 29, Batch 1000/1000: LR=9.52e-04, Loss=1.76291e-01 LER=6.287e-02
***Loss=1.76291e-01
Epoch 29 Train Time 70.18509244918823s

Model Saved
Training epoch 30, Batch 500/1000: LR=9.49e-04, Loss=1.82138e-01 LER=6.577e-02
***Loss=1.82138e-01
Training epoch 30, Batch 1000/1000: LR=9.49e-04, Loss=1.80364e-01 LER=6.528e-02
***Loss=1.80364e-01
Epoch 30 Train Time 70.04665946960449s

Test LER  p=7.00e-02: 3.57e-02 p=8.00e-02: 4.89e-02 p=9.00e-02: 6.47e-02 p=1.00e-01: 7.95e-02 p=1.10e-01: 1.00e-01
Mean LER = 6.584e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 133.34602093696594 s

Training epoch 31, Batch 500/1000: LR=9.46e-04, Loss=1.79701e-01 LER=6.419e-02
***Loss=1.79701e-01
Training epoch 31, Batch 1000/1000: LR=9.46e-04, Loss=1.79037e-01 LER=6.382e-02
***Loss=1.79037e-01
Epoch 31 Train Time 64.85184693336487s

Training epoch 32, Batch 500/1000: LR=9.42e-04, Loss=1.80906e-01 LER=6.537e-02
***Loss=1.80906e-01
Training epoch 32, Batch 1000/1000: LR=9.42e-04, Loss=1.80136e-01 LER=6.482e-02
***Loss=1.80136e-01
Epoch 32 Train Time 67.32807350158691s

Training epoch 33, Batch 500/1000: LR=9.38e-04, Loss=1.77708e-01 LER=6.389e-02
***Loss=1.77708e-01
Training epoch 33, Batch 1000/1000: LR=9.38e-04, Loss=1.77399e-01 LER=6.358e-02
***Loss=1.77399e-01
Epoch 33 Train Time 68.46403527259827s

Training epoch 34, Batch 500/1000: LR=9.34e-04, Loss=1.76310e-01 LER=6.363e-02
***Loss=1.76310e-01
Training epoch 34, Batch 1000/1000: LR=9.34e-04, Loss=1.77322e-01 LER=6.380e-02
***Loss=1.77322e-01
Epoch 34 Train Time 69.65930700302124s

Training epoch 35, Batch 500/1000: LR=9.30e-04, Loss=1.76340e-01 LER=6.292e-02
***Loss=1.76340e-01
Training epoch 35, Batch 1000/1000: LR=9.30e-04, Loss=1.76025e-01 LER=6.315e-02
***Loss=1.76025e-01
Epoch 35 Train Time 72.5087411403656s

Model Saved
Training epoch 36, Batch 500/1000: LR=9.26e-04, Loss=1.81205e-01 LER=6.469e-02
***Loss=1.81205e-01
Training epoch 36, Batch 1000/1000: LR=9.26e-04, Loss=1.79430e-01 LER=6.406e-02
***Loss=1.79430e-01
Epoch 36 Train Time 68.07920932769775s

Training epoch 37, Batch 500/1000: LR=9.22e-04, Loss=1.77131e-01 LER=6.359e-02
***Loss=1.77131e-01
Training epoch 37, Batch 1000/1000: LR=9.22e-04, Loss=1.76867e-01 LER=6.375e-02
***Loss=1.76867e-01
Epoch 37 Train Time 67.17105460166931s

Training epoch 38, Batch 500/1000: LR=9.18e-04, Loss=1.76069e-01 LER=6.309e-02
***Loss=1.76069e-01
Training epoch 38, Batch 1000/1000: LR=9.18e-04, Loss=1.76554e-01 LER=6.316e-02
***Loss=1.76554e-01
Epoch 38 Train Time 68.41258335113525s

Training epoch 39, Batch 500/1000: LR=9.14e-04, Loss=1.76229e-01 LER=6.352e-02
***Loss=1.76229e-01
Training epoch 39, Batch 1000/1000: LR=9.14e-04, Loss=1.76097e-01 LER=6.364e-02
***Loss=1.76097e-01
Epoch 39 Train Time 71.31631660461426s

Training epoch 40, Batch 500/1000: LR=9.09e-04, Loss=1.74488e-01 LER=6.228e-02
***Loss=1.74488e-01
Training epoch 40, Batch 1000/1000: LR=9.09e-04, Loss=1.74301e-01 LER=6.269e-02
***Loss=1.74301e-01
Epoch 40 Train Time 67.93446063995361s

Model Saved
Test LER  p=7.00e-02: 3.50e-02 p=8.00e-02: 4.68e-02 p=9.00e-02: 6.25e-02 p=1.00e-01: 7.85e-02 p=1.10e-01: 9.89e-02
Mean LER = 6.434e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 127.18917107582092 s

Training epoch 41, Batch 500/1000: LR=9.05e-04, Loss=1.75266e-01 LER=6.295e-02
***Loss=1.75266e-01
Training epoch 41, Batch 1000/1000: LR=9.05e-04, Loss=1.75176e-01 LER=6.284e-02
***Loss=1.75176e-01
Epoch 41 Train Time 65.13728594779968s

Training epoch 42, Batch 500/1000: LR=9.00e-04, Loss=1.74998e-01 LER=6.283e-02
***Loss=1.74998e-01
Training epoch 42, Batch 1000/1000: LR=9.00e-04, Loss=1.74698e-01 LER=6.268e-02
***Loss=1.74698e-01
Epoch 42 Train Time 65.97823190689087s

Training epoch 43, Batch 500/1000: LR=8.95e-04, Loss=1.74651e-01 LER=6.309e-02
***Loss=1.74651e-01
Training epoch 43, Batch 1000/1000: LR=8.95e-04, Loss=1.75380e-01 LER=6.337e-02
***Loss=1.75380e-01
Epoch 43 Train Time 68.64389824867249s

Training epoch 44, Batch 500/1000: LR=8.90e-04, Loss=1.73463e-01 LER=6.272e-02
***Loss=1.73463e-01
Training epoch 44, Batch 1000/1000: LR=8.90e-04, Loss=1.72808e-01 LER=6.212e-02
***Loss=1.72808e-01
Epoch 44 Train Time 72.29803609848022s

Model Saved
Training epoch 45, Batch 500/1000: LR=8.85e-04, Loss=1.73193e-01 LER=6.208e-02
***Loss=1.73193e-01
Training epoch 45, Batch 1000/1000: LR=8.85e-04, Loss=1.73138e-01 LER=6.220e-02
***Loss=1.73138e-01
Epoch 45 Train Time 66.85414052009583s

Training epoch 46, Batch 500/1000: LR=8.80e-04, Loss=1.72516e-01 LER=6.199e-02
***Loss=1.72516e-01
Training epoch 46, Batch 1000/1000: LR=8.80e-04, Loss=1.73196e-01 LER=6.263e-02
***Loss=1.73196e-01
Epoch 46 Train Time 69.12195658683777s

Training epoch 47, Batch 500/1000: LR=8.75e-04, Loss=1.73597e-01 LER=6.235e-02
***Loss=1.73597e-01
Training epoch 47, Batch 1000/1000: LR=8.75e-04, Loss=1.73210e-01 LER=6.236e-02
***Loss=1.73210e-01
Epoch 47 Train Time 67.66022992134094s

Training epoch 48, Batch 500/1000: LR=8.70e-04, Loss=1.72575e-01 LER=6.127e-02
***Loss=1.72575e-01
Training epoch 48, Batch 1000/1000: LR=8.70e-04, Loss=1.73753e-01 LER=6.203e-02
***Loss=1.73753e-01
Epoch 48 Train Time 67.28426623344421s

Training epoch 49, Batch 500/1000: LR=8.65e-04, Loss=1.73308e-01 LER=6.205e-02
***Loss=1.73308e-01
Training epoch 49, Batch 1000/1000: LR=8.65e-04, Loss=1.73376e-01 LER=6.180e-02
***Loss=1.73376e-01
Epoch 49 Train Time 65.58033108711243s

Training epoch 50, Batch 500/1000: LR=8.59e-04, Loss=1.73704e-01 LER=6.263e-02
***Loss=1.73704e-01
Training epoch 50, Batch 1000/1000: LR=8.59e-04, Loss=1.73716e-01 LER=6.295e-02
***Loss=1.73716e-01
Epoch 50 Train Time 68.076730966568s

Test LER  p=7.00e-02: 3.32e-02 p=8.00e-02: 4.74e-02 p=9.00e-02: 6.13e-02 p=1.00e-01: 7.88e-02 p=1.10e-01: 1.01e-01
Mean LER = 6.435e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 132.14239716529846 s

Training epoch 51, Batch 500/1000: LR=8.54e-04, Loss=1.74635e-01 LER=6.298e-02
***Loss=1.74635e-01
Training epoch 51, Batch 1000/1000: LR=8.54e-04, Loss=1.73552e-01 LER=6.255e-02
***Loss=1.73552e-01
Epoch 51 Train Time 66.22243618965149s

Training epoch 52, Batch 500/1000: LR=8.48e-04, Loss=1.74614e-01 LER=6.348e-02
***Loss=1.74614e-01
Training epoch 52, Batch 1000/1000: LR=8.48e-04, Loss=1.73520e-01 LER=6.286e-02
***Loss=1.73520e-01
Epoch 52 Train Time 67.21136689186096s

Training epoch 53, Batch 500/1000: LR=8.42e-04, Loss=1.72724e-01 LER=6.208e-02
***Loss=1.72724e-01
Training epoch 53, Batch 1000/1000: LR=8.42e-04, Loss=1.71990e-01 LER=6.164e-02
***Loss=1.71990e-01
Epoch 53 Train Time 67.05240654945374s

Model Saved
Training epoch 54, Batch 500/1000: LR=8.37e-04, Loss=1.72968e-01 LER=6.231e-02
***Loss=1.72968e-01
Training epoch 54, Batch 1000/1000: LR=8.37e-04, Loss=1.72635e-01 LER=6.223e-02
***Loss=1.72635e-01
Epoch 54 Train Time 69.42088389396667s

Training epoch 55, Batch 500/1000: LR=8.31e-04, Loss=1.70739e-01 LER=6.112e-02
***Loss=1.70739e-01
Training epoch 55, Batch 1000/1000: LR=8.31e-04, Loss=1.71532e-01 LER=6.124e-02
***Loss=1.71532e-01
Epoch 55 Train Time 71.36932063102722s

Model Saved
Training epoch 56, Batch 500/1000: LR=8.25e-04, Loss=1.71174e-01 LER=6.199e-02
***Loss=1.71174e-01
Training epoch 56, Batch 1000/1000: LR=8.25e-04, Loss=1.70110e-01 LER=6.123e-02
***Loss=1.70110e-01
Epoch 56 Train Time 71.78095626831055s

Model Saved
Training epoch 57, Batch 500/1000: LR=8.19e-04, Loss=1.72441e-01 LER=6.220e-02
***Loss=1.72441e-01
Training epoch 57, Batch 1000/1000: LR=8.19e-04, Loss=1.74080e-01 LER=6.269e-02
***Loss=1.74080e-01
Epoch 57 Train Time 76.44505453109741s

Training epoch 58, Batch 500/1000: LR=8.13e-04, Loss=1.72110e-01 LER=6.167e-02
***Loss=1.72110e-01
Training epoch 58, Batch 1000/1000: LR=8.13e-04, Loss=1.72883e-01 LER=6.224e-02
***Loss=1.72883e-01
Epoch 58 Train Time 68.74894070625305s

Training epoch 59, Batch 500/1000: LR=8.07e-04, Loss=1.70000e-01 LER=6.129e-02
***Loss=1.70000e-01
Training epoch 59, Batch 1000/1000: LR=8.07e-04, Loss=1.71032e-01 LER=6.193e-02
***Loss=1.71032e-01
Epoch 59 Train Time 67.87199449539185s

Training epoch 60, Batch 500/1000: LR=8.00e-04, Loss=1.73928e-01 LER=6.254e-02
***Loss=1.73928e-01
Training epoch 60, Batch 1000/1000: LR=8.00e-04, Loss=1.73456e-01 LER=6.245e-02
***Loss=1.73456e-01
Epoch 60 Train Time 65.37825512886047s

Test LER  p=7.00e-02: 3.30e-02 p=8.00e-02: 4.70e-02 p=9.00e-02: 6.11e-02 p=1.00e-01: 7.92e-02 p=1.10e-01: 9.92e-02
Mean LER = 6.392e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 124.04106068611145 s

Training epoch 61, Batch 500/1000: LR=7.94e-04, Loss=1.71047e-01 LER=6.128e-02
***Loss=1.71047e-01
Training epoch 61, Batch 1000/1000: LR=7.94e-04, Loss=1.72117e-01 LER=6.192e-02
***Loss=1.72117e-01
Epoch 61 Train Time 62.07700204849243s

Training epoch 62, Batch 500/1000: LR=7.88e-04, Loss=1.69648e-01 LER=6.070e-02
***Loss=1.69648e-01
Training epoch 62, Batch 1000/1000: LR=7.88e-04, Loss=1.68914e-01 LER=6.026e-02
***Loss=1.68914e-01
Epoch 62 Train Time 67.5103657245636s

Model Saved
Training epoch 63, Batch 500/1000: LR=7.81e-04, Loss=1.70087e-01 LER=6.085e-02
***Loss=1.70087e-01
Training epoch 63, Batch 1000/1000: LR=7.81e-04, Loss=1.69716e-01 LER=6.059e-02
***Loss=1.69716e-01
Epoch 63 Train Time 69.42656993865967s

Training epoch 64, Batch 500/1000: LR=7.75e-04, Loss=1.69256e-01 LER=6.059e-02
***Loss=1.69256e-01
Training epoch 64, Batch 1000/1000: LR=7.75e-04, Loss=1.70495e-01 LER=6.112e-02
***Loss=1.70495e-01
Epoch 64 Train Time 71.09800601005554s

Training epoch 65, Batch 500/1000: LR=7.68e-04, Loss=1.69740e-01 LER=6.077e-02
***Loss=1.69740e-01
Training epoch 65, Batch 1000/1000: LR=7.68e-04, Loss=1.70676e-01 LER=6.090e-02
***Loss=1.70676e-01
Epoch 65 Train Time 66.01968932151794s

Training epoch 66, Batch 500/1000: LR=7.61e-04, Loss=1.71708e-01 LER=6.168e-02
***Loss=1.71708e-01
Training epoch 66, Batch 1000/1000: LR=7.61e-04, Loss=1.71585e-01 LER=6.174e-02
***Loss=1.71585e-01
Epoch 66 Train Time 67.63225507736206s

Training epoch 67, Batch 500/1000: LR=7.55e-04, Loss=1.69803e-01 LER=6.087e-02
***Loss=1.69803e-01
Training epoch 67, Batch 1000/1000: LR=7.55e-04, Loss=1.70468e-01 LER=6.091e-02
***Loss=1.70468e-01
Epoch 67 Train Time 75.62321138381958s

Training epoch 68, Batch 500/1000: LR=7.48e-04, Loss=1.71389e-01 LER=6.152e-02
***Loss=1.71389e-01
Training epoch 68, Batch 1000/1000: LR=7.48e-04, Loss=1.71872e-01 LER=6.172e-02
***Loss=1.71872e-01
Epoch 68 Train Time 67.56504034996033s

Training epoch 69, Batch 500/1000: LR=7.41e-04, Loss=1.68928e-01 LER=6.063e-02
***Loss=1.68928e-01
Training epoch 69, Batch 1000/1000: LR=7.41e-04, Loss=1.69631e-01 LER=6.101e-02
***Loss=1.69631e-01
Epoch 69 Train Time 65.65875959396362s

Training epoch 70, Batch 500/1000: LR=7.34e-04, Loss=1.68338e-01 LER=5.984e-02
***Loss=1.68338e-01
Training epoch 70, Batch 1000/1000: LR=7.34e-04, Loss=1.69292e-01 LER=6.025e-02
***Loss=1.69292e-01
Epoch 70 Train Time 67.2713885307312s

Test LER  p=7.00e-02: 3.42e-02 p=8.00e-02: 4.58e-02 p=9.00e-02: 6.12e-02 p=1.00e-01: 7.86e-02 p=1.10e-01: 9.76e-02
Mean LER = 6.347e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 129.88306379318237 s

Training epoch 71, Batch 500/1000: LR=7.27e-04, Loss=1.69138e-01 LER=6.109e-02
***Loss=1.69138e-01
Training epoch 71, Batch 1000/1000: LR=7.27e-04, Loss=1.68324e-01 LER=6.066e-02
***Loss=1.68324e-01
Epoch 71 Train Time 66.77599835395813s

Model Saved
Training epoch 72, Batch 500/1000: LR=7.20e-04, Loss=1.68324e-01 LER=6.052e-02
***Loss=1.68324e-01
Training epoch 72, Batch 1000/1000: LR=7.20e-04, Loss=1.69675e-01 LER=6.075e-02
***Loss=1.69675e-01
Epoch 72 Train Time 67.19254946708679s

Training epoch 73, Batch 500/1000: LR=7.13e-04, Loss=1.68109e-01 LER=6.110e-02
***Loss=1.68109e-01
Training epoch 73, Batch 1000/1000: LR=7.13e-04, Loss=1.69509e-01 LER=6.107e-02
***Loss=1.69509e-01
Epoch 73 Train Time 67.07807970046997s

Training epoch 74, Batch 500/1000: LR=7.06e-04, Loss=1.69392e-01 LER=6.146e-02
***Loss=1.69392e-01
Training epoch 74, Batch 1000/1000: LR=7.06e-04, Loss=1.69528e-01 LER=6.122e-02
***Loss=1.69528e-01
Epoch 74 Train Time 65.43725514411926s

Training epoch 75, Batch 500/1000: LR=6.99e-04, Loss=1.68901e-01 LER=6.133e-02
***Loss=1.68901e-01
Training epoch 75, Batch 1000/1000: LR=6.99e-04, Loss=1.68121e-01 LER=6.070e-02
***Loss=1.68121e-01
Epoch 75 Train Time 67.09976577758789s

Model Saved
Training epoch 76, Batch 500/1000: LR=6.92e-04, Loss=1.71364e-01 LER=6.155e-02
***Loss=1.71364e-01
Training epoch 76, Batch 1000/1000: LR=6.92e-04, Loss=1.71004e-01 LER=6.156e-02
***Loss=1.71004e-01
Epoch 76 Train Time 66.22483682632446s

Training epoch 77, Batch 500/1000: LR=6.84e-04, Loss=1.70291e-01 LER=6.118e-02
***Loss=1.70291e-01
Training epoch 77, Batch 1000/1000: LR=6.84e-04, Loss=1.70228e-01 LER=6.123e-02
***Loss=1.70228e-01
Epoch 77 Train Time 65.10754823684692s

Training epoch 78, Batch 500/1000: LR=6.77e-04, Loss=1.68365e-01 LER=6.010e-02
***Loss=1.68365e-01
Training epoch 78, Batch 1000/1000: LR=6.77e-04, Loss=1.68299e-01 LER=6.013e-02
***Loss=1.68299e-01
Epoch 78 Train Time 66.01334834098816s

Training epoch 79, Batch 500/1000: LR=6.70e-04, Loss=1.67849e-01 LER=6.095e-02
***Loss=1.67849e-01
Training epoch 79, Batch 1000/1000: LR=6.70e-04, Loss=1.67836e-01 LER=6.088e-02
***Loss=1.67836e-01
Epoch 79 Train Time 87.8307638168335s

Model Saved
Training epoch 80, Batch 500/1000: LR=6.62e-04, Loss=1.69293e-01 LER=6.096e-02
***Loss=1.69293e-01
Training epoch 80, Batch 1000/1000: LR=6.62e-04, Loss=1.69568e-01 LER=6.102e-02
***Loss=1.69568e-01
Epoch 80 Train Time 68.01277732849121s

Test LER  p=7.00e-02: 3.25e-02 p=8.00e-02: 4.65e-02 p=9.00e-02: 6.00e-02 p=1.00e-01: 7.90e-02 p=1.10e-01: 9.69e-02
Mean LER = 6.298e-02
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 129.84757661819458 s

Training epoch 81, Batch 500/1000: LR=6.55e-04, Loss=1.68306e-01 LER=6.029e-02
***Loss=1.68306e-01
Training epoch 81, Batch 1000/1000: LR=6.55e-04, Loss=1.69509e-01 LER=6.068e-02
***Loss=1.69509e-01
Epoch 81 Train Time 64.75857043266296s

Training epoch 82, Batch 500/1000: LR=6.47e-04, Loss=1.71423e-01 LER=6.073e-02
***Loss=1.71423e-01
Training epoch 82, Batch 1000/1000: LR=6.47e-04, Loss=1.68491e-01 LER=6.006e-02
***Loss=1.68491e-01
Epoch 82 Train Time 67.84198260307312s

Training epoch 83, Batch 500/1000: LR=6.40e-04, Loss=1.68418e-01 LER=6.048e-02
***Loss=1.68418e-01
Training epoch 83, Batch 1000/1000: LR=6.40e-04, Loss=1.68812e-01 LER=6.080e-02
***Loss=1.68812e-01
Epoch 83 Train Time 68.40424036979675s

Training epoch 84, Batch 500/1000: LR=6.32e-04, Loss=1.68050e-01 LER=6.029e-02
***Loss=1.68050e-01
Training epoch 84, Batch 1000/1000: LR=6.32e-04, Loss=1.67790e-01 LER=6.012e-02
***Loss=1.67790e-01
Epoch 84 Train Time 66.53421545028687s

Model Saved
Training epoch 85, Batch 500/1000: LR=6.25e-04, Loss=1.66806e-01 LER=6.037e-02
***Loss=1.66806e-01
Training epoch 85, Batch 1000/1000: LR=6.25e-04, Loss=1.67768e-01 LER=6.046e-02
***Loss=1.67768e-01
Epoch 85 Train Time 68.62041807174683s

Model Saved
Training epoch 86, Batch 500/1000: LR=6.17e-04, Loss=1.67858e-01 LER=6.070e-02
***Loss=1.67858e-01
Training epoch 86, Batch 1000/1000: LR=6.17e-04, Loss=1.67693e-01 LER=6.036e-02
***Loss=1.67693e-01
Epoch 86 Train Time 63.10074591636658s

Model Saved
Training epoch 87, Batch 500/1000: LR=6.09e-04, Loss=1.69129e-01 LER=6.035e-02
***Loss=1.69129e-01
Training epoch 87, Batch 1000/1000: LR=6.09e-04, Loss=1.68334e-01 LER=6.055e-02
***Loss=1.68334e-01
Epoch 87 Train Time 71.619375705719s

Training epoch 88, Batch 500/1000: LR=6.02e-04, Loss=1.67130e-01 LER=5.984e-02
***Loss=1.67130e-01
Training epoch 88, Batch 1000/1000: LR=6.02e-04, Loss=1.67786e-01 LER=6.038e-02
***Loss=1.67786e-01
Epoch 88 Train Time 70.21068000793457s

Training epoch 89, Batch 500/1000: LR=5.94e-04, Loss=1.65494e-01 LER=5.937e-02
***Loss=1.65494e-01
Training epoch 89, Batch 1000/1000: LR=5.94e-04, Loss=1.65706e-01 LER=5.961e-02
***Loss=1.65706e-01
Epoch 89 Train Time 64.51923441886902s

Model Saved
