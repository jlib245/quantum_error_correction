Path to model/logs: Final_Results_QECCT/surface/Transformer_Code_L_5/noise_model_independent/repetition_1/11_11_2025_18_31_50
Namespace(epochs=300, workers=8, lr=0.0008, gpus='0', batch_size=1024, test_batch_size=512, seed=42, device='cuda', patience=40, min_delta=0.0, code_type='surface', code_L=5, repetitions=1, noise_type='independent', y_ratio=0.3, N_dec=10, d_model=256, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x7e9ea257ec00>, path='Final_Results_QECCT/surface/Transformer_Code_L_5/noise_model_independent/repetition_1/11_11_2025_18_31_50')
NVIDIA GPU (CUDA)를 사용합니다: NVIDIA A100-SXM4-40GB
PC matrix shape torch.Size([24, 50])
ECC_Transformer(
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder): Encoder(
    (layers): ModuleList(
      (0-9): 10 x EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0-1): 2 x SublayerConnection(
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (input_embedding): Linear(in_features=24, out_features=256, bias=True)
  (output_classifier): Linear(in_features=256, out_features=4, bias=True)
  (criterion): CrossEntropyLoss()
)
# of Parameters: 7906052
Training epoch 1, Batch 49/98: LR=8.00e-04, Loss=1.30027e+00 LER=5.512e-01
***Loss=1.30027e+00
Training epoch 1, Batch 98/98: LR=8.00e-04, Loss=1.26551e+00 LER=5.446e-01
***Loss=1.26551e+00
Epoch 1 Train Time 10.89155912399292s

Model Saved - New best loss: 1.26551e+00
Training epoch 2, Batch 49/98: LR=8.00e-04, Loss=1.24286e+00 LER=5.342e-01
***Loss=1.24286e+00
Training epoch 2, Batch 98/98: LR=8.00e-04, Loss=1.17607e+00 LER=5.029e-01
***Loss=1.17607e+00
Epoch 2 Train Time 10.817844152450562s

Model Saved - New best loss: 1.17607e+00
Training epoch 3, Batch 49/98: LR=8.00e-04, Loss=1.23771e+00 LER=5.352e-01
***Loss=1.23771e+00
Training epoch 3, Batch 98/98: LR=8.00e-04, Loss=1.22019e+00 LER=5.335e-01
***Loss=1.22019e+00
Epoch 3 Train Time 11.30479884147644s

No improvement. Patience: 1/40
Training epoch 4, Batch 49/98: LR=8.00e-04, Loss=1.01432e+00 LER=4.225e-01
***Loss=1.01432e+00
Training epoch 4, Batch 98/98: LR=8.00e-04, Loss=9.97000e-01 LER=4.126e-01
***Loss=9.97000e-01
Epoch 4 Train Time 11.266480684280396s

Model Saved - New best loss: 9.97000e-01
Training epoch 5, Batch 49/98: LR=8.00e-04, Loss=9.60808e-01 LER=3.950e-01
***Loss=9.60808e-01
Training epoch 5, Batch 98/98: LR=8.00e-04, Loss=9.39635e-01 LER=3.963e-01
***Loss=9.39635e-01
Epoch 5 Train Time 11.173950433731079s

Model Saved - New best loss: 9.39635e-01
Training epoch 6, Batch 49/98: LR=7.99e-04, Loss=8.34659e-01 LER=3.581e-01
***Loss=8.34659e-01
Training epoch 6, Batch 98/98: LR=7.99e-04, Loss=7.75172e-01 LER=3.542e-01
***Loss=7.75172e-01
Epoch 6 Train Time 11.01941967010498s

Model Saved - New best loss: 7.75172e-01
Training epoch 7, Batch 49/98: LR=7.99e-04, Loss=6.74299e-01 LER=3.367e-01
***Loss=6.74299e-01
Training epoch 7, Batch 98/98: LR=7.99e-04, Loss=6.59395e-01 LER=3.223e-01
***Loss=6.59395e-01
Epoch 7 Train Time 11.174929857254028s

Model Saved - New best loss: 6.59395e-01
Training epoch 8, Batch 49/98: LR=7.99e-04, Loss=6.12983e-01 LER=2.628e-01
***Loss=6.12983e-01
Training epoch 8, Batch 98/98: LR=7.99e-04, Loss=5.81657e-01 LER=2.340e-01
***Loss=5.81657e-01
Epoch 8 Train Time 11.160095453262329s

Model Saved - New best loss: 5.81657e-01
Training epoch 9, Batch 49/98: LR=7.99e-04, Loss=5.33840e-01 LER=1.966e-01
***Loss=5.33840e-01
Training epoch 9, Batch 98/98: LR=7.99e-04, Loss=5.15255e-01 LER=1.898e-01
***Loss=5.15255e-01
Epoch 9 Train Time 11.2002694606781s

Model Saved - New best loss: 5.15255e-01
Training epoch 10, Batch 49/98: LR=7.98e-04, Loss=4.90918e-01 LER=1.853e-01
***Loss=4.90918e-01
Training epoch 10, Batch 98/98: LR=7.98e-04, Loss=4.74950e-01 LER=1.786e-01
***Loss=4.74950e-01
Epoch 10 Train Time 10.847016334533691s

Model Saved - New best loss: 4.74950e-01
Test LER  p=7.00e-02: 1.17e-01 p=8.00e-02: 1.44e-01 p=9.00e-02: 1.71e-01 p=1.00e-01: 1.89e-01 p=1.10e-01: 2.35e-01
Mean LER = 1.713e-01
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.09430503845215 s

Training epoch 11, Batch 49/98: LR=7.98e-04, Loss=4.25878e-01 LER=1.602e-01
***Loss=4.25878e-01
Training epoch 11, Batch 98/98: LR=7.98e-04, Loss=4.14647e-01 LER=1.568e-01
***Loss=4.14647e-01
Epoch 11 Train Time 10.714516401290894s

Model Saved - New best loss: 4.14647e-01
Training epoch 12, Batch 49/98: LR=7.97e-04, Loss=3.97992e-01 LER=1.533e-01
***Loss=3.97992e-01
Training epoch 12, Batch 98/98: LR=7.97e-04, Loss=3.87070e-01 LER=1.484e-01
***Loss=3.87070e-01
Epoch 12 Train Time 11.101897954940796s

Model Saved - New best loss: 3.87070e-01
Training epoch 13, Batch 49/98: LR=7.97e-04, Loss=3.53906e-01 LER=1.350e-01
***Loss=3.53906e-01
Training epoch 13, Batch 98/98: LR=7.97e-04, Loss=3.52336e-01 LER=1.331e-01
***Loss=3.52336e-01
Epoch 13 Train Time 11.218411684036255s

Model Saved - New best loss: 3.52336e-01
Training epoch 14, Batch 49/98: LR=7.96e-04, Loss=3.41621e-01 LER=1.266e-01
***Loss=3.41621e-01
Training epoch 14, Batch 98/98: LR=7.96e-04, Loss=3.29365e-01 LER=1.218e-01
***Loss=3.29365e-01
Epoch 14 Train Time 11.08766484260559s

Model Saved - New best loss: 3.29365e-01
Training epoch 15, Batch 49/98: LR=7.96e-04, Loss=3.18396e-01 LER=1.177e-01
***Loss=3.18396e-01
Training epoch 15, Batch 98/98: LR=7.96e-04, Loss=3.15618e-01 LER=1.163e-01
***Loss=3.15618e-01
Epoch 15 Train Time 11.200773477554321s

Model Saved - New best loss: 3.15618e-01
Training epoch 16, Batch 49/98: LR=7.95e-04, Loss=3.07843e-01 LER=1.137e-01
***Loss=3.07843e-01
Training epoch 16, Batch 98/98: LR=7.95e-04, Loss=3.03622e-01 LER=1.128e-01
***Loss=3.03622e-01
Epoch 16 Train Time 11.023513793945312s

Model Saved - New best loss: 3.03622e-01
Training epoch 17, Batch 49/98: LR=7.94e-04, Loss=2.98755e-01 LER=1.101e-01
***Loss=2.98755e-01
Training epoch 17, Batch 98/98: LR=7.94e-04, Loss=2.94060e-01 LER=1.078e-01
***Loss=2.94060e-01
Epoch 17 Train Time 11.71490216255188s

Model Saved - New best loss: 2.94060e-01
Training epoch 18, Batch 49/98: LR=7.94e-04, Loss=2.83724e-01 LER=1.046e-01
***Loss=2.83724e-01
Training epoch 18, Batch 98/98: LR=7.94e-04, Loss=2.82614e-01 LER=1.051e-01
***Loss=2.82614e-01
Epoch 18 Train Time 11.111606121063232s

Model Saved - New best loss: 2.82614e-01
Training epoch 19, Batch 49/98: LR=7.93e-04, Loss=2.72851e-01 LER=9.911e-02
***Loss=2.72851e-01
Training epoch 19, Batch 98/98: LR=7.93e-04, Loss=2.76803e-01 LER=1.013e-01
***Loss=2.76803e-01
Epoch 19 Train Time 11.46661639213562s

Model Saved - New best loss: 2.76803e-01
Training epoch 20, Batch 49/98: LR=7.92e-04, Loss=2.71190e-01 LER=9.879e-02
***Loss=2.71190e-01
Training epoch 20, Batch 98/98: LR=7.92e-04, Loss=2.68254e-01 LER=9.790e-02
***Loss=2.68254e-01
Epoch 20 Train Time 11.252379894256592s

Model Saved - New best loss: 2.68254e-01
Test LER  p=7.00e-02: 5.39e-02 p=8.00e-02: 7.19e-02 p=9.00e-02: 9.23e-02 p=1.00e-01: 1.23e-01 p=1.10e-01: 1.41e-01
Mean LER = 9.625e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.382213592529297 s

Training epoch 21, Batch 49/98: LR=7.91e-04, Loss=2.69289e-01 LER=9.919e-02
***Loss=2.69289e-01
Training epoch 21, Batch 98/98: LR=7.91e-04, Loss=2.63161e-01 LER=9.664e-02
***Loss=2.63161e-01
Epoch 21 Train Time 11.357837915420532s

Model Saved - New best loss: 2.63161e-01
Training epoch 22, Batch 49/98: LR=7.90e-04, Loss=2.60681e-01 LER=9.544e-02
***Loss=2.60681e-01
Training epoch 22, Batch 98/98: LR=7.90e-04, Loss=2.59834e-01 LER=9.556e-02
***Loss=2.59834e-01
Epoch 22 Train Time 11.211282968521118s

Model Saved - New best loss: 2.59834e-01
Training epoch 23, Batch 49/98: LR=7.89e-04, Loss=2.57117e-01 LER=9.383e-02
***Loss=2.57117e-01
Training epoch 23, Batch 98/98: LR=7.89e-04, Loss=2.55921e-01 LER=9.344e-02
***Loss=2.55921e-01
Epoch 23 Train Time 11.307581424713135s

Model Saved - New best loss: 2.55921e-01
Training epoch 24, Batch 49/98: LR=7.88e-04, Loss=2.55543e-01 LER=9.265e-02
***Loss=2.55543e-01
Training epoch 24, Batch 98/98: LR=7.88e-04, Loss=2.50012e-01 LER=9.060e-02
***Loss=2.50012e-01
Epoch 24 Train Time 11.27150583267212s

Model Saved - New best loss: 2.50012e-01
Training epoch 25, Batch 49/98: LR=7.87e-04, Loss=2.47521e-01 LER=8.982e-02
***Loss=2.47521e-01
Training epoch 25, Batch 98/98: LR=7.87e-04, Loss=2.45956e-01 LER=8.888e-02
***Loss=2.45956e-01
Epoch 25 Train Time 11.245024681091309s

Model Saved - New best loss: 2.45956e-01
Training epoch 26, Batch 49/98: LR=7.86e-04, Loss=2.41321e-01 LER=8.723e-02
***Loss=2.41321e-01
Training epoch 26, Batch 98/98: LR=7.86e-04, Loss=2.39628e-01 LER=8.631e-02
***Loss=2.39628e-01
Epoch 26 Train Time 11.090058088302612s

Model Saved - New best loss: 2.39628e-01
Training epoch 27, Batch 49/98: LR=7.85e-04, Loss=2.41603e-01 LER=8.725e-02
***Loss=2.41603e-01
Training epoch 27, Batch 98/98: LR=7.85e-04, Loss=2.39365e-01 LER=8.703e-02
***Loss=2.39365e-01
Epoch 27 Train Time 10.849262237548828s

Model Saved - New best loss: 2.39365e-01
Training epoch 28, Batch 49/98: LR=7.84e-04, Loss=2.31525e-01 LER=8.454e-02
***Loss=2.31525e-01
Training epoch 28, Batch 98/98: LR=7.84e-04, Loss=2.33275e-01 LER=8.456e-02
***Loss=2.33275e-01
Epoch 28 Train Time 10.988370418548584s

Model Saved - New best loss: 2.33275e-01
Training epoch 29, Batch 49/98: LR=7.83e-04, Loss=2.33718e-01 LER=8.315e-02
***Loss=2.33718e-01
Training epoch 29, Batch 98/98: LR=7.83e-04, Loss=2.30404e-01 LER=8.287e-02
***Loss=2.30404e-01
Epoch 29 Train Time 11.172801971435547s

Model Saved - New best loss: 2.30404e-01
Training epoch 30, Batch 49/98: LR=7.82e-04, Loss=2.24045e-01 LER=8.157e-02
***Loss=2.24045e-01
Training epoch 30, Batch 98/98: LR=7.82e-04, Loss=2.25201e-01 LER=8.155e-02
***Loss=2.25201e-01
Epoch 30 Train Time 11.27652907371521s

Model Saved - New best loss: 2.25201e-01
Test LER  p=7.00e-02: 4.36e-02 p=8.00e-02: 6.13e-02 p=9.00e-02: 8.15e-02 p=1.00e-01: 1.03e-01 p=1.10e-01: 1.23e-01
Mean LER = 8.242e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.540730237960815 s

Training epoch 31, Batch 49/98: LR=7.80e-04, Loss=2.25816e-01 LER=8.195e-02
***Loss=2.25816e-01
Training epoch 31, Batch 98/98: LR=7.80e-04, Loss=2.25499e-01 LER=8.173e-02
***Loss=2.25499e-01
Epoch 31 Train Time 10.947849988937378s

No improvement. Patience: 1/40
Training epoch 32, Batch 49/98: LR=7.79e-04, Loss=2.25128e-01 LER=8.090e-02
***Loss=2.25128e-01
Training epoch 32, Batch 98/98: LR=7.79e-04, Loss=2.24614e-01 LER=8.024e-02
***Loss=2.24614e-01
Epoch 32 Train Time 11.077421426773071s

Model Saved - New best loss: 2.24614e-01
Training epoch 33, Batch 49/98: LR=7.78e-04, Loss=2.22782e-01 LER=7.962e-02
***Loss=2.22782e-01
Training epoch 33, Batch 98/98: LR=7.78e-04, Loss=2.22266e-01 LER=7.911e-02
***Loss=2.22266e-01
Epoch 33 Train Time 10.992294788360596s

Model Saved - New best loss: 2.22266e-01
Training epoch 34, Batch 49/98: LR=7.76e-04, Loss=2.19376e-01 LER=7.926e-02
***Loss=2.19376e-01
Training epoch 34, Batch 98/98: LR=7.76e-04, Loss=2.16867e-01 LER=7.829e-02
***Loss=2.16867e-01
Epoch 34 Train Time 10.670348882675171s

Model Saved - New best loss: 2.16867e-01
Training epoch 35, Batch 49/98: LR=7.75e-04, Loss=2.21889e-01 LER=8.010e-02
***Loss=2.21889e-01
Training epoch 35, Batch 98/98: LR=7.75e-04, Loss=2.21327e-01 LER=7.988e-02
***Loss=2.21327e-01
Epoch 35 Train Time 10.795608043670654s

No improvement. Patience: 1/40
Training epoch 36, Batch 49/98: LR=7.73e-04, Loss=2.19174e-01 LER=7.850e-02
***Loss=2.19174e-01
Training epoch 36, Batch 98/98: LR=7.73e-04, Loss=2.17593e-01 LER=7.815e-02
***Loss=2.17593e-01
Epoch 36 Train Time 11.047162771224976s

No improvement. Patience: 2/40
Training epoch 37, Batch 49/98: LR=7.72e-04, Loss=2.17488e-01 LER=7.886e-02
***Loss=2.17488e-01
Training epoch 37, Batch 98/98: LR=7.72e-04, Loss=2.14199e-01 LER=7.722e-02
***Loss=2.14199e-01
Epoch 37 Train Time 11.1169753074646s

Model Saved - New best loss: 2.14199e-01
Training epoch 38, Batch 49/98: LR=7.70e-04, Loss=2.11069e-01 LER=7.667e-02
***Loss=2.11069e-01
Training epoch 38, Batch 98/98: LR=7.70e-04, Loss=2.12338e-01 LER=7.642e-02
***Loss=2.12338e-01
Epoch 38 Train Time 11.090750932693481s

Model Saved - New best loss: 2.12338e-01
Training epoch 39, Batch 49/98: LR=7.69e-04, Loss=2.09102e-01 LER=7.585e-02
***Loss=2.09102e-01
Training epoch 39, Batch 98/98: LR=7.69e-04, Loss=2.09007e-01 LER=7.509e-02
***Loss=2.09007e-01
Epoch 39 Train Time 11.222415924072266s

Model Saved - New best loss: 2.09007e-01
Training epoch 40, Batch 49/98: LR=7.67e-04, Loss=2.13848e-01 LER=7.805e-02
***Loss=2.13848e-01
Training epoch 40, Batch 98/98: LR=7.67e-04, Loss=2.13126e-01 LER=7.689e-02
***Loss=2.13126e-01
Epoch 40 Train Time 11.467212438583374s

No improvement. Patience: 1/40
Test LER  p=7.00e-02: 4.10e-02 p=8.00e-02: 5.92e-02 p=9.00e-02: 7.36e-02 p=1.00e-01: 8.99e-02 p=1.10e-01: 1.16e-01
Mean LER = 7.604e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.507259130477905 s

Training epoch 41, Batch 49/98: LR=7.65e-04, Loss=2.08856e-01 LER=7.541e-02
***Loss=2.08856e-01
Training epoch 41, Batch 98/98: LR=7.65e-04, Loss=2.09021e-01 LER=7.548e-02
***Loss=2.09021e-01
Epoch 41 Train Time 10.905309677124023s

No improvement. Patience: 2/40
Training epoch 42, Batch 49/98: LR=7.64e-04, Loss=2.06208e-01 LER=7.605e-02
***Loss=2.06208e-01
Training epoch 42, Batch 98/98: LR=7.64e-04, Loss=2.06589e-01 LER=7.486e-02
***Loss=2.06589e-01
Epoch 42 Train Time 11.049957036972046s

Model Saved - New best loss: 2.06589e-01
Training epoch 43, Batch 49/98: LR=7.62e-04, Loss=2.05864e-01 LER=7.354e-02
***Loss=2.05864e-01
Training epoch 43, Batch 98/98: LR=7.62e-04, Loss=2.04460e-01 LER=7.233e-02
***Loss=2.04460e-01
Epoch 43 Train Time 10.885387182235718s

Model Saved - New best loss: 2.04460e-01
Training epoch 44, Batch 49/98: LR=7.60e-04, Loss=2.04986e-01 LER=7.396e-02
***Loss=2.04986e-01
Training epoch 44, Batch 98/98: LR=7.60e-04, Loss=2.04728e-01 LER=7.324e-02
***Loss=2.04728e-01
Epoch 44 Train Time 11.176657438278198s

No improvement. Patience: 1/40
Training epoch 45, Batch 49/98: LR=7.58e-04, Loss=2.02357e-01 LER=7.219e-02
***Loss=2.02357e-01
Training epoch 45, Batch 98/98: LR=7.58e-04, Loss=2.01358e-01 LER=7.190e-02
***Loss=2.01358e-01
Epoch 45 Train Time 11.020456314086914s

Model Saved - New best loss: 2.01358e-01
Training epoch 46, Batch 49/98: LR=7.56e-04, Loss=2.05346e-01 LER=7.380e-02
***Loss=2.05346e-01
Training epoch 46, Batch 98/98: LR=7.56e-04, Loss=2.02116e-01 LER=7.257e-02
***Loss=2.02116e-01
Epoch 46 Train Time 11.05800461769104s

No improvement. Patience: 1/40
Training epoch 47, Batch 49/98: LR=7.55e-04, Loss=2.01611e-01 LER=7.189e-02
***Loss=2.01611e-01
Training epoch 47, Batch 98/98: LR=7.55e-04, Loss=2.02277e-01 LER=7.236e-02
***Loss=2.02277e-01
Epoch 47 Train Time 10.733965158462524s

No improvement. Patience: 2/40
Training epoch 48, Batch 49/98: LR=7.53e-04, Loss=2.05095e-01 LER=7.416e-02
***Loss=2.05095e-01
Training epoch 48, Batch 98/98: LR=7.53e-04, Loss=2.03807e-01 LER=7.340e-02
***Loss=2.03807e-01
Epoch 48 Train Time 11.62748122215271s

No improvement. Patience: 3/40
Training epoch 49, Batch 49/98: LR=7.51e-04, Loss=2.05058e-01 LER=7.456e-02
***Loss=2.05058e-01
Training epoch 49, Batch 98/98: LR=7.51e-04, Loss=2.03984e-01 LER=7.318e-02
***Loss=2.03984e-01
Epoch 49 Train Time 11.00761103630066s

No improvement. Patience: 4/40
Training epoch 50, Batch 49/98: LR=7.49e-04, Loss=2.00702e-01 LER=7.274e-02
***Loss=2.00702e-01
Training epoch 50, Batch 98/98: LR=7.49e-04, Loss=2.00763e-01 LER=7.264e-02
***Loss=2.00763e-01
Epoch 50 Train Time 10.635635137557983s

Model Saved - New best loss: 2.00763e-01
Test LER  p=7.00e-02: 4.08e-02 p=8.00e-02: 5.37e-02 p=9.00e-02: 7.17e-02 p=1.00e-01: 9.75e-02 p=1.10e-01: 1.09e-01
Mean LER = 7.449e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.589630126953125 s

Training epoch 51, Batch 49/98: LR=7.46e-04, Loss=1.97704e-01 LER=7.065e-02
***Loss=1.97704e-01
Training epoch 51, Batch 98/98: LR=7.46e-04, Loss=1.98371e-01 LER=7.099e-02
***Loss=1.98371e-01
Epoch 51 Train Time 11.004730701446533s

Model Saved - New best loss: 1.98371e-01
Training epoch 52, Batch 49/98: LR=7.44e-04, Loss=1.98260e-01 LER=7.037e-02
***Loss=1.98260e-01
Training epoch 52, Batch 98/98: LR=7.44e-04, Loss=1.98811e-01 LER=7.129e-02
***Loss=1.98811e-01
Epoch 52 Train Time 10.980839252471924s

No improvement. Patience: 1/40
Training epoch 53, Batch 49/98: LR=7.42e-04, Loss=1.97865e-01 LER=7.083e-02
***Loss=1.97865e-01
Training epoch 53, Batch 98/98: LR=7.42e-04, Loss=2.00040e-01 LER=7.196e-02
***Loss=2.00040e-01
Epoch 53 Train Time 10.793083429336548s

No improvement. Patience: 2/40
Training epoch 54, Batch 49/98: LR=7.40e-04, Loss=1.97783e-01 LER=7.139e-02
***Loss=1.97783e-01
Training epoch 54, Batch 98/98: LR=7.40e-04, Loss=1.98850e-01 LER=7.217e-02
***Loss=1.98850e-01
Epoch 54 Train Time 11.197036027908325s

No improvement. Patience: 3/40
Training epoch 55, Batch 49/98: LR=7.38e-04, Loss=1.94723e-01 LER=6.852e-02
***Loss=1.94723e-01
Training epoch 55, Batch 98/98: LR=7.38e-04, Loss=1.96206e-01 LER=7.005e-02
***Loss=1.96206e-01
Epoch 55 Train Time 10.93727731704712s

Model Saved - New best loss: 1.96206e-01
Training epoch 56, Batch 49/98: LR=7.36e-04, Loss=1.98675e-01 LER=7.336e-02
***Loss=1.98675e-01
Training epoch 56, Batch 98/98: LR=7.36e-04, Loss=1.96288e-01 LER=7.150e-02
***Loss=1.96288e-01
Epoch 56 Train Time 11.325706005096436s

No improvement. Patience: 1/40
Training epoch 57, Batch 49/98: LR=7.33e-04, Loss=1.98638e-01 LER=7.139e-02
***Loss=1.98638e-01
Training epoch 57, Batch 98/98: LR=7.33e-04, Loss=1.94649e-01 LER=7.002e-02
***Loss=1.94649e-01
Epoch 57 Train Time 10.823432922363281s

Model Saved - New best loss: 1.94649e-01
Training epoch 58, Batch 49/98: LR=7.31e-04, Loss=1.88293e-01 LER=6.854e-02
***Loss=1.88293e-01
Training epoch 58, Batch 98/98: LR=7.31e-04, Loss=1.92156e-01 LER=6.937e-02
***Loss=1.92156e-01
Epoch 58 Train Time 10.819950580596924s

Model Saved - New best loss: 1.92156e-01
Training epoch 59, Batch 49/98: LR=7.29e-04, Loss=1.95247e-01 LER=7.003e-02
***Loss=1.95247e-01
Training epoch 59, Batch 98/98: LR=7.29e-04, Loss=1.95827e-01 LER=6.989e-02
***Loss=1.95827e-01
Epoch 59 Train Time 10.895549535751343s

No improvement. Patience: 1/40
Training epoch 60, Batch 49/98: LR=7.26e-04, Loss=1.95217e-01 LER=7.105e-02
***Loss=1.95217e-01
Training epoch 60, Batch 98/98: LR=7.26e-04, Loss=1.93551e-01 LER=7.014e-02
***Loss=1.93551e-01
Epoch 60 Train Time 11.355384588241577s

No improvement. Patience: 2/40
Test LER  p=7.00e-02: 3.83e-02 p=8.00e-02: 5.28e-02 p=9.00e-02: 6.79e-02 p=1.00e-01: 8.53e-02 p=1.10e-01: 1.04e-01
Mean LER = 6.961e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.901082515716553 s

Training epoch 61, Batch 49/98: LR=7.24e-04, Loss=1.93452e-01 LER=6.942e-02
***Loss=1.93452e-01
Training epoch 61, Batch 98/98: LR=7.24e-04, Loss=1.92872e-01 LER=6.926e-02
***Loss=1.92872e-01
Epoch 61 Train Time 10.88755202293396s

No improvement. Patience: 3/40
Training epoch 62, Batch 49/98: LR=7.21e-04, Loss=1.97916e-01 LER=7.139e-02
***Loss=1.97916e-01
Training epoch 62, Batch 98/98: LR=7.21e-04, Loss=1.92575e-01 LER=6.960e-02
***Loss=1.92575e-01
Epoch 62 Train Time 11.191519737243652s

No improvement. Patience: 4/40
Training epoch 63, Batch 49/98: LR=7.19e-04, Loss=1.95259e-01 LER=7.041e-02
***Loss=1.95259e-01
Training epoch 63, Batch 98/98: LR=7.19e-04, Loss=1.92640e-01 LER=6.935e-02
***Loss=1.92640e-01
Epoch 63 Train Time 11.011005640029907s

No improvement. Patience: 5/40
Training epoch 64, Batch 49/98: LR=7.16e-04, Loss=1.92099e-01 LER=6.912e-02
***Loss=1.92099e-01
Training epoch 64, Batch 98/98: LR=7.16e-04, Loss=1.92041e-01 LER=6.858e-02
***Loss=1.92041e-01
Epoch 64 Train Time 11.179297685623169s

Model Saved - New best loss: 1.92041e-01
Training epoch 65, Batch 49/98: LR=7.14e-04, Loss=1.90141e-01 LER=6.810e-02
***Loss=1.90141e-01
Training epoch 65, Batch 98/98: LR=7.14e-04, Loss=1.92331e-01 LER=6.939e-02
***Loss=1.92331e-01
Epoch 65 Train Time 11.26475977897644s

No improvement. Patience: 1/40
Training epoch 66, Batch 49/98: LR=7.11e-04, Loss=1.91058e-01 LER=6.770e-02
***Loss=1.91058e-01
Training epoch 66, Batch 98/98: LR=7.11e-04, Loss=1.90827e-01 LER=6.862e-02
***Loss=1.90827e-01
Epoch 66 Train Time 11.37398910522461s

Model Saved - New best loss: 1.90827e-01
Training epoch 67, Batch 49/98: LR=7.08e-04, Loss=1.93981e-01 LER=7.029e-02
***Loss=1.93981e-01
Training epoch 67, Batch 98/98: LR=7.08e-04, Loss=1.91677e-01 LER=6.991e-02
***Loss=1.91677e-01
Epoch 67 Train Time 10.874581336975098s

No improvement. Patience: 1/40
Training epoch 68, Batch 49/98: LR=7.06e-04, Loss=1.90872e-01 LER=6.942e-02
***Loss=1.90872e-01
Training epoch 68, Batch 98/98: LR=7.06e-04, Loss=1.91182e-01 LER=6.862e-02
***Loss=1.91182e-01
Epoch 68 Train Time 11.042516708374023s

No improvement. Patience: 2/40
Training epoch 69, Batch 49/98: LR=7.03e-04, Loss=1.91363e-01 LER=6.886e-02
***Loss=1.91363e-01
Training epoch 69, Batch 98/98: LR=7.03e-04, Loss=1.91062e-01 LER=6.892e-02
***Loss=1.91062e-01
Epoch 69 Train Time 11.227537631988525s

No improvement. Patience: 3/40
Training epoch 70, Batch 49/98: LR=7.00e-04, Loss=1.89577e-01 LER=6.826e-02
***Loss=1.89577e-01
Training epoch 70, Batch 98/98: LR=7.00e-04, Loss=1.89204e-01 LER=6.771e-02
***Loss=1.89204e-01
Epoch 70 Train Time 10.791961669921875s

Model Saved - New best loss: 1.89204e-01
Test LER  p=7.00e-02: 3.32e-02 p=8.00e-02: 4.90e-02 p=9.00e-02: 6.37e-02 p=1.00e-01: 8.13e-02 p=1.10e-01: 1.03e-01
Mean LER = 6.613e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.513964891433716 s

Training epoch 71, Batch 49/98: LR=6.97e-04, Loss=1.89171e-01 LER=6.862e-02
***Loss=1.89171e-01
Training epoch 71, Batch 98/98: LR=6.97e-04, Loss=1.89044e-01 LER=6.786e-02
***Loss=1.89044e-01
Epoch 71 Train Time 10.964243412017822s

Model Saved - New best loss: 1.89044e-01
Training epoch 72, Batch 49/98: LR=6.95e-04, Loss=1.85821e-01 LER=6.655e-02
***Loss=1.85821e-01
Training epoch 72, Batch 98/98: LR=6.95e-04, Loss=1.88588e-01 LER=6.731e-02
***Loss=1.88588e-01
Epoch 72 Train Time 11.308425188064575s

Model Saved - New best loss: 1.88588e-01
Training epoch 73, Batch 49/98: LR=6.92e-04, Loss=1.93161e-01 LER=6.997e-02
***Loss=1.93161e-01
Training epoch 73, Batch 98/98: LR=6.92e-04, Loss=1.91032e-01 LER=6.898e-02
***Loss=1.91032e-01
Epoch 73 Train Time 10.939543724060059s

No improvement. Patience: 1/40
Training epoch 74, Batch 49/98: LR=6.89e-04, Loss=1.87788e-01 LER=6.969e-02
***Loss=1.87788e-01
Training epoch 74, Batch 98/98: LR=6.89e-04, Loss=1.86904e-01 LER=6.734e-02
***Loss=1.86904e-01
Epoch 74 Train Time 11.148974418640137s

Model Saved - New best loss: 1.86904e-01
Training epoch 75, Batch 49/98: LR=6.86e-04, Loss=1.89039e-01 LER=6.768e-02
***Loss=1.89039e-01
Training epoch 75, Batch 98/98: LR=6.86e-04, Loss=1.88211e-01 LER=6.711e-02
***Loss=1.88211e-01
Epoch 75 Train Time 11.264500379562378s

No improvement. Patience: 1/40
Training epoch 76, Batch 49/98: LR=6.83e-04, Loss=1.87345e-01 LER=6.764e-02
***Loss=1.87345e-01
Training epoch 76, Batch 98/98: LR=6.83e-04, Loss=1.86494e-01 LER=6.677e-02
***Loss=1.86494e-01
Epoch 76 Train Time 10.86929726600647s

Model Saved - New best loss: 1.86494e-01
Training epoch 77, Batch 49/98: LR=6.80e-04, Loss=1.94532e-01 LER=7.037e-02
***Loss=1.94532e-01
Training epoch 77, Batch 98/98: LR=6.80e-04, Loss=1.91633e-01 LER=6.931e-02
***Loss=1.91633e-01
Epoch 77 Train Time 11.14830207824707s

No improvement. Patience: 1/40
Training epoch 78, Batch 49/98: LR=6.77e-04, Loss=1.88750e-01 LER=6.844e-02
***Loss=1.88750e-01
Training epoch 78, Batch 98/98: LR=6.77e-04, Loss=1.86009e-01 LER=6.737e-02
***Loss=1.86009e-01
Epoch 78 Train Time 11.190199851989746s

Model Saved - New best loss: 1.86009e-01
Training epoch 79, Batch 49/98: LR=6.74e-04, Loss=1.82480e-01 LER=6.455e-02
***Loss=1.82480e-01
Training epoch 79, Batch 98/98: LR=6.74e-04, Loss=1.83772e-01 LER=6.624e-02
***Loss=1.83772e-01
Epoch 79 Train Time 11.141075611114502s

Model Saved - New best loss: 1.83772e-01
Training epoch 80, Batch 49/98: LR=6.71e-04, Loss=1.88473e-01 LER=6.860e-02
***Loss=1.88473e-01
Training epoch 80, Batch 98/98: LR=6.71e-04, Loss=1.87156e-01 LER=6.770e-02
***Loss=1.87156e-01
Epoch 80 Train Time 10.995607137680054s

No improvement. Patience: 1/40
Test LER  p=7.00e-02: 3.68e-02 p=8.00e-02: 5.28e-02 p=9.00e-02: 6.72e-02 p=1.00e-01: 8.75e-02 p=1.10e-01: 1.03e-01
Mean LER = 6.953e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.430742979049683 s

Training epoch 81, Batch 49/98: LR=6.68e-04, Loss=1.85953e-01 LER=6.653e-02
***Loss=1.85953e-01
Training epoch 81, Batch 98/98: LR=6.68e-04, Loss=1.87106e-01 LER=6.731e-02
***Loss=1.87106e-01
Epoch 81 Train Time 10.877524614334106s

No improvement. Patience: 2/40
Training epoch 82, Batch 49/98: LR=6.65e-04, Loss=1.82762e-01 LER=6.467e-02
***Loss=1.82762e-01
Training epoch 82, Batch 98/98: LR=6.65e-04, Loss=1.84276e-01 LER=6.554e-02
***Loss=1.84276e-01
Epoch 82 Train Time 10.917627334594727s

No improvement. Patience: 3/40
Training epoch 83, Batch 49/98: LR=6.62e-04, Loss=1.84010e-01 LER=6.565e-02
***Loss=1.84010e-01
Training epoch 83, Batch 98/98: LR=6.62e-04, Loss=1.87836e-01 LER=6.731e-02
***Loss=1.87836e-01
Epoch 83 Train Time 10.990877389907837s

No improvement. Patience: 4/40
Training epoch 84, Batch 49/98: LR=6.58e-04, Loss=1.88900e-01 LER=6.766e-02
***Loss=1.88900e-01
Training epoch 84, Batch 98/98: LR=6.58e-04, Loss=1.86855e-01 LER=6.703e-02
***Loss=1.86855e-01
Epoch 84 Train Time 11.165098667144775s

No improvement. Patience: 5/40
Training epoch 85, Batch 49/98: LR=6.55e-04, Loss=1.85440e-01 LER=6.720e-02
***Loss=1.85440e-01
Training epoch 85, Batch 98/98: LR=6.55e-04, Loss=1.87974e-01 LER=6.781e-02
***Loss=1.87974e-01
Epoch 85 Train Time 10.96323013305664s

No improvement. Patience: 6/40
Training epoch 86, Batch 49/98: LR=6.52e-04, Loss=1.84011e-01 LER=6.569e-02
***Loss=1.84011e-01
Training epoch 86, Batch 98/98: LR=6.52e-04, Loss=1.85875e-01 LER=6.692e-02
***Loss=1.85875e-01
Epoch 86 Train Time 10.724312782287598s

No improvement. Patience: 7/40
Training epoch 87, Batch 49/98: LR=6.49e-04, Loss=1.86539e-01 LER=6.780e-02
***Loss=1.86539e-01
Training epoch 87, Batch 98/98: LR=6.49e-04, Loss=1.85048e-01 LER=6.667e-02
***Loss=1.85048e-01
Epoch 87 Train Time 11.189692735671997s

No improvement. Patience: 8/40
Training epoch 88, Batch 49/98: LR=6.45e-04, Loss=1.84038e-01 LER=6.796e-02
***Loss=1.84038e-01
Training epoch 88, Batch 98/98: LR=6.45e-04, Loss=1.84170e-01 LER=6.757e-02
***Loss=1.84170e-01
Epoch 88 Train Time 11.013639450073242s

No improvement. Patience: 9/40
Training epoch 89, Batch 49/98: LR=6.42e-04, Loss=1.84468e-01 LER=6.682e-02
***Loss=1.84468e-01
Training epoch 89, Batch 98/98: LR=6.42e-04, Loss=1.83852e-01 LER=6.602e-02
***Loss=1.83852e-01
Epoch 89 Train Time 10.677670955657959s

No improvement. Patience: 10/40
Training epoch 90, Batch 49/98: LR=6.39e-04, Loss=1.89284e-01 LER=6.760e-02
***Loss=1.89284e-01
Training epoch 90, Batch 98/98: LR=6.39e-04, Loss=1.87727e-01 LER=6.747e-02
***Loss=1.87727e-01
Epoch 90 Train Time 10.999029159545898s

No improvement. Patience: 11/40
Test LER  p=7.00e-02: 3.66e-02 p=8.00e-02: 5.03e-02 p=9.00e-02: 6.40e-02 p=1.00e-01: 8.63e-02 p=1.10e-01: 9.99e-02
Mean LER = 6.742e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.578290224075317 s

Training epoch 91, Batch 49/98: LR=6.35e-04, Loss=1.86561e-01 LER=6.623e-02
***Loss=1.86561e-01
Training epoch 91, Batch 98/98: LR=6.35e-04, Loss=1.84544e-01 LER=6.612e-02
***Loss=1.84544e-01
Epoch 91 Train Time 10.899307489395142s

No improvement. Patience: 12/40
Training epoch 92, Batch 49/98: LR=6.32e-04, Loss=1.82269e-01 LER=6.497e-02
***Loss=1.82269e-01
Training epoch 92, Batch 98/98: LR=6.32e-04, Loss=1.85287e-01 LER=6.580e-02
***Loss=1.85287e-01
Epoch 92 Train Time 11.1434645652771s

No improvement. Patience: 13/40
Training epoch 93, Batch 49/98: LR=6.29e-04, Loss=1.78431e-01 LER=6.328e-02
***Loss=1.78431e-01
Training epoch 93, Batch 98/98: LR=6.29e-04, Loss=1.81313e-01 LER=6.473e-02
***Loss=1.81313e-01
Epoch 93 Train Time 11.043846607208252s

Model Saved - New best loss: 1.81313e-01
Training epoch 94, Batch 49/98: LR=6.25e-04, Loss=1.82350e-01 LER=6.393e-02
***Loss=1.82350e-01
Training epoch 94, Batch 98/98: LR=6.25e-04, Loss=1.84194e-01 LER=6.559e-02
***Loss=1.84194e-01
Epoch 94 Train Time 11.097772359848022s

No improvement. Patience: 1/40
Training epoch 95, Batch 49/98: LR=6.22e-04, Loss=1.86628e-01 LER=6.619e-02
***Loss=1.86628e-01
Training epoch 95, Batch 98/98: LR=6.22e-04, Loss=1.85858e-01 LER=6.689e-02
***Loss=1.85858e-01
Epoch 95 Train Time 10.922866106033325s

No improvement. Patience: 2/40
Training epoch 96, Batch 49/98: LR=6.18e-04, Loss=1.85895e-01 LER=6.702e-02
***Loss=1.85895e-01
Training epoch 96, Batch 98/98: LR=6.18e-04, Loss=1.84506e-01 LER=6.654e-02
***Loss=1.84506e-01
Epoch 96 Train Time 11.06299114227295s

No improvement. Patience: 3/40
Training epoch 97, Batch 49/98: LR=6.15e-04, Loss=1.85981e-01 LER=6.645e-02
***Loss=1.85981e-01
Training epoch 97, Batch 98/98: LR=6.15e-04, Loss=1.83898e-01 LER=6.639e-02
***Loss=1.83898e-01
Epoch 97 Train Time 11.263846635818481s

No improvement. Patience: 4/40
Training epoch 98, Batch 49/98: LR=6.11e-04, Loss=1.79990e-01 LER=6.499e-02
***Loss=1.79990e-01
Training epoch 98, Batch 98/98: LR=6.11e-04, Loss=1.82321e-01 LER=6.556e-02
***Loss=1.82321e-01
Epoch 98 Train Time 11.226567029953003s

No improvement. Patience: 5/40
Training epoch 99, Batch 49/98: LR=6.07e-04, Loss=1.83545e-01 LER=6.625e-02
***Loss=1.83545e-01
Training epoch 99, Batch 98/98: LR=6.07e-04, Loss=1.82434e-01 LER=6.591e-02
***Loss=1.82434e-01
Epoch 99 Train Time 10.8951895236969s

No improvement. Patience: 6/40
Training epoch 100, Batch 49/98: LR=6.04e-04, Loss=1.80869e-01 LER=6.475e-02
***Loss=1.80869e-01
Training epoch 100, Batch 98/98: LR=6.04e-04, Loss=1.82507e-01 LER=6.576e-02
***Loss=1.82507e-01
Epoch 100 Train Time 10.577741622924805s

No improvement. Patience: 7/40
Test LER  p=7.00e-02: 3.58e-02 p=8.00e-02: 4.96e-02 p=9.00e-02: 6.46e-02 p=1.00e-01: 7.52e-02 p=1.10e-01: 1.04e-01
Mean LER = 6.578e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.55172872543335 s

Training epoch 101, Batch 49/98: LR=6.00e-04, Loss=1.83991e-01 LER=6.593e-02
***Loss=1.83991e-01
Training epoch 101, Batch 98/98: LR=6.00e-04, Loss=1.83040e-01 LER=6.599e-02
***Loss=1.83040e-01
Epoch 101 Train Time 10.79243278503418s

No improvement. Patience: 8/40
Training epoch 102, Batch 49/98: LR=5.97e-04, Loss=1.79097e-01 LER=6.334e-02
***Loss=1.79097e-01
Training epoch 102, Batch 98/98: LR=5.97e-04, Loss=1.83590e-01 LER=6.503e-02
***Loss=1.83590e-01
Epoch 102 Train Time 10.70971393585205s

No improvement. Patience: 9/40
Training epoch 103, Batch 49/98: LR=5.93e-04, Loss=1.83587e-01 LER=6.541e-02
***Loss=1.83587e-01
Training epoch 103, Batch 98/98: LR=5.93e-04, Loss=1.81455e-01 LER=6.562e-02
***Loss=1.81455e-01
Epoch 103 Train Time 10.670923471450806s

No improvement. Patience: 10/40
Training epoch 104, Batch 49/98: LR=5.89e-04, Loss=1.78153e-01 LER=6.533e-02
***Loss=1.78153e-01
Training epoch 104, Batch 98/98: LR=5.89e-04, Loss=1.79080e-01 LER=6.546e-02
***Loss=1.79080e-01
Epoch 104 Train Time 11.541516304016113s

Model Saved - New best loss: 1.79080e-01
Training epoch 105, Batch 49/98: LR=5.86e-04, Loss=1.79314e-01 LER=6.639e-02
***Loss=1.79314e-01
Training epoch 105, Batch 98/98: LR=5.86e-04, Loss=1.80657e-01 LER=6.581e-02
***Loss=1.80657e-01
Epoch 105 Train Time 11.662324666976929s

No improvement. Patience: 1/40
Training epoch 106, Batch 49/98: LR=5.82e-04, Loss=1.80173e-01 LER=6.417e-02
***Loss=1.80173e-01
Training epoch 106, Batch 98/98: LR=5.82e-04, Loss=1.79849e-01 LER=6.403e-02
***Loss=1.79849e-01
Epoch 106 Train Time 11.00927734375s

No improvement. Patience: 2/40
Training epoch 107, Batch 49/98: LR=5.78e-04, Loss=1.82412e-01 LER=6.479e-02
***Loss=1.82412e-01
Training epoch 107, Batch 98/98: LR=5.78e-04, Loss=1.81548e-01 LER=6.514e-02
***Loss=1.81548e-01
Epoch 107 Train Time 10.962119579315186s

No improvement. Patience: 3/40
Training epoch 108, Batch 49/98: LR=5.74e-04, Loss=1.82504e-01 LER=6.603e-02
***Loss=1.82504e-01
Training epoch 108, Batch 98/98: LR=5.74e-04, Loss=1.83400e-01 LER=6.697e-02
***Loss=1.83400e-01
Epoch 108 Train Time 10.828394412994385s

No improvement. Patience: 4/40
Training epoch 109, Batch 49/98: LR=5.71e-04, Loss=1.86311e-01 LER=6.730e-02
***Loss=1.86311e-01
Training epoch 109, Batch 98/98: LR=5.71e-04, Loss=1.84133e-01 LER=6.690e-02
***Loss=1.84133e-01
Epoch 109 Train Time 10.91786813735962s

No improvement. Patience: 5/40
Training epoch 110, Batch 49/98: LR=5.67e-04, Loss=1.77336e-01 LER=6.310e-02
***Loss=1.77336e-01
Training epoch 110, Batch 98/98: LR=5.67e-04, Loss=1.79386e-01 LER=6.425e-02
***Loss=1.79386e-01
Epoch 110 Train Time 11.16267442703247s

No improvement. Patience: 6/40
Test LER  p=7.00e-02: 3.84e-02 p=8.00e-02: 4.74e-02 p=9.00e-02: 6.69e-02 p=1.00e-01: 8.05e-02 p=1.10e-01: 1.02e-01
Mean LER = 6.707e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.942986488342285 s

Training epoch 111, Batch 49/98: LR=5.63e-04, Loss=1.83272e-01 LER=6.603e-02
***Loss=1.83272e-01
Training epoch 111, Batch 98/98: LR=5.63e-04, Loss=1.79542e-01 LER=6.493e-02
***Loss=1.79542e-01
Epoch 111 Train Time 10.55744743347168s

No improvement. Patience: 7/40
Training epoch 112, Batch 49/98: LR=5.59e-04, Loss=1.76418e-01 LER=6.314e-02
***Loss=1.76418e-01
Training epoch 112, Batch 98/98: LR=5.59e-04, Loss=1.78527e-01 LER=6.395e-02
***Loss=1.78527e-01
Epoch 112 Train Time 11.075759887695312s

Model Saved - New best loss: 1.78527e-01
Training epoch 113, Batch 49/98: LR=5.55e-04, Loss=1.78732e-01 LER=6.340e-02
***Loss=1.78732e-01
Training epoch 113, Batch 98/98: LR=5.55e-04, Loss=1.79007e-01 LER=6.393e-02
***Loss=1.79007e-01
Epoch 113 Train Time 11.374895095825195s

No improvement. Patience: 1/40
Training epoch 114, Batch 49/98: LR=5.51e-04, Loss=1.83218e-01 LER=6.659e-02
***Loss=1.83218e-01
Training epoch 114, Batch 98/98: LR=5.51e-04, Loss=1.80468e-01 LER=6.484e-02
***Loss=1.80468e-01
Epoch 114 Train Time 10.895262479782104s

No improvement. Patience: 2/40
Training epoch 115, Batch 49/98: LR=5.48e-04, Loss=1.75625e-01 LER=6.266e-02
***Loss=1.75625e-01
Training epoch 115, Batch 98/98: LR=5.48e-04, Loss=1.78252e-01 LER=6.438e-02
***Loss=1.78252e-01
Epoch 115 Train Time 11.029847145080566s

Model Saved - New best loss: 1.78252e-01
Training epoch 116, Batch 49/98: LR=5.44e-04, Loss=1.79287e-01 LER=6.435e-02
***Loss=1.79287e-01
Training epoch 116, Batch 98/98: LR=5.44e-04, Loss=1.81473e-01 LER=6.540e-02
***Loss=1.81473e-01
Epoch 116 Train Time 11.293361186981201s

No improvement. Patience: 1/40
Training epoch 117, Batch 49/98: LR=5.40e-04, Loss=1.80559e-01 LER=6.469e-02
***Loss=1.80559e-01
Training epoch 117, Batch 98/98: LR=5.40e-04, Loss=1.79701e-01 LER=6.416e-02
***Loss=1.79701e-01
Epoch 117 Train Time 10.800801277160645s

No improvement. Patience: 2/40
Training epoch 118, Batch 49/98: LR=5.36e-04, Loss=1.77597e-01 LER=6.290e-02
***Loss=1.77597e-01
Training epoch 118, Batch 98/98: LR=5.36e-04, Loss=1.79860e-01 LER=6.411e-02
***Loss=1.79860e-01
Epoch 118 Train Time 10.758968114852905s

No improvement. Patience: 3/40
Training epoch 119, Batch 49/98: LR=5.32e-04, Loss=1.77438e-01 LER=6.306e-02
***Loss=1.77438e-01
Training epoch 119, Batch 98/98: LR=5.32e-04, Loss=1.80068e-01 LER=6.394e-02
***Loss=1.80068e-01
Epoch 119 Train Time 11.321272850036621s

No improvement. Patience: 4/40
Training epoch 120, Batch 49/98: LR=5.28e-04, Loss=1.79320e-01 LER=6.431e-02
***Loss=1.79320e-01
Training epoch 120, Batch 98/98: LR=5.28e-04, Loss=1.78180e-01 LER=6.368e-02
***Loss=1.78180e-01
Epoch 120 Train Time 11.018271207809448s

Model Saved - New best loss: 1.78180e-01
Test LER  p=7.00e-02: 3.54e-02 p=8.00e-02: 4.40e-02 p=9.00e-02: 6.64e-02 p=1.00e-01: 8.28e-02 p=1.10e-01: 1.02e-01
Mean LER = 6.621e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.546103954315186 s

Training epoch 121, Batch 49/98: LR=5.24e-04, Loss=1.83315e-01 LER=6.782e-02
***Loss=1.83315e-01
Training epoch 121, Batch 98/98: LR=5.24e-04, Loss=1.81486e-01 LER=6.630e-02
***Loss=1.81486e-01
Epoch 121 Train Time 10.833755016326904s

No improvement. Patience: 1/40
Training epoch 122, Batch 49/98: LR=5.20e-04, Loss=1.82660e-01 LER=6.587e-02
***Loss=1.82660e-01
Training epoch 122, Batch 98/98: LR=5.20e-04, Loss=1.80621e-01 LER=6.506e-02
***Loss=1.80621e-01
Epoch 122 Train Time 11.064042806625366s

No improvement. Patience: 2/40
Training epoch 123, Batch 49/98: LR=5.16e-04, Loss=1.87521e-01 LER=6.702e-02
***Loss=1.87521e-01
Training epoch 123, Batch 98/98: LR=5.16e-04, Loss=1.82680e-01 LER=6.583e-02
***Loss=1.82680e-01
Epoch 123 Train Time 10.971977710723877s

No improvement. Patience: 3/40
Training epoch 124, Batch 49/98: LR=5.12e-04, Loss=1.80017e-01 LER=6.461e-02
***Loss=1.80017e-01
Training epoch 124, Batch 98/98: LR=5.12e-04, Loss=1.80215e-01 LER=6.420e-02
***Loss=1.80215e-01
Epoch 124 Train Time 11.3248872756958s

No improvement. Patience: 4/40
Training epoch 125, Batch 49/98: LR=5.08e-04, Loss=1.76504e-01 LER=6.429e-02
***Loss=1.76504e-01
Training epoch 125, Batch 98/98: LR=5.08e-04, Loss=1.76562e-01 LER=6.414e-02
***Loss=1.76562e-01
Epoch 125 Train Time 11.031192064285278s

Model Saved - New best loss: 1.76562e-01
Training epoch 126, Batch 49/98: LR=5.04e-04, Loss=1.79026e-01 LER=6.409e-02
***Loss=1.79026e-01
Training epoch 126, Batch 98/98: LR=5.04e-04, Loss=1.79900e-01 LER=6.454e-02
***Loss=1.79900e-01
Epoch 126 Train Time 10.547398328781128s

No improvement. Patience: 1/40
Training epoch 127, Batch 49/98: LR=5.00e-04, Loss=1.79712e-01 LER=6.477e-02
***Loss=1.79712e-01
Training epoch 127, Batch 98/98: LR=5.00e-04, Loss=1.77956e-01 LER=6.380e-02
***Loss=1.77956e-01
Epoch 127 Train Time 11.247264385223389s

No improvement. Patience: 2/40
Training epoch 128, Batch 49/98: LR=4.96e-04, Loss=1.81527e-01 LER=6.435e-02
***Loss=1.81527e-01
Training epoch 128, Batch 98/98: LR=4.96e-04, Loss=1.79917e-01 LER=6.460e-02
***Loss=1.79917e-01
Epoch 128 Train Time 11.35495924949646s

No improvement. Patience: 3/40
Training epoch 129, Batch 49/98: LR=4.92e-04, Loss=1.80856e-01 LER=6.553e-02
***Loss=1.80856e-01
Training epoch 129, Batch 98/98: LR=4.92e-04, Loss=1.79744e-01 LER=6.463e-02
***Loss=1.79744e-01
Epoch 129 Train Time 11.183364391326904s

No improvement. Patience: 4/40
Training epoch 130, Batch 49/98: LR=4.88e-04, Loss=1.77774e-01 LER=6.467e-02
***Loss=1.77774e-01
Training epoch 130, Batch 98/98: LR=4.88e-04, Loss=1.76937e-01 LER=6.372e-02
***Loss=1.76937e-01
Epoch 130 Train Time 11.077146053314209s

No improvement. Patience: 5/40
Test LER  p=7.00e-02: 3.64e-02 p=8.00e-02: 4.53e-02 p=9.00e-02: 6.62e-02 p=1.00e-01: 7.91e-02 p=1.10e-01: 9.77e-02
Mean LER = 6.494e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.49227213859558 s

Training epoch 131, Batch 49/98: LR=4.84e-04, Loss=1.73995e-01 LER=6.262e-02
***Loss=1.73995e-01
Training epoch 131, Batch 98/98: LR=4.84e-04, Loss=1.78559e-01 LER=6.492e-02
***Loss=1.78559e-01
Epoch 131 Train Time 11.158536911010742s

No improvement. Patience: 6/40
Training epoch 132, Batch 49/98: LR=4.79e-04, Loss=1.77446e-01 LER=6.392e-02
***Loss=1.77446e-01
Training epoch 132, Batch 98/98: LR=4.79e-04, Loss=1.74324e-01 LER=6.275e-02
***Loss=1.74324e-01
Epoch 132 Train Time 10.962785243988037s

Model Saved - New best loss: 1.74324e-01
Training epoch 133, Batch 49/98: LR=4.75e-04, Loss=1.77766e-01 LER=6.413e-02
***Loss=1.77766e-01
Training epoch 133, Batch 98/98: LR=4.75e-04, Loss=1.77990e-01 LER=6.411e-02
***Loss=1.77990e-01
Epoch 133 Train Time 11.32077932357788s

No improvement. Patience: 1/40
Training epoch 134, Batch 49/98: LR=4.71e-04, Loss=1.74437e-01 LER=6.274e-02
***Loss=1.74437e-01
Training epoch 134, Batch 98/98: LR=4.71e-04, Loss=1.76464e-01 LER=6.322e-02
***Loss=1.76464e-01
Epoch 134 Train Time 11.179326057434082s

No improvement. Patience: 2/40
Training epoch 135, Batch 49/98: LR=4.67e-04, Loss=1.76009e-01 LER=6.364e-02
***Loss=1.76009e-01
Training epoch 135, Batch 98/98: LR=4.67e-04, Loss=1.77368e-01 LER=6.432e-02
***Loss=1.77368e-01
Epoch 135 Train Time 10.75075101852417s

No improvement. Patience: 3/40
Training epoch 136, Batch 49/98: LR=4.63e-04, Loss=1.79506e-01 LER=6.459e-02
***Loss=1.79506e-01
Training epoch 136, Batch 98/98: LR=4.63e-04, Loss=1.80702e-01 LER=6.519e-02
***Loss=1.80702e-01
Epoch 136 Train Time 10.728626489639282s

No improvement. Patience: 4/40
Training epoch 137, Batch 49/98: LR=4.59e-04, Loss=1.75692e-01 LER=6.386e-02
***Loss=1.75692e-01
Training epoch 137, Batch 98/98: LR=4.59e-04, Loss=1.76180e-01 LER=6.369e-02
***Loss=1.76180e-01
Epoch 137 Train Time 11.52027177810669s

No improvement. Patience: 5/40
Training epoch 138, Batch 49/98: LR=4.55e-04, Loss=1.77058e-01 LER=6.370e-02
***Loss=1.77058e-01
Training epoch 138, Batch 98/98: LR=4.55e-04, Loss=1.78381e-01 LER=6.421e-02
***Loss=1.78381e-01
Epoch 138 Train Time 11.108031272888184s

No improvement. Patience: 6/40
Training epoch 139, Batch 49/98: LR=4.51e-04, Loss=1.78403e-01 LER=6.485e-02
***Loss=1.78403e-01
Training epoch 139, Batch 98/98: LR=4.51e-04, Loss=1.78520e-01 LER=6.434e-02
***Loss=1.78520e-01
Epoch 139 Train Time 11.182515382766724s

No improvement. Patience: 7/40
Training epoch 140, Batch 49/98: LR=4.46e-04, Loss=1.76316e-01 LER=6.336e-02
***Loss=1.76316e-01
Training epoch 140, Batch 98/98: LR=4.46e-04, Loss=1.77608e-01 LER=6.426e-02
***Loss=1.77608e-01
Epoch 140 Train Time 10.856919050216675s

No improvement. Patience: 8/40
Test LER  p=7.00e-02: 3.05e-02 p=8.00e-02: 5.10e-02 p=9.00e-02: 6.32e-02 p=1.00e-01: 7.98e-02 p=1.10e-01: 9.58e-02
Mean LER = 6.404e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.31967329978943 s

Training epoch 141, Batch 49/98: LR=4.42e-04, Loss=1.81163e-01 LER=6.493e-02
***Loss=1.81163e-01
Training epoch 141, Batch 98/98: LR=4.42e-04, Loss=1.79612e-01 LER=6.435e-02
***Loss=1.79612e-01
Epoch 141 Train Time 11.279430150985718s

No improvement. Patience: 9/40
Training epoch 142, Batch 49/98: LR=4.38e-04, Loss=1.75578e-01 LER=6.324e-02
***Loss=1.75578e-01
Training epoch 142, Batch 98/98: LR=4.38e-04, Loss=1.76476e-01 LER=6.381e-02
***Loss=1.76476e-01
Epoch 142 Train Time 10.983566045761108s

No improvement. Patience: 10/40
Training epoch 143, Batch 49/98: LR=4.34e-04, Loss=1.75398e-01 LER=6.366e-02
***Loss=1.75398e-01
Training epoch 143, Batch 98/98: LR=4.34e-04, Loss=1.75788e-01 LER=6.408e-02
***Loss=1.75788e-01
Epoch 143 Train Time 10.483615636825562s

No improvement. Patience: 11/40
Training epoch 144, Batch 49/98: LR=4.30e-04, Loss=1.78639e-01 LER=6.429e-02
***Loss=1.78639e-01
Training epoch 144, Batch 98/98: LR=4.30e-04, Loss=1.77475e-01 LER=6.430e-02
***Loss=1.77475e-01
Epoch 144 Train Time 10.74045705795288s

No improvement. Patience: 12/40
Training epoch 145, Batch 49/98: LR=4.26e-04, Loss=1.76634e-01 LER=6.477e-02
***Loss=1.76634e-01
Training epoch 145, Batch 98/98: LR=4.26e-04, Loss=1.78575e-01 LER=6.472e-02
***Loss=1.78575e-01
Epoch 145 Train Time 11.061472654342651s

No improvement. Patience: 13/40
Training epoch 146, Batch 49/98: LR=4.21e-04, Loss=1.74689e-01 LER=6.184e-02
***Loss=1.74689e-01
Training epoch 146, Batch 98/98: LR=4.21e-04, Loss=1.75620e-01 LER=6.249e-02
***Loss=1.75620e-01
Epoch 146 Train Time 10.790471076965332s

No improvement. Patience: 14/40
Training epoch 147, Batch 49/98: LR=4.17e-04, Loss=1.73816e-01 LER=6.250e-02
***Loss=1.73816e-01
Training epoch 147, Batch 98/98: LR=4.17e-04, Loss=1.73744e-01 LER=6.225e-02
***Loss=1.73744e-01
Epoch 147 Train Time 11.26707649230957s

Model Saved - New best loss: 1.73744e-01
Training epoch 148, Batch 49/98: LR=4.13e-04, Loss=1.75049e-01 LER=6.284e-02
***Loss=1.75049e-01
Training epoch 148, Batch 98/98: LR=4.13e-04, Loss=1.77700e-01 LER=6.363e-02
***Loss=1.77700e-01
Epoch 148 Train Time 11.136842727661133s

No improvement. Patience: 1/40
Training epoch 149, Batch 49/98: LR=4.09e-04, Loss=1.78113e-01 LER=6.388e-02
***Loss=1.78113e-01
Training epoch 149, Batch 98/98: LR=4.09e-04, Loss=1.76466e-01 LER=6.304e-02
***Loss=1.76466e-01
Epoch 149 Train Time 10.956700086593628s

No improvement. Patience: 2/40
Training epoch 150, Batch 49/98: LR=4.05e-04, Loss=1.78441e-01 LER=6.431e-02
***Loss=1.78441e-01
Training epoch 150, Batch 98/98: LR=4.05e-04, Loss=1.77803e-01 LER=6.482e-02
***Loss=1.77803e-01
Epoch 150 Train Time 10.904998064041138s

No improvement. Patience: 3/40
Test LER  p=7.00e-02: 3.26e-02 p=8.00e-02: 4.57e-02 p=9.00e-02: 6.32e-02 p=1.00e-01: 8.09e-02 p=1.10e-01: 1.02e-01
Mean LER = 6.494e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.634037256240845 s

Training epoch 151, Batch 49/98: LR=4.00e-04, Loss=1.75869e-01 LER=6.302e-02
***Loss=1.75869e-01
Training epoch 151, Batch 98/98: LR=4.00e-04, Loss=1.75668e-01 LER=6.368e-02
***Loss=1.75668e-01
Epoch 151 Train Time 10.799014568328857s

No improvement. Patience: 4/40
Training epoch 152, Batch 49/98: LR=3.96e-04, Loss=1.74914e-01 LER=6.206e-02
***Loss=1.74914e-01
Training epoch 152, Batch 98/98: LR=3.96e-04, Loss=1.74613e-01 LER=6.302e-02
***Loss=1.74613e-01
Epoch 152 Train Time 10.828542232513428s

No improvement. Patience: 5/40
Training epoch 153, Batch 49/98: LR=3.92e-04, Loss=1.77947e-01 LER=6.399e-02
***Loss=1.77947e-01
Training epoch 153, Batch 98/98: LR=3.92e-04, Loss=1.76460e-01 LER=6.384e-02
***Loss=1.76460e-01
Epoch 153 Train Time 11.0935218334198s

No improvement. Patience: 6/40
Training epoch 154, Batch 49/98: LR=3.88e-04, Loss=1.78472e-01 LER=6.505e-02
***Loss=1.78472e-01
Training epoch 154, Batch 98/98: LR=3.88e-04, Loss=1.77830e-01 LER=6.467e-02
***Loss=1.77830e-01
Epoch 154 Train Time 11.320562601089478s

No improvement. Patience: 7/40
Training epoch 155, Batch 49/98: LR=3.84e-04, Loss=1.81331e-01 LER=6.595e-02
***Loss=1.81331e-01
Training epoch 155, Batch 98/98: LR=3.84e-04, Loss=1.77779e-01 LER=6.449e-02
***Loss=1.77779e-01
Epoch 155 Train Time 11.018613576889038s

No improvement. Patience: 8/40
Training epoch 156, Batch 49/98: LR=3.80e-04, Loss=1.75771e-01 LER=6.284e-02
***Loss=1.75771e-01
Training epoch 156, Batch 98/98: LR=3.80e-04, Loss=1.76720e-01 LER=6.333e-02
***Loss=1.76720e-01
Epoch 156 Train Time 11.09787130355835s

No improvement. Patience: 9/40
Training epoch 157, Batch 49/98: LR=3.75e-04, Loss=1.80753e-01 LER=6.569e-02
***Loss=1.80753e-01
Training epoch 157, Batch 98/98: LR=3.75e-04, Loss=1.78253e-01 LER=6.439e-02
***Loss=1.78253e-01
Epoch 157 Train Time 11.042421340942383s

No improvement. Patience: 10/40
Training epoch 158, Batch 49/98: LR=3.71e-04, Loss=1.75088e-01 LER=6.360e-02
***Loss=1.75088e-01
Training epoch 158, Batch 98/98: LR=3.71e-04, Loss=1.73290e-01 LER=6.257e-02
***Loss=1.73290e-01
Epoch 158 Train Time 11.089092016220093s

Model Saved - New best loss: 1.73290e-01
Training epoch 159, Batch 49/98: LR=3.67e-04, Loss=1.71295e-01 LER=6.250e-02
***Loss=1.71295e-01
Training epoch 159, Batch 98/98: LR=3.67e-04, Loss=1.73332e-01 LER=6.309e-02
***Loss=1.73332e-01
Epoch 159 Train Time 11.124216079711914s

No improvement. Patience: 1/40
Training epoch 160, Batch 49/98: LR=3.63e-04, Loss=1.69260e-01 LER=6.049e-02
***Loss=1.69260e-01
Training epoch 160, Batch 98/98: LR=3.63e-04, Loss=1.69778e-01 LER=6.123e-02
***Loss=1.69778e-01
Epoch 160 Train Time 10.917450189590454s

Model Saved - New best loss: 1.69778e-01
Test LER  p=7.00e-02: 3.32e-02 p=8.00e-02: 4.98e-02 p=9.00e-02: 6.13e-02 p=1.00e-01: 7.00e-02 p=1.10e-01: 1.00e-01
Mean LER = 6.295e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.538734197616577 s

Training epoch 161, Batch 49/98: LR=3.59e-04, Loss=1.76361e-01 LER=6.459e-02
***Loss=1.76361e-01
Training epoch 161, Batch 98/98: LR=3.59e-04, Loss=1.75009e-01 LER=6.308e-02
***Loss=1.75009e-01
Epoch 161 Train Time 10.901854991912842s

No improvement. Patience: 1/40
Training epoch 162, Batch 49/98: LR=3.55e-04, Loss=1.74442e-01 LER=6.328e-02
***Loss=1.74442e-01
Training epoch 162, Batch 98/98: LR=3.55e-04, Loss=1.74914e-01 LER=6.347e-02
***Loss=1.74914e-01
Epoch 162 Train Time 11.072952270507812s

No improvement. Patience: 2/40
Training epoch 163, Batch 49/98: LR=3.50e-04, Loss=1.73460e-01 LER=6.300e-02
***Loss=1.73460e-01
Training epoch 163, Batch 98/98: LR=3.50e-04, Loss=1.73379e-01 LER=6.272e-02
***Loss=1.73379e-01
Epoch 163 Train Time 11.060613632202148s

No improvement. Patience: 3/40
Training epoch 164, Batch 49/98: LR=3.46e-04, Loss=1.76478e-01 LER=6.346e-02
***Loss=1.76478e-01
Training epoch 164, Batch 98/98: LR=3.46e-04, Loss=1.76514e-01 LER=6.348e-02
***Loss=1.76514e-01
Epoch 164 Train Time 11.050366640090942s

No improvement. Patience: 4/40
Training epoch 165, Batch 49/98: LR=3.42e-04, Loss=1.74393e-01 LER=6.447e-02
***Loss=1.74393e-01
Training epoch 165, Batch 98/98: LR=3.42e-04, Loss=1.74399e-01 LER=6.349e-02
***Loss=1.74399e-01
Epoch 165 Train Time 11.079180240631104s

No improvement. Patience: 5/40
Training epoch 166, Batch 49/98: LR=3.38e-04, Loss=1.73316e-01 LER=6.284e-02
***Loss=1.73316e-01
Training epoch 166, Batch 98/98: LR=3.38e-04, Loss=1.72671e-01 LER=6.210e-02
***Loss=1.72671e-01
Epoch 166 Train Time 10.981408596038818s

No improvement. Patience: 6/40
Training epoch 167, Batch 49/98: LR=3.34e-04, Loss=1.73267e-01 LER=6.264e-02
***Loss=1.73267e-01
Training epoch 167, Batch 98/98: LR=3.34e-04, Loss=1.71419e-01 LER=6.171e-02
***Loss=1.71419e-01
Epoch 167 Train Time 11.263903141021729s

No improvement. Patience: 7/40
Training epoch 168, Batch 49/98: LR=3.30e-04, Loss=1.74738e-01 LER=6.218e-02
***Loss=1.74738e-01
Training epoch 168, Batch 98/98: LR=3.30e-04, Loss=1.72422e-01 LER=6.181e-02
***Loss=1.72422e-01
Epoch 168 Train Time 10.803805112838745s

No improvement. Patience: 8/40
Training epoch 169, Batch 49/98: LR=3.26e-04, Loss=1.75724e-01 LER=6.332e-02
***Loss=1.75724e-01
Training epoch 169, Batch 98/98: LR=3.26e-04, Loss=1.73892e-01 LER=6.228e-02
***Loss=1.73892e-01
Epoch 169 Train Time 11.785727262496948s

No improvement. Patience: 9/40
Training epoch 170, Batch 49/98: LR=3.22e-04, Loss=1.77031e-01 LER=6.411e-02
***Loss=1.77031e-01
Training epoch 170, Batch 98/98: LR=3.22e-04, Loss=1.75161e-01 LER=6.359e-02
***Loss=1.75161e-01
Epoch 170 Train Time 11.144677639007568s

No improvement. Patience: 10/40
Test LER  p=7.00e-02: 3.57e-02 p=8.00e-02: 4.42e-02 p=9.00e-02: 5.81e-02 p=1.00e-01: 8.00e-02 p=1.10e-01: 9.68e-02
Mean LER = 6.297e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.496503591537476 s

Training epoch 171, Batch 49/98: LR=3.17e-04, Loss=1.73690e-01 LER=6.274e-02
***Loss=1.73690e-01
Training epoch 171, Batch 98/98: LR=3.17e-04, Loss=1.71756e-01 LER=6.250e-02
***Loss=1.71756e-01
Epoch 171 Train Time 10.767098188400269s

No improvement. Patience: 11/40
Training epoch 172, Batch 49/98: LR=3.13e-04, Loss=1.71713e-01 LER=6.085e-02
***Loss=1.71713e-01
Training epoch 172, Batch 98/98: LR=3.13e-04, Loss=1.73261e-01 LER=6.203e-02
***Loss=1.73261e-01
Epoch 172 Train Time 11.025114059448242s

No improvement. Patience: 12/40
Training epoch 173, Batch 49/98: LR=3.09e-04, Loss=1.72009e-01 LER=6.226e-02
***Loss=1.72009e-01
Training epoch 173, Batch 98/98: LR=3.09e-04, Loss=1.72280e-01 LER=6.230e-02
***Loss=1.72280e-01
Epoch 173 Train Time 11.312939643859863s

No improvement. Patience: 13/40
Training epoch 174, Batch 49/98: LR=3.05e-04, Loss=1.69102e-01 LER=6.114e-02
***Loss=1.69102e-01
Training epoch 174, Batch 98/98: LR=3.05e-04, Loss=1.70083e-01 LER=6.169e-02
***Loss=1.70083e-01
Epoch 174 Train Time 11.332040071487427s

No improvement. Patience: 14/40
Training epoch 175, Batch 49/98: LR=3.01e-04, Loss=1.71621e-01 LER=6.033e-02
***Loss=1.71621e-01
Training epoch 175, Batch 98/98: LR=3.01e-04, Loss=1.73267e-01 LER=6.178e-02
***Loss=1.73267e-01
Epoch 175 Train Time 10.850843906402588s

No improvement. Patience: 15/40
Training epoch 176, Batch 49/98: LR=2.97e-04, Loss=1.73716e-01 LER=6.350e-02
***Loss=1.73716e-01
Training epoch 176, Batch 98/98: LR=2.97e-04, Loss=1.73948e-01 LER=6.360e-02
***Loss=1.73948e-01
Epoch 176 Train Time 10.89521312713623s

No improvement. Patience: 16/40
Training epoch 177, Batch 49/98: LR=2.93e-04, Loss=1.73747e-01 LER=6.274e-02
***Loss=1.73747e-01
Training epoch 177, Batch 98/98: LR=2.93e-04, Loss=1.71836e-01 LER=6.260e-02
***Loss=1.71836e-01
Epoch 177 Train Time 11.274004220962524s

No improvement. Patience: 17/40
Training epoch 178, Batch 49/98: LR=2.89e-04, Loss=1.67372e-01 LER=6.154e-02
***Loss=1.67372e-01
Training epoch 178, Batch 98/98: LR=2.89e-04, Loss=1.69885e-01 LER=6.165e-02
***Loss=1.69885e-01
Epoch 178 Train Time 11.161952257156372s

No improvement. Patience: 18/40
Training epoch 179, Batch 49/98: LR=2.85e-04, Loss=1.70452e-01 LER=6.087e-02
***Loss=1.70452e-01
Training epoch 179, Batch 98/98: LR=2.85e-04, Loss=1.71492e-01 LER=6.163e-02
***Loss=1.71492e-01
Epoch 179 Train Time 11.078948020935059s

No improvement. Patience: 19/40
Training epoch 180, Batch 49/98: LR=2.81e-04, Loss=1.75307e-01 LER=6.326e-02
***Loss=1.75307e-01
Training epoch 180, Batch 98/98: LR=2.81e-04, Loss=1.73090e-01 LER=6.271e-02
***Loss=1.73090e-01
Epoch 180 Train Time 11.438807010650635s

No improvement. Patience: 20/40
Test LER  p=7.00e-02: 3.08e-02 p=8.00e-02: 4.55e-02 p=9.00e-02: 5.93e-02 p=1.00e-01: 7.25e-02 p=1.10e-01: 9.36e-02
Mean LER = 6.031e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.44636821746826 s

Training epoch 181, Batch 49/98: LR=2.77e-04, Loss=1.73179e-01 LER=6.304e-02
***Loss=1.73179e-01
Training epoch 181, Batch 98/98: LR=2.77e-04, Loss=1.71638e-01 LER=6.225e-02
***Loss=1.71638e-01
Epoch 181 Train Time 11.265645265579224s

No improvement. Patience: 21/40
Training epoch 182, Batch 49/98: LR=2.73e-04, Loss=1.73305e-01 LER=6.144e-02
***Loss=1.73305e-01
Training epoch 182, Batch 98/98: LR=2.73e-04, Loss=1.72346e-01 LER=6.184e-02
***Loss=1.72346e-01
Epoch 182 Train Time 11.172251462936401s

No improvement. Patience: 22/40
Training epoch 183, Batch 49/98: LR=2.69e-04, Loss=1.75427e-01 LER=6.302e-02
***Loss=1.75427e-01
Training epoch 183, Batch 98/98: LR=2.69e-04, Loss=1.74766e-01 LER=6.260e-02
***Loss=1.74766e-01
Epoch 183 Train Time 11.3666832447052s

No improvement. Patience: 23/40
Training epoch 184, Batch 49/98: LR=2.65e-04, Loss=1.72736e-01 LER=6.342e-02
***Loss=1.72736e-01
Training epoch 184, Batch 98/98: LR=2.65e-04, Loss=1.72693e-01 LER=6.266e-02
***Loss=1.72693e-01
Epoch 184 Train Time 11.463647842407227s

No improvement. Patience: 24/40
Training epoch 185, Batch 49/98: LR=2.61e-04, Loss=1.72956e-01 LER=6.326e-02
***Loss=1.72956e-01
Training epoch 185, Batch 98/98: LR=2.61e-04, Loss=1.73536e-01 LER=6.284e-02
***Loss=1.73536e-01
Epoch 185 Train Time 11.388892889022827s

No improvement. Patience: 25/40
Training epoch 186, Batch 49/98: LR=2.57e-04, Loss=1.73920e-01 LER=6.306e-02
***Loss=1.73920e-01
Training epoch 186, Batch 98/98: LR=2.57e-04, Loss=1.74879e-01 LER=6.329e-02
***Loss=1.74879e-01
Epoch 186 Train Time 10.864346504211426s

No improvement. Patience: 26/40
Training epoch 187, Batch 49/98: LR=2.53e-04, Loss=1.71453e-01 LER=6.152e-02
***Loss=1.71453e-01
Training epoch 187, Batch 98/98: LR=2.53e-04, Loss=1.73352e-01 LER=6.199e-02
***Loss=1.73352e-01
Epoch 187 Train Time 10.695235252380371s

No improvement. Patience: 27/40
Training epoch 188, Batch 49/98: LR=2.50e-04, Loss=1.70902e-01 LER=6.126e-02
***Loss=1.70902e-01
Training epoch 188, Batch 98/98: LR=2.50e-04, Loss=1.71681e-01 LER=6.175e-02
***Loss=1.71681e-01
Epoch 188 Train Time 11.071372985839844s

No improvement. Patience: 28/40
Training epoch 189, Batch 49/98: LR=2.46e-04, Loss=1.69335e-01 LER=6.150e-02
***Loss=1.69335e-01
Training epoch 189, Batch 98/98: LR=2.46e-04, Loss=1.72506e-01 LER=6.204e-02
***Loss=1.72506e-01
Epoch 189 Train Time 11.03940224647522s

No improvement. Patience: 29/40
Training epoch 190, Batch 49/98: LR=2.42e-04, Loss=1.73995e-01 LER=6.354e-02
***Loss=1.73995e-01
Training epoch 190, Batch 98/98: LR=2.42e-04, Loss=1.73544e-01 LER=6.331e-02
***Loss=1.73544e-01
Epoch 190 Train Time 11.403482675552368s

No improvement. Patience: 30/40
Test LER  p=7.00e-02: 3.35e-02 p=8.00e-02: 4.68e-02 p=9.00e-02: 5.79e-02 p=1.00e-01: 7.77e-02 p=1.10e-01: 9.45e-02
Mean LER = 6.209e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.468728065490723 s

Training epoch 191, Batch 49/98: LR=2.38e-04, Loss=1.68986e-01 LER=6.061e-02
***Loss=1.68986e-01
Training epoch 191, Batch 98/98: LR=2.38e-04, Loss=1.72669e-01 LER=6.207e-02
***Loss=1.72669e-01
Epoch 191 Train Time 11.275808572769165s

No improvement. Patience: 31/40
Training epoch 192, Batch 49/98: LR=2.34e-04, Loss=1.71291e-01 LER=6.232e-02
***Loss=1.71291e-01
Training epoch 192, Batch 98/98: LR=2.34e-04, Loss=1.72332e-01 LER=6.333e-02
***Loss=1.72332e-01
Epoch 192 Train Time 11.129645586013794s

No improvement. Patience: 32/40
Training epoch 193, Batch 49/98: LR=2.30e-04, Loss=1.73490e-01 LER=6.170e-02
***Loss=1.73490e-01
Training epoch 193, Batch 98/98: LR=2.30e-04, Loss=1.71565e-01 LER=6.135e-02
***Loss=1.71565e-01
Epoch 193 Train Time 11.348222255706787s

No improvement. Patience: 33/40
Training epoch 194, Batch 49/98: LR=2.27e-04, Loss=1.68581e-01 LER=6.114e-02
***Loss=1.68581e-01
Training epoch 194, Batch 98/98: LR=2.27e-04, Loss=1.68741e-01 LER=6.084e-02
***Loss=1.68741e-01
Epoch 194 Train Time 11.247509717941284s

Model Saved - New best loss: 1.68741e-01
Training epoch 195, Batch 49/98: LR=2.23e-04, Loss=1.73229e-01 LER=6.318e-02
***Loss=1.73229e-01
Training epoch 195, Batch 98/98: LR=2.23e-04, Loss=1.71729e-01 LER=6.195e-02
***Loss=1.71729e-01
Epoch 195 Train Time 11.076216459274292s

No improvement. Patience: 1/40
Training epoch 196, Batch 49/98: LR=2.19e-04, Loss=1.72679e-01 LER=6.148e-02
***Loss=1.72679e-01
Training epoch 196, Batch 98/98: LR=2.19e-04, Loss=1.71964e-01 LER=6.184e-02
***Loss=1.71964e-01
Epoch 196 Train Time 10.865358114242554s

No improvement. Patience: 2/40
Training epoch 197, Batch 49/98: LR=2.15e-04, Loss=1.74307e-01 LER=6.276e-02
***Loss=1.74307e-01
Training epoch 197, Batch 98/98: LR=2.15e-04, Loss=1.72558e-01 LER=6.225e-02
***Loss=1.72558e-01
Epoch 197 Train Time 11.187843084335327s

No improvement. Patience: 3/40
Training epoch 198, Batch 49/98: LR=2.12e-04, Loss=1.70210e-01 LER=6.252e-02
***Loss=1.70210e-01
Training epoch 198, Batch 98/98: LR=2.12e-04, Loss=1.70694e-01 LER=6.184e-02
***Loss=1.70694e-01
Epoch 198 Train Time 10.873167037963867s

No improvement. Patience: 4/40
Training epoch 199, Batch 49/98: LR=2.08e-04, Loss=1.71427e-01 LER=6.174e-02
***Loss=1.71427e-01
Training epoch 199, Batch 98/98: LR=2.08e-04, Loss=1.70740e-01 LER=6.123e-02
***Loss=1.70740e-01
Epoch 199 Train Time 11.42190146446228s

No improvement. Patience: 5/40
Training epoch 200, Batch 49/98: LR=2.04e-04, Loss=1.70275e-01 LER=6.214e-02
***Loss=1.70275e-01
Training epoch 200, Batch 98/98: LR=2.04e-04, Loss=1.71782e-01 LER=6.261e-02
***Loss=1.71782e-01
Epoch 200 Train Time 12.027672290802002s

No improvement. Patience: 6/40
Test LER  p=7.00e-02: 3.19e-02 p=8.00e-02: 4.88e-02 p=9.00e-02: 6.22e-02 p=1.00e-01: 7.10e-02 p=1.10e-01: 9.28e-02
Mean LER = 6.135e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.479819297790527 s

Training epoch 201, Batch 49/98: LR=2.01e-04, Loss=1.71082e-01 LER=6.304e-02
***Loss=1.71082e-01
Training epoch 201, Batch 98/98: LR=2.01e-04, Loss=1.71257e-01 LER=6.241e-02
***Loss=1.71257e-01
Epoch 201 Train Time 10.840291500091553s

No improvement. Patience: 7/40
Training epoch 202, Batch 49/98: LR=1.97e-04, Loss=1.67242e-01 LER=5.955e-02
***Loss=1.67242e-01
Training epoch 202, Batch 98/98: LR=1.97e-04, Loss=1.69630e-01 LER=6.112e-02
***Loss=1.69630e-01
Epoch 202 Train Time 11.148768663406372s

No improvement. Patience: 8/40
Training epoch 203, Batch 49/98: LR=1.94e-04, Loss=1.72866e-01 LER=6.284e-02
***Loss=1.72866e-01
Training epoch 203, Batch 98/98: LR=1.94e-04, Loss=1.69850e-01 LER=6.168e-02
***Loss=1.69850e-01
Epoch 203 Train Time 10.88557505607605s

No improvement. Patience: 9/40
Training epoch 204, Batch 49/98: LR=1.90e-04, Loss=1.73219e-01 LER=6.320e-02
***Loss=1.73219e-01
Training epoch 204, Batch 98/98: LR=1.90e-04, Loss=1.72536e-01 LER=6.243e-02
***Loss=1.72536e-01
Epoch 204 Train Time 10.824836492538452s

No improvement. Patience: 10/40
Training epoch 205, Batch 49/98: LR=1.86e-04, Loss=1.72562e-01 LER=6.222e-02
***Loss=1.72562e-01
Training epoch 205, Batch 98/98: LR=1.86e-04, Loss=1.72877e-01 LER=6.320e-02
***Loss=1.72877e-01
Epoch 205 Train Time 10.637278318405151s

No improvement. Patience: 11/40
Training epoch 206, Batch 49/98: LR=1.83e-04, Loss=1.68152e-01 LER=6.105e-02
***Loss=1.68152e-01
Training epoch 206, Batch 98/98: LR=1.83e-04, Loss=1.69822e-01 LER=6.154e-02
***Loss=1.69822e-01
Epoch 206 Train Time 10.865178346633911s

No improvement. Patience: 12/40
Training epoch 207, Batch 49/98: LR=1.79e-04, Loss=1.73463e-01 LER=6.310e-02
***Loss=1.73463e-01
Training epoch 207, Batch 98/98: LR=1.79e-04, Loss=1.71517e-01 LER=6.216e-02
***Loss=1.71517e-01
Epoch 207 Train Time 11.05294418334961s

No improvement. Patience: 13/40
Training epoch 208, Batch 49/98: LR=1.76e-04, Loss=1.73768e-01 LER=6.264e-02
***Loss=1.73768e-01
Training epoch 208, Batch 98/98: LR=1.76e-04, Loss=1.72658e-01 LER=6.231e-02
***Loss=1.72658e-01
Epoch 208 Train Time 11.09919786453247s

No improvement. Patience: 14/40
Training epoch 209, Batch 49/98: LR=1.72e-04, Loss=1.68200e-01 LER=6.158e-02
***Loss=1.68200e-01
Training epoch 209, Batch 98/98: LR=1.72e-04, Loss=1.69487e-01 LER=6.189e-02
***Loss=1.69487e-01
Epoch 209 Train Time 11.219839096069336s

No improvement. Patience: 15/40
Training epoch 210, Batch 49/98: LR=1.69e-04, Loss=1.68803e-01 LER=6.142e-02
***Loss=1.68803e-01
Training epoch 210, Batch 98/98: LR=1.69e-04, Loss=1.67788e-01 LER=6.108e-02
***Loss=1.67788e-01
Epoch 210 Train Time 11.103487968444824s

Model Saved - New best loss: 1.67788e-01
Test LER  p=7.00e-02: 3.35e-02 p=8.00e-02: 4.15e-02 p=9.00e-02: 6.08e-02 p=1.00e-01: 6.89e-02 p=1.10e-01: 8.91e-02
Mean LER = 5.877e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.91321301460266 s

Training epoch 211, Batch 49/98: LR=1.66e-04, Loss=1.70318e-01 LER=6.256e-02
***Loss=1.70318e-01
Training epoch 211, Batch 98/98: LR=1.66e-04, Loss=1.70845e-01 LER=6.229e-02
***Loss=1.70845e-01
Epoch 211 Train Time 11.288593053817749s

No improvement. Patience: 1/40
Training epoch 212, Batch 49/98: LR=1.62e-04, Loss=1.73173e-01 LER=6.178e-02
***Loss=1.73173e-01
Training epoch 212, Batch 98/98: LR=1.62e-04, Loss=1.72161e-01 LER=6.190e-02
***Loss=1.72161e-01
Epoch 212 Train Time 11.108729362487793s

No improvement. Patience: 2/40
Training epoch 213, Batch 49/98: LR=1.59e-04, Loss=1.70972e-01 LER=6.226e-02
***Loss=1.70972e-01
Training epoch 213, Batch 98/98: LR=1.59e-04, Loss=1.71433e-01 LER=6.244e-02
***Loss=1.71433e-01
Epoch 213 Train Time 11.251649379730225s

No improvement. Patience: 3/40
Training epoch 214, Batch 49/98: LR=1.56e-04, Loss=1.71592e-01 LER=6.134e-02
***Loss=1.71592e-01
Training epoch 214, Batch 98/98: LR=1.56e-04, Loss=1.69019e-01 LER=6.046e-02
***Loss=1.69019e-01
Epoch 214 Train Time 11.115060329437256s

No improvement. Patience: 4/40
Training epoch 215, Batch 49/98: LR=1.52e-04, Loss=1.73251e-01 LER=6.330e-02
***Loss=1.73251e-01
Training epoch 215, Batch 98/98: LR=1.52e-04, Loss=1.72983e-01 LER=6.331e-02
***Loss=1.72983e-01
Epoch 215 Train Time 11.085330724716187s

No improvement. Patience: 5/40
Training epoch 216, Batch 49/98: LR=1.49e-04, Loss=1.68225e-01 LER=6.186e-02
***Loss=1.68225e-01
Training epoch 216, Batch 98/98: LR=1.49e-04, Loss=1.69751e-01 LER=6.202e-02
***Loss=1.69751e-01
Epoch 216 Train Time 11.170325517654419s

No improvement. Patience: 6/40
Training epoch 217, Batch 49/98: LR=1.46e-04, Loss=1.67623e-01 LER=5.987e-02
***Loss=1.67623e-01
Training epoch 217, Batch 98/98: LR=1.46e-04, Loss=1.66453e-01 LER=5.999e-02
***Loss=1.66453e-01
Epoch 217 Train Time 11.383317232131958s

Model Saved - New best loss: 1.66453e-01
Training epoch 218, Batch 49/98: LR=1.43e-04, Loss=1.70872e-01 LER=6.164e-02
***Loss=1.70872e-01
Training epoch 218, Batch 98/98: LR=1.43e-04, Loss=1.71748e-01 LER=6.226e-02
***Loss=1.71748e-01
Epoch 218 Train Time 11.488236427307129s

No improvement. Patience: 1/40
Training epoch 219, Batch 49/98: LR=1.39e-04, Loss=1.68874e-01 LER=6.097e-02
***Loss=1.68874e-01
Training epoch 219, Batch 98/98: LR=1.39e-04, Loss=1.70180e-01 LER=6.135e-02
***Loss=1.70180e-01
Epoch 219 Train Time 11.218143224716187s

No improvement. Patience: 2/40
Training epoch 220, Batch 49/98: LR=1.36e-04, Loss=1.68887e-01 LER=6.101e-02
***Loss=1.68887e-01
Training epoch 220, Batch 98/98: LR=1.36e-04, Loss=1.66494e-01 LER=6.044e-02
***Loss=1.66494e-01
Epoch 220 Train Time 10.660745859146118s

No improvement. Patience: 3/40
Test LER  p=7.00e-02: 3.20e-02 p=8.00e-02: 4.66e-02 p=9.00e-02: 6.20e-02 p=1.00e-01: 7.99e-02 p=1.10e-01: 9.80e-02
Mean LER = 6.371e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.603995084762573 s

Training epoch 221, Batch 49/98: LR=1.33e-04, Loss=1.71455e-01 LER=6.258e-02
***Loss=1.71455e-01
Training epoch 221, Batch 98/98: LR=1.33e-04, Loss=1.73398e-01 LER=6.310e-02
***Loss=1.73398e-01
Epoch 221 Train Time 10.92433786392212s

No improvement. Patience: 4/40
Training epoch 222, Batch 49/98: LR=1.30e-04, Loss=1.71371e-01 LER=6.212e-02
***Loss=1.71371e-01
Training epoch 222, Batch 98/98: LR=1.30e-04, Loss=1.71376e-01 LER=6.185e-02
***Loss=1.71376e-01
Epoch 222 Train Time 11.047025203704834s

No improvement. Patience: 5/40
Training epoch 223, Batch 49/98: LR=1.27e-04, Loss=1.73981e-01 LER=6.324e-02
***Loss=1.73981e-01
Training epoch 223, Batch 98/98: LR=1.27e-04, Loss=1.72189e-01 LER=6.255e-02
***Loss=1.72189e-01
Epoch 223 Train Time 10.87960696220398s

No improvement. Patience: 6/40
Training epoch 224, Batch 49/98: LR=1.24e-04, Loss=1.69087e-01 LER=6.107e-02
***Loss=1.69087e-01
Training epoch 224, Batch 98/98: LR=1.24e-04, Loss=1.69955e-01 LER=6.115e-02
***Loss=1.69955e-01
Epoch 224 Train Time 11.384321928024292s

No improvement. Patience: 7/40
Training epoch 225, Batch 49/98: LR=1.21e-04, Loss=1.69313e-01 LER=6.083e-02
***Loss=1.69313e-01
Training epoch 225, Batch 98/98: LR=1.21e-04, Loss=1.70557e-01 LER=6.135e-02
***Loss=1.70557e-01
Epoch 225 Train Time 11.054665803909302s

No improvement. Patience: 8/40
Training epoch 226, Batch 49/98: LR=1.18e-04, Loss=1.71508e-01 LER=6.254e-02
***Loss=1.71508e-01
Training epoch 226, Batch 98/98: LR=1.18e-04, Loss=1.70685e-01 LER=6.188e-02
***Loss=1.70685e-01
Epoch 226 Train Time 11.28662919998169s

No improvement. Patience: 9/40
Training epoch 227, Batch 49/98: LR=1.15e-04, Loss=1.71457e-01 LER=6.288e-02
***Loss=1.71457e-01
Training epoch 227, Batch 98/98: LR=1.15e-04, Loss=1.69886e-01 LER=6.191e-02
***Loss=1.69886e-01
Epoch 227 Train Time 11.327257871627808s

No improvement. Patience: 10/40
Training epoch 228, Batch 49/98: LR=1.12e-04, Loss=1.71932e-01 LER=6.306e-02
***Loss=1.71932e-01
Training epoch 228, Batch 98/98: LR=1.12e-04, Loss=1.71148e-01 LER=6.267e-02
***Loss=1.71148e-01
Epoch 228 Train Time 11.001725435256958s

No improvement. Patience: 11/40
Training epoch 229, Batch 49/98: LR=1.09e-04, Loss=1.71229e-01 LER=6.166e-02
***Loss=1.71229e-01
Training epoch 229, Batch 98/98: LR=1.09e-04, Loss=1.69242e-01 LER=6.121e-02
***Loss=1.69242e-01
Epoch 229 Train Time 10.730342626571655s

No improvement. Patience: 12/40
Training epoch 230, Batch 49/98: LR=1.06e-04, Loss=1.67938e-01 LER=6.085e-02
***Loss=1.67938e-01
Training epoch 230, Batch 98/98: LR=1.06e-04, Loss=1.69450e-01 LER=6.119e-02
***Loss=1.69450e-01
Epoch 230 Train Time 10.77103328704834s

No improvement. Patience: 13/40
Test LER  p=7.00e-02: 3.01e-02 p=8.00e-02: 4.90e-02 p=9.00e-02: 6.06e-02 p=1.00e-01: 6.87e-02 p=1.10e-01: 9.75e-02
Mean LER = 6.119e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.49488639831543 s

Training epoch 231, Batch 49/98: LR=1.04e-04, Loss=1.70663e-01 LER=6.144e-02
***Loss=1.70663e-01
Training epoch 231, Batch 98/98: LR=1.04e-04, Loss=1.70728e-01 LER=6.212e-02
***Loss=1.70728e-01
Epoch 231 Train Time 10.784456491470337s

No improvement. Patience: 14/40
Training epoch 232, Batch 49/98: LR=1.01e-04, Loss=1.64367e-01 LER=5.915e-02
***Loss=1.64367e-01
Training epoch 232, Batch 98/98: LR=1.01e-04, Loss=1.67311e-01 LER=6.043e-02
***Loss=1.67311e-01
Epoch 232 Train Time 11.00670075416565s

No improvement. Patience: 15/40
Training epoch 233, Batch 49/98: LR=9.81e-05, Loss=1.66926e-01 LER=6.003e-02
***Loss=1.66926e-01
Training epoch 233, Batch 98/98: LR=9.81e-05, Loss=1.67635e-01 LER=6.053e-02
***Loss=1.67635e-01
Epoch 233 Train Time 10.965742588043213s

No improvement. Patience: 16/40
Training epoch 234, Batch 49/98: LR=9.54e-05, Loss=1.74809e-01 LER=6.208e-02
***Loss=1.74809e-01
Training epoch 234, Batch 98/98: LR=9.54e-05, Loss=1.74403e-01 LER=6.247e-02
***Loss=1.74403e-01
Epoch 234 Train Time 11.114142656326294s

No improvement. Patience: 17/40
Training epoch 235, Batch 49/98: LR=9.27e-05, Loss=1.67306e-01 LER=6.087e-02
***Loss=1.67306e-01
Training epoch 235, Batch 98/98: LR=9.27e-05, Loss=1.67965e-01 LER=6.020e-02
***Loss=1.67965e-01
Epoch 235 Train Time 10.987435340881348s

No improvement. Patience: 18/40
Training epoch 236, Batch 49/98: LR=9.00e-05, Loss=1.69625e-01 LER=6.196e-02
***Loss=1.69625e-01
Training epoch 236, Batch 98/98: LR=9.00e-05, Loss=1.69763e-01 LER=6.210e-02
***Loss=1.69763e-01
Epoch 236 Train Time 11.17619013786316s

No improvement. Patience: 19/40
Training epoch 237, Batch 49/98: LR=8.74e-05, Loss=1.71958e-01 LER=6.224e-02
***Loss=1.71958e-01
Training epoch 237, Batch 98/98: LR=8.74e-05, Loss=1.70972e-01 LER=6.165e-02
***Loss=1.70972e-01
Epoch 237 Train Time 10.996230840682983s

No improvement. Patience: 20/40
Training epoch 238, Batch 49/98: LR=8.48e-05, Loss=1.68471e-01 LER=6.037e-02
***Loss=1.68471e-01
Training epoch 238, Batch 98/98: LR=8.48e-05, Loss=1.71101e-01 LER=6.146e-02
***Loss=1.71101e-01
Epoch 238 Train Time 10.475399255752563s

No improvement. Patience: 21/40
Training epoch 239, Batch 49/98: LR=8.23e-05, Loss=1.72520e-01 LER=6.270e-02
***Loss=1.72520e-01
Training epoch 239, Batch 98/98: LR=8.23e-05, Loss=1.69053e-01 LER=6.152e-02
***Loss=1.69053e-01
Epoch 239 Train Time 11.05234432220459s

No improvement. Patience: 22/40
Training epoch 240, Batch 49/98: LR=7.98e-05, Loss=1.68298e-01 LER=6.051e-02
***Loss=1.68298e-01
Training epoch 240, Batch 98/98: LR=7.98e-05, Loss=1.69033e-01 LER=6.135e-02
***Loss=1.69033e-01
Epoch 240 Train Time 11.073486089706421s

No improvement. Patience: 23/40
Test LER  p=7.00e-02: 3.41e-02 p=8.00e-02: 4.45e-02 p=9.00e-02: 5.85e-02 p=1.00e-01: 7.50e-02 p=1.10e-01: 9.54e-02
Mean LER = 6.150e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.62151575088501 s

Training epoch 241, Batch 49/98: LR=7.73e-05, Loss=1.68726e-01 LER=6.019e-02
***Loss=1.68726e-01
Training epoch 241, Batch 98/98: LR=7.73e-05, Loss=1.69646e-01 LER=6.115e-02
***Loss=1.69646e-01
Epoch 241 Train Time 10.992191314697266s

No improvement. Patience: 24/40
Training epoch 242, Batch 49/98: LR=7.49e-05, Loss=1.74458e-01 LER=6.282e-02
***Loss=1.74458e-01
Training epoch 242, Batch 98/98: LR=7.49e-05, Loss=1.72384e-01 LER=6.230e-02
***Loss=1.72384e-01
Epoch 242 Train Time 10.985182046890259s

No improvement. Patience: 25/40
Training epoch 243, Batch 49/98: LR=7.25e-05, Loss=1.67438e-01 LER=6.108e-02
***Loss=1.67438e-01
Training epoch 243, Batch 98/98: LR=7.25e-05, Loss=1.67221e-01 LER=6.102e-02
***Loss=1.67221e-01
Epoch 243 Train Time 11.204674005508423s

No improvement. Patience: 26/40
Training epoch 244, Batch 49/98: LR=7.01e-05, Loss=1.70110e-01 LER=6.212e-02
***Loss=1.70110e-01
Training epoch 244, Batch 98/98: LR=7.01e-05, Loss=1.70966e-01 LER=6.233e-02
***Loss=1.70966e-01
Epoch 244 Train Time 11.053123950958252s

No improvement. Patience: 27/40
Training epoch 245, Batch 49/98: LR=6.77e-05, Loss=1.67747e-01 LER=6.083e-02
***Loss=1.67747e-01
Training epoch 245, Batch 98/98: LR=6.77e-05, Loss=1.68330e-01 LER=6.114e-02
***Loss=1.68330e-01
Epoch 245 Train Time 11.027859687805176s

No improvement. Patience: 28/40
Training epoch 246, Batch 49/98: LR=6.55e-05, Loss=1.74122e-01 LER=6.230e-02
***Loss=1.74122e-01
Training epoch 246, Batch 98/98: LR=6.55e-05, Loss=1.71795e-01 LER=6.195e-02
***Loss=1.71795e-01
Epoch 246 Train Time 10.622174501419067s

No improvement. Patience: 29/40
Training epoch 247, Batch 49/98: LR=6.32e-05, Loss=1.64360e-01 LER=5.917e-02
***Loss=1.64360e-01
Training epoch 247, Batch 98/98: LR=6.32e-05, Loss=1.67193e-01 LER=6.016e-02
***Loss=1.67193e-01
Epoch 247 Train Time 10.805980682373047s

No improvement. Patience: 30/40
Training epoch 248, Batch 49/98: LR=6.10e-05, Loss=1.70013e-01 LER=6.162e-02
***Loss=1.70013e-01
Training epoch 248, Batch 98/98: LR=6.10e-05, Loss=1.69072e-01 LER=6.119e-02
***Loss=1.69072e-01
Epoch 248 Train Time 11.321442127227783s

No improvement. Patience: 31/40
Training epoch 249, Batch 49/98: LR=5.88e-05, Loss=1.68101e-01 LER=6.144e-02
***Loss=1.68101e-01
Training epoch 249, Batch 98/98: LR=5.88e-05, Loss=1.68070e-01 LER=6.132e-02
***Loss=1.68070e-01
Epoch 249 Train Time 10.863821983337402s

No improvement. Patience: 32/40
Training epoch 250, Batch 49/98: LR=5.66e-05, Loss=1.69052e-01 LER=6.108e-02
***Loss=1.69052e-01
Training epoch 250, Batch 98/98: LR=5.66e-05, Loss=1.67679e-01 LER=6.106e-02
***Loss=1.67679e-01
Epoch 250 Train Time 10.902329444885254s

No improvement. Patience: 33/40
Test LER  p=7.00e-02: 3.27e-02 p=8.00e-02: 4.31e-02 p=9.00e-02: 5.67e-02 p=1.00e-01: 7.53e-02 p=1.10e-01: 9.39e-02
Mean LER = 6.035e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.463745594024658 s

Training epoch 251, Batch 49/98: LR=5.45e-05, Loss=1.67660e-01 LER=6.005e-02
***Loss=1.67660e-01
Training epoch 251, Batch 98/98: LR=5.45e-05, Loss=1.65990e-01 LER=5.962e-02
***Loss=1.65990e-01
Epoch 251 Train Time 11.779778003692627s

Model Saved - New best loss: 1.65990e-01
Training epoch 252, Batch 49/98: LR=5.25e-05, Loss=1.69481e-01 LER=6.160e-02
***Loss=1.69481e-01
Training epoch 252, Batch 98/98: LR=5.25e-05, Loss=1.68151e-01 LER=6.119e-02
***Loss=1.68151e-01
Epoch 252 Train Time 11.243747472763062s

No improvement. Patience: 1/40
Training epoch 253, Batch 49/98: LR=5.04e-05, Loss=1.70361e-01 LER=6.180e-02
***Loss=1.70361e-01
Training epoch 253, Batch 98/98: LR=5.04e-05, Loss=1.69801e-01 LER=6.237e-02
***Loss=1.69801e-01
Epoch 253 Train Time 11.244762659072876s

No improvement. Patience: 2/40
Training epoch 254, Batch 49/98: LR=4.84e-05, Loss=1.69076e-01 LER=6.053e-02
***Loss=1.69076e-01
Training epoch 254, Batch 98/98: LR=4.84e-05, Loss=1.69110e-01 LER=6.060e-02
***Loss=1.69110e-01
Epoch 254 Train Time 11.108458757400513s

No improvement. Patience: 3/40
Training epoch 255, Batch 49/98: LR=4.65e-05, Loss=1.69959e-01 LER=6.118e-02
***Loss=1.69959e-01
Training epoch 255, Batch 98/98: LR=4.65e-05, Loss=1.70196e-01 LER=6.130e-02
***Loss=1.70196e-01
Epoch 255 Train Time 10.779370307922363s

No improvement. Patience: 4/40
Training epoch 256, Batch 49/98: LR=4.45e-05, Loss=1.67160e-01 LER=6.047e-02
***Loss=1.67160e-01
Training epoch 256, Batch 98/98: LR=4.45e-05, Loss=1.66286e-01 LER=6.026e-02
***Loss=1.66286e-01
Epoch 256 Train Time 10.899102687835693s

No improvement. Patience: 5/40
Training epoch 257, Batch 49/98: LR=4.27e-05, Loss=1.72441e-01 LER=6.206e-02
***Loss=1.72441e-01
Training epoch 257, Batch 98/98: LR=4.27e-05, Loss=1.70659e-01 LER=6.139e-02
***Loss=1.70659e-01
Epoch 257 Train Time 11.277011156082153s

No improvement. Patience: 6/40
Training epoch 258, Batch 49/98: LR=4.08e-05, Loss=1.68385e-01 LER=6.184e-02
***Loss=1.68385e-01
Training epoch 258, Batch 98/98: LR=4.08e-05, Loss=1.68258e-01 LER=6.134e-02
***Loss=1.68258e-01
Epoch 258 Train Time 11.338626384735107s

No improvement. Patience: 7/40
Training epoch 259, Batch 49/98: LR=3.90e-05, Loss=1.70416e-01 LER=6.170e-02
***Loss=1.70416e-01
Training epoch 259, Batch 98/98: LR=3.90e-05, Loss=1.68138e-01 LER=6.087e-02
***Loss=1.68138e-01
Epoch 259 Train Time 11.06252408027649s

No improvement. Patience: 8/40
Training epoch 260, Batch 49/98: LR=3.73e-05, Loss=1.68129e-01 LER=6.144e-02
***Loss=1.68129e-01
Training epoch 260, Batch 98/98: LR=3.73e-05, Loss=1.68521e-01 LER=6.188e-02
***Loss=1.68521e-01
Epoch 260 Train Time 11.57652759552002s

No improvement. Patience: 9/40
Test LER  p=7.00e-02: 3.58e-02 p=8.00e-02: 4.59e-02 p=9.00e-02: 5.97e-02 p=1.00e-01: 7.89e-02 p=1.10e-01: 9.78e-02
Mean LER = 6.361e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.491379976272583 s

Training epoch 261, Batch 49/98: LR=3.55e-05, Loss=1.64835e-01 LER=5.877e-02
***Loss=1.64835e-01
Training epoch 261, Batch 98/98: LR=3.55e-05, Loss=1.67434e-01 LER=6.057e-02
***Loss=1.67434e-01
Epoch 261 Train Time 11.297427654266357s

No improvement. Patience: 10/40
Training epoch 262, Batch 49/98: LR=3.39e-05, Loss=1.67835e-01 LER=6.091e-02
***Loss=1.67835e-01
Training epoch 262, Batch 98/98: LR=3.39e-05, Loss=1.68919e-01 LER=6.099e-02
***Loss=1.68919e-01
Epoch 262 Train Time 11.237212419509888s

No improvement. Patience: 11/40
Training epoch 263, Batch 49/98: LR=3.22e-05, Loss=1.68709e-01 LER=6.057e-02
***Loss=1.68709e-01
Training epoch 263, Batch 98/98: LR=3.22e-05, Loss=1.70832e-01 LER=6.169e-02
***Loss=1.70832e-01
Epoch 263 Train Time 11.090747117996216s

No improvement. Patience: 12/40
Training epoch 264, Batch 49/98: LR=3.06e-05, Loss=1.68283e-01 LER=6.152e-02
***Loss=1.68283e-01
Training epoch 264, Batch 98/98: LR=3.06e-05, Loss=1.69184e-01 LER=6.193e-02
***Loss=1.69184e-01
Epoch 264 Train Time 11.016918182373047s

No improvement. Patience: 13/40
Training epoch 265, Batch 49/98: LR=2.91e-05, Loss=1.68755e-01 LER=6.063e-02
***Loss=1.68755e-01
Training epoch 265, Batch 98/98: LR=2.91e-05, Loss=1.69206e-01 LER=6.093e-02
***Loss=1.69206e-01
Epoch 265 Train Time 11.049898624420166s

No improvement. Patience: 14/40
Training epoch 266, Batch 49/98: LR=2.75e-05, Loss=1.64334e-01 LER=5.939e-02
***Loss=1.64334e-01
Training epoch 266, Batch 98/98: LR=2.75e-05, Loss=1.69269e-01 LER=6.057e-02
***Loss=1.69269e-01
Epoch 266 Train Time 10.757509469985962s

No improvement. Patience: 15/40
Training epoch 267, Batch 49/98: LR=2.61e-05, Loss=1.69571e-01 LER=6.120e-02
***Loss=1.69571e-01
Training epoch 267, Batch 98/98: LR=2.61e-05, Loss=1.68950e-01 LER=6.119e-02
***Loss=1.68950e-01
Epoch 267 Train Time 11.271318912506104s

No improvement. Patience: 16/40
Training epoch 268, Batch 49/98: LR=2.46e-05, Loss=1.74382e-01 LER=6.330e-02
***Loss=1.74382e-01
Training epoch 268, Batch 98/98: LR=2.46e-05, Loss=1.71493e-01 LER=6.239e-02
***Loss=1.71493e-01
Epoch 268 Train Time 10.811959266662598s

No improvement. Patience: 17/40
Training epoch 269, Batch 49/98: LR=2.32e-05, Loss=1.69096e-01 LER=6.198e-02
***Loss=1.69096e-01
Training epoch 269, Batch 98/98: LR=2.32e-05, Loss=1.68088e-01 LER=6.126e-02
***Loss=1.68088e-01
Epoch 269 Train Time 11.008111953735352s

No improvement. Patience: 18/40
Training epoch 270, Batch 49/98: LR=2.19e-05, Loss=1.72060e-01 LER=6.274e-02
***Loss=1.72060e-01
Training epoch 270, Batch 98/98: LR=2.19e-05, Loss=1.71329e-01 LER=6.250e-02
***Loss=1.71329e-01
Epoch 270 Train Time 11.11237120628357s

No improvement. Patience: 19/40
Test LER  p=7.00e-02: 3.09e-02 p=8.00e-02: 4.51e-02 p=9.00e-02: 5.84e-02 p=1.00e-01: 7.61e-02 p=1.10e-01: 8.89e-02
Mean LER = 5.986e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.528826475143433 s

Training epoch 271, Batch 49/98: LR=2.06e-05, Loss=1.68319e-01 LER=6.138e-02
***Loss=1.68319e-01
Training epoch 271, Batch 98/98: LR=2.06e-05, Loss=1.67494e-01 LER=6.041e-02
***Loss=1.67494e-01
Epoch 271 Train Time 11.084634065628052s

No improvement. Patience: 20/40
Training epoch 272, Batch 49/98: LR=1.93e-05, Loss=1.70107e-01 LER=6.206e-02
***Loss=1.70107e-01
Training epoch 272, Batch 98/98: LR=1.93e-05, Loss=1.68572e-01 LER=6.138e-02
***Loss=1.68572e-01
Epoch 272 Train Time 11.090518951416016s

No improvement. Patience: 21/40
Training epoch 273, Batch 49/98: LR=1.81e-05, Loss=1.67652e-01 LER=6.118e-02
***Loss=1.67652e-01
Training epoch 273, Batch 98/98: LR=1.81e-05, Loss=1.66310e-01 LER=6.088e-02
***Loss=1.66310e-01
Epoch 273 Train Time 11.224854946136475s

No improvement. Patience: 22/40
Training epoch 274, Batch 49/98: LR=1.69e-05, Loss=1.68967e-01 LER=6.116e-02
***Loss=1.68967e-01
Training epoch 274, Batch 98/98: LR=1.69e-05, Loss=1.68323e-01 LER=6.124e-02
***Loss=1.68323e-01
Epoch 274 Train Time 10.926867723464966s

No improvement. Patience: 23/40
Training epoch 275, Batch 49/98: LR=1.57e-05, Loss=1.64017e-01 LER=5.911e-02
***Loss=1.64017e-01
Training epoch 275, Batch 98/98: LR=1.57e-05, Loss=1.67153e-01 LER=5.979e-02
***Loss=1.67153e-01
Epoch 275 Train Time 10.878061532974243s

No improvement. Patience: 24/40
Training epoch 276, Batch 49/98: LR=1.46e-05, Loss=1.64647e-01 LER=5.991e-02
***Loss=1.64647e-01
Training epoch 276, Batch 98/98: LR=1.46e-05, Loss=1.66405e-01 LER=6.115e-02
***Loss=1.66405e-01
Epoch 276 Train Time 10.872975826263428s

No improvement. Patience: 25/40
Training epoch 277, Batch 49/98: LR=1.36e-05, Loss=1.71935e-01 LER=6.186e-02
***Loss=1.71935e-01
Training epoch 277, Batch 98/98: LR=1.36e-05, Loss=1.70943e-01 LER=6.125e-02
***Loss=1.70943e-01
Epoch 277 Train Time 11.240041971206665s

No improvement. Patience: 26/40
Training epoch 278, Batch 49/98: LR=1.25e-05, Loss=1.68753e-01 LER=6.174e-02
***Loss=1.68753e-01
Training epoch 278, Batch 98/98: LR=1.25e-05, Loss=1.68687e-01 LER=6.154e-02
***Loss=1.68687e-01
Epoch 278 Train Time 11.042634010314941s

No improvement. Patience: 27/40
Training epoch 279, Batch 49/98: LR=1.16e-05, Loss=1.67318e-01 LER=6.031e-02
***Loss=1.67318e-01
Training epoch 279, Batch 98/98: LR=1.16e-05, Loss=1.70372e-01 LER=6.083e-02
***Loss=1.70372e-01
Epoch 279 Train Time 11.008286237716675s

No improvement. Patience: 28/40
Training epoch 280, Batch 49/98: LR=1.06e-05, Loss=1.65872e-01 LER=5.965e-02
***Loss=1.65872e-01
Training epoch 280, Batch 98/98: LR=1.06e-05, Loss=1.67392e-01 LER=6.087e-02
***Loss=1.67392e-01
Epoch 280 Train Time 10.895988464355469s

No improvement. Patience: 29/40
Test LER  p=7.00e-02: 3.11e-02 p=8.00e-02: 4.72e-02 p=9.00e-02: 5.69e-02 p=1.00e-01: 7.69e-02 p=1.10e-01: 9.39e-02
Mean LER = 6.119e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.842869997024536 s

Training epoch 281, Batch 49/98: LR=9.73e-06, Loss=1.70498e-01 LER=6.190e-02
***Loss=1.70498e-01
Training epoch 281, Batch 98/98: LR=9.73e-06, Loss=1.72206e-01 LER=6.260e-02
***Loss=1.72206e-01
Epoch 281 Train Time 11.071103811264038s

No improvement. Patience: 30/40
Training epoch 282, Batch 49/98: LR=8.88e-06, Loss=1.64720e-01 LER=5.957e-02
***Loss=1.64720e-01
Training epoch 282, Batch 98/98: LR=8.88e-06, Loss=1.65210e-01 LER=6.035e-02
***Loss=1.65210e-01
Epoch 282 Train Time 10.963808059692383s

Model Saved - New best loss: 1.65210e-01
Training epoch 283, Batch 49/98: LR=8.08e-06, Loss=1.66427e-01 LER=6.029e-02
***Loss=1.66427e-01
Training epoch 283, Batch 98/98: LR=8.08e-06, Loss=1.69305e-01 LER=6.114e-02
***Loss=1.69305e-01
Epoch 283 Train Time 10.973900556564331s

No improvement. Patience: 1/40
Training epoch 284, Batch 49/98: LR=7.31e-06, Loss=1.65558e-01 LER=5.997e-02
***Loss=1.65558e-01
Training epoch 284, Batch 98/98: LR=7.31e-06, Loss=1.65722e-01 LER=6.025e-02
***Loss=1.65722e-01
Epoch 284 Train Time 11.070722103118896s

No improvement. Patience: 2/40
Training epoch 285, Batch 49/98: LR=6.59e-06, Loss=1.65409e-01 LER=6.037e-02
***Loss=1.65409e-01
Training epoch 285, Batch 98/98: LR=6.59e-06, Loss=1.67198e-01 LER=6.125e-02
***Loss=1.67198e-01
Epoch 285 Train Time 11.286805629730225s

No improvement. Patience: 3/40
Training epoch 286, Batch 49/98: LR=5.92e-06, Loss=1.70723e-01 LER=6.097e-02
***Loss=1.70723e-01
Training epoch 286, Batch 98/98: LR=5.92e-06, Loss=1.69868e-01 LER=6.125e-02
***Loss=1.69868e-01
Epoch 286 Train Time 10.996609449386597s

No improvement. Patience: 4/40
Training epoch 287, Batch 49/98: LR=5.29e-06, Loss=1.68435e-01 LER=6.192e-02
***Loss=1.68435e-01
Training epoch 287, Batch 98/98: LR=5.29e-06, Loss=1.68813e-01 LER=6.162e-02
***Loss=1.68813e-01
Epoch 287 Train Time 10.91682481765747s

No improvement. Patience: 5/40
Training epoch 288, Batch 49/98: LR=4.70e-06, Loss=1.67075e-01 LER=6.021e-02
***Loss=1.67075e-01
Training epoch 288, Batch 98/98: LR=4.70e-06, Loss=1.68502e-01 LER=6.082e-02
***Loss=1.68502e-01
Epoch 288 Train Time 10.987003326416016s

No improvement. Patience: 6/40
Training epoch 289, Batch 49/98: LR=4.15e-06, Loss=1.70071e-01 LER=6.190e-02
***Loss=1.70071e-01
Training epoch 289, Batch 98/98: LR=4.15e-06, Loss=1.67219e-01 LER=6.056e-02
***Loss=1.67219e-01
Epoch 289 Train Time 10.660494089126587s

No improvement. Patience: 7/40
Training epoch 290, Batch 49/98: LR=3.65e-06, Loss=1.68335e-01 LER=6.057e-02
***Loss=1.68335e-01
Training epoch 290, Batch 98/98: LR=3.65e-06, Loss=1.69041e-01 LER=6.128e-02
***Loss=1.69041e-01
Epoch 290 Train Time 11.05516791343689s

No improvement. Patience: 8/40
Test LER  p=7.00e-02: 3.13e-02 p=8.00e-02: 4.65e-02 p=9.00e-02: 5.68e-02 p=1.00e-01: 7.21e-02 p=1.10e-01: 9.34e-02
Mean LER = 6.002e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.607624769210815 s

Training epoch 291, Batch 49/98: LR=3.19e-06, Loss=1.65176e-01 LER=5.987e-02
***Loss=1.65176e-01
Training epoch 291, Batch 98/98: LR=3.19e-06, Loss=1.66094e-01 LER=6.057e-02
***Loss=1.66094e-01
Epoch 291 Train Time 11.045178413391113s

No improvement. Patience: 9/40
Training epoch 292, Batch 49/98: LR=2.77e-06, Loss=1.66875e-01 LER=6.063e-02
***Loss=1.66875e-01
Training epoch 292, Batch 98/98: LR=2.77e-06, Loss=1.66743e-01 LER=6.005e-02
***Loss=1.66743e-01
Epoch 292 Train Time 10.635982751846313s

No improvement. Patience: 10/40
Training epoch 293, Batch 49/98: LR=2.40e-06, Loss=1.68803e-01 LER=6.105e-02
***Loss=1.68803e-01
Training epoch 293, Batch 98/98: LR=2.40e-06, Loss=1.66897e-01 LER=6.069e-02
***Loss=1.66897e-01
Epoch 293 Train Time 11.435288906097412s

No improvement. Patience: 11/40
Training epoch 294, Batch 49/98: LR=2.07e-06, Loss=1.67822e-01 LER=6.021e-02
***Loss=1.67822e-01
Training epoch 294, Batch 98/98: LR=2.07e-06, Loss=1.69654e-01 LER=6.150e-02
***Loss=1.69654e-01
Epoch 294 Train Time 11.064436674118042s

No improvement. Patience: 12/40
Training epoch 295, Batch 49/98: LR=1.79e-06, Loss=1.66793e-01 LER=6.069e-02
***Loss=1.66793e-01
Training epoch 295, Batch 98/98: LR=1.79e-06, Loss=1.66115e-01 LER=6.049e-02
***Loss=1.66115e-01
Epoch 295 Train Time 11.35791015625s

No improvement. Patience: 13/40
Training epoch 296, Batch 49/98: LR=1.55e-06, Loss=1.66948e-01 LER=6.061e-02
***Loss=1.66948e-01
Training epoch 296, Batch 98/98: LR=1.55e-06, Loss=1.66251e-01 LER=6.051e-02
***Loss=1.66251e-01
Epoch 296 Train Time 11.353983879089355s

No improvement. Patience: 14/40
Training epoch 297, Batch 49/98: LR=1.35e-06, Loss=1.62943e-01 LER=5.917e-02
***Loss=1.62943e-01
Training epoch 297, Batch 98/98: LR=1.35e-06, Loss=1.63622e-01 LER=5.887e-02
***Loss=1.63622e-01
Epoch 297 Train Time 11.048919200897217s

Model Saved - New best loss: 1.63622e-01
Training epoch 298, Batch 49/98: LR=1.20e-06, Loss=1.72047e-01 LER=6.226e-02
***Loss=1.72047e-01
Training epoch 298, Batch 98/98: LR=1.20e-06, Loss=1.70490e-01 LER=6.190e-02
***Loss=1.70490e-01
Epoch 298 Train Time 10.886310815811157s

No improvement. Patience: 1/40
Training epoch 299, Batch 49/98: LR=1.09e-06, Loss=1.66759e-01 LER=5.987e-02
***Loss=1.66759e-01
Training epoch 299, Batch 98/98: LR=1.09e-06, Loss=1.68020e-01 LER=6.063e-02
***Loss=1.68020e-01
Epoch 299 Train Time 10.91002345085144s

No improvement. Patience: 2/40
Training epoch 300, Batch 49/98: LR=1.02e-06, Loss=1.68420e-01 LER=6.051e-02
***Loss=1.68420e-01
Training epoch 300, Batch 98/98: LR=1.02e-06, Loss=1.66886e-01 LER=6.008e-02
***Loss=1.66886e-01
Epoch 300 Train Time 10.942846536636353s

No improvement. Patience: 3/40
Test LER  p=7.00e-02: 3.16e-02 p=8.00e-02: 4.56e-02 p=9.00e-02: 5.95e-02 p=1.00e-01: 7.33e-02 p=1.10e-01: 1.02e-01
Mean LER = 6.232e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 30.635395050048828 s

Best model loaded
Test LER  p=7.00e-02: 3.44e-02 p=8.00e-02: 4.79e-02 p=9.00e-02: 5.45e-02 p=1.00e-01: 7.46e-02 p=1.10e-01: 9.39e-02
Mean LER = 6.105e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.147297859191895 s

