Path to model/logs: Final_Results_QECCT/surface/Transformer_Code_L_5/noise_model_independent/repetition_1/11_11_2025_22_20_35
Namespace(epochs=300, workers=8, lr=0.001, gpus='0', batch_size=1024, test_batch_size=512, seed=42, device='cuda', patience=40, min_delta=0.0, code_type='surface', code_L=5, repetitions=1, noise_type='independent', y_ratio=0.3, N_dec=10, d_model=256, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x7f25e16c3290>, path='Final_Results_QECCT/surface/Transformer_Code_L_5/noise_model_independent/repetition_1/11_11_2025_22_20_35')
NVIDIA GPU (CUDA)를 사용합니다: NVIDIA A100-SXM4-40GB
PC matrix shape torch.Size([24, 50])
ECC_Transformer(
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder): Encoder(
    (layers): ModuleList(
      (0-9): 10 x EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0-1): 2 x SublayerConnection(
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (input_embedding): Linear(in_features=24, out_features=256, bias=True)
  (output_classifier): Linear(in_features=256, out_features=4, bias=True)
  (criterion): CrossEntropyLoss()
)
# of Parameters: 7906052
Training epoch 1, Batch 49/98: LR=1.00e-03, Loss=1.32727e+00 LER=5.543e-01
***Loss=1.32727e+00
Training epoch 1, Batch 98/98: LR=1.00e-03, Loss=1.27986e+00 LER=5.462e-01
***Loss=1.27986e+00
Epoch 1 Train Time 11.656678438186646s

Model Saved - New best loss: 1.27986e+00
Training epoch 2, Batch 49/98: LR=1.00e-03, Loss=1.23371e+00 LER=5.375e-01
***Loss=1.23371e+00
Training epoch 2, Batch 98/98: LR=1.00e-03, Loss=1.22862e+00 LER=5.293e-01
***Loss=1.22862e+00
Epoch 2 Train Time 11.416205883026123s

Model Saved - New best loss: 1.22862e+00
Training epoch 3, Batch 49/98: LR=1.00e-03, Loss=1.00782e+00 LER=4.121e-01
***Loss=1.00782e+00
Training epoch 3, Batch 98/98: LR=1.00e-03, Loss=9.56041e-01 LER=3.951e-01
***Loss=9.56041e-01
Epoch 3 Train Time 11.396342515945435s

Model Saved - New best loss: 9.56041e-01
Training epoch 4, Batch 49/98: LR=1.00e-03, Loss=8.15488e-01 LER=3.510e-01
***Loss=8.15488e-01
Training epoch 4, Batch 98/98: LR=1.00e-03, Loss=7.53468e-01 LER=3.440e-01
***Loss=7.53468e-01
Epoch 4 Train Time 11.405508518218994s

Model Saved - New best loss: 7.53468e-01
Training epoch 5, Batch 49/98: LR=1.00e-03, Loss=6.39173e-01 LER=3.022e-01
***Loss=6.39173e-01
Training epoch 5, Batch 98/98: LR=1.00e-03, Loss=6.06702e-01 LER=2.706e-01
***Loss=6.06702e-01
Epoch 5 Train Time 11.338344097137451s

Model Saved - New best loss: 6.06702e-01
Training epoch 6, Batch 49/98: LR=9.99e-04, Loss=4.98784e-01 LER=1.966e-01
***Loss=4.98784e-01
Training epoch 6, Batch 98/98: LR=9.99e-04, Loss=4.68099e-01 LER=1.853e-01
***Loss=4.68099e-01
Epoch 6 Train Time 11.505168437957764s

Model Saved - New best loss: 4.68099e-01
Training epoch 7, Batch 49/98: LR=9.99e-04, Loss=3.79549e-01 LER=1.487e-01
***Loss=3.79549e-01
Training epoch 7, Batch 98/98: LR=9.99e-04, Loss=3.53920e-01 LER=1.384e-01
***Loss=3.53920e-01
Epoch 7 Train Time 11.13611888885498s

Model Saved - New best loss: 3.53920e-01
Training epoch 8, Batch 49/98: LR=9.99e-04, Loss=3.07585e-01 LER=1.178e-01
***Loss=3.07585e-01
Training epoch 8, Batch 98/98: LR=9.99e-04, Loss=3.00664e-01 LER=1.140e-01
***Loss=3.00664e-01
Epoch 8 Train Time 11.649229288101196s

Model Saved - New best loss: 3.00664e-01
Training epoch 9, Batch 49/98: LR=9.98e-04, Loss=2.85065e-01 LER=1.075e-01
***Loss=2.85065e-01
Training epoch 9, Batch 98/98: LR=9.98e-04, Loss=2.80402e-01 LER=1.054e-01
***Loss=2.80402e-01
Epoch 9 Train Time 11.295128345489502s

Model Saved - New best loss: 2.80402e-01
Training epoch 10, Batch 49/98: LR=9.98e-04, Loss=2.67037e-01 LER=1.013e-01
***Loss=2.67037e-01
Training epoch 10, Batch 98/98: LR=9.98e-04, Loss=2.61734e-01 LER=9.796e-02
***Loss=2.61734e-01
Epoch 10 Train Time 11.391866207122803s

Model Saved - New best loss: 2.61734e-01
Test LER  p=7.00e-02: 5.29e-02 p=8.00e-02: 7.19e-02 p=9.00e-02: 9.09e-02 p=1.00e-01: 1.14e-01 p=1.10e-01: 1.38e-01
Mean LER = 9.352e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.878484964370728 s

Training epoch 11, Batch 49/98: LR=9.97e-04, Loss=2.56457e-01 LER=9.648e-02
***Loss=2.56457e-01
Training epoch 11, Batch 98/98: LR=9.97e-04, Loss=2.52896e-01 LER=9.442e-02
***Loss=2.52896e-01
Epoch 11 Train Time 11.456871032714844s

Model Saved - New best loss: 2.52896e-01
Training epoch 12, Batch 49/98: LR=9.97e-04, Loss=2.50268e-01 LER=9.359e-02
***Loss=2.50268e-01
Training epoch 12, Batch 98/98: LR=9.97e-04, Loss=2.48622e-01 LER=9.312e-02
***Loss=2.48622e-01
Epoch 12 Train Time 11.727810382843018s

Model Saved - New best loss: 2.48622e-01
Training epoch 13, Batch 49/98: LR=9.96e-04, Loss=2.41588e-01 LER=8.905e-02
***Loss=2.41588e-01
Training epoch 13, Batch 98/98: LR=9.96e-04, Loss=2.38628e-01 LER=8.812e-02
***Loss=2.38628e-01
Epoch 13 Train Time 11.56934118270874s

Model Saved - New best loss: 2.38628e-01
Training epoch 14, Batch 49/98: LR=9.95e-04, Loss=2.38911e-01 LER=8.847e-02
***Loss=2.38911e-01
Training epoch 14, Batch 98/98: LR=9.95e-04, Loss=2.34439e-01 LER=8.690e-02
***Loss=2.34439e-01
Epoch 14 Train Time 11.435056447982788s

Model Saved - New best loss: 2.34439e-01
Training epoch 15, Batch 49/98: LR=9.95e-04, Loss=2.25381e-01 LER=8.315e-02
***Loss=2.25381e-01
Training epoch 15, Batch 98/98: LR=9.95e-04, Loss=2.27971e-01 LER=8.384e-02
***Loss=2.27971e-01
Epoch 15 Train Time 11.679116249084473s

Model Saved - New best loss: 2.27971e-01
Training epoch 16, Batch 49/98: LR=9.94e-04, Loss=2.34665e-01 LER=8.658e-02
***Loss=2.34665e-01
Training epoch 16, Batch 98/98: LR=9.94e-04, Loss=2.29632e-01 LER=8.391e-02
***Loss=2.29632e-01
Epoch 16 Train Time 11.450677871704102s

No improvement. Patience: 1/40
Training epoch 17, Batch 49/98: LR=9.93e-04, Loss=2.25404e-01 LER=8.305e-02
***Loss=2.25404e-01
Training epoch 17, Batch 98/98: LR=9.93e-04, Loss=2.26777e-01 LER=8.332e-02
***Loss=2.26777e-01
Epoch 17 Train Time 11.212999105453491s

Model Saved - New best loss: 2.26777e-01
Training epoch 18, Batch 49/98: LR=9.92e-04, Loss=2.21388e-01 LER=8.181e-02
***Loss=2.21388e-01
Training epoch 18, Batch 98/98: LR=9.92e-04, Loss=2.22768e-01 LER=8.220e-02
***Loss=2.22768e-01
Epoch 18 Train Time 11.786404132843018s

Model Saved - New best loss: 2.22768e-01
Training epoch 19, Batch 49/98: LR=9.91e-04, Loss=2.19858e-01 LER=8.052e-02
***Loss=2.19858e-01
Training epoch 19, Batch 98/98: LR=9.91e-04, Loss=2.23548e-01 LER=8.189e-02
***Loss=2.23548e-01
Epoch 19 Train Time 11.860249757766724s

No improvement. Patience: 1/40
Training epoch 20, Batch 49/98: LR=9.90e-04, Loss=2.22401e-01 LER=8.155e-02
***Loss=2.22401e-01
Training epoch 20, Batch 98/98: LR=9.90e-04, Loss=2.19859e-01 LER=8.060e-02
***Loss=2.19859e-01
Epoch 20 Train Time 11.769438028335571s

Model Saved - New best loss: 2.19859e-01
Test LER  p=7.00e-02: 4.57e-02 p=8.00e-02: 6.21e-02 p=9.00e-02: 7.83e-02 p=1.00e-01: 1.02e-01 p=1.10e-01: 1.23e-01
Mean LER = 8.227e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.45909571647644 s

Training epoch 21, Batch 49/98: LR=9.89e-04, Loss=2.20030e-01 LER=8.034e-02
***Loss=2.20030e-01
Training epoch 21, Batch 98/98: LR=9.89e-04, Loss=2.17998e-01 LER=7.924e-02
***Loss=2.17998e-01
Epoch 21 Train Time 11.22754716873169s

Model Saved - New best loss: 2.17998e-01
Training epoch 22, Batch 49/98: LR=9.88e-04, Loss=2.16588e-01 LER=7.836e-02
***Loss=2.16588e-01
Training epoch 22, Batch 98/98: LR=9.88e-04, Loss=2.14137e-01 LER=7.812e-02
***Loss=2.14137e-01
Epoch 22 Train Time 11.938086986541748s

Model Saved - New best loss: 2.14137e-01
Training epoch 23, Batch 49/98: LR=9.87e-04, Loss=2.13185e-01 LER=7.854e-02
***Loss=2.13185e-01
Training epoch 23, Batch 98/98: LR=9.87e-04, Loss=2.14339e-01 LER=7.817e-02
***Loss=2.14339e-01
Epoch 23 Train Time 11.477286577224731s

No improvement. Patience: 1/40
Training epoch 24, Batch 49/98: LR=9.86e-04, Loss=2.17848e-01 LER=7.902e-02
***Loss=2.17848e-01
Training epoch 24, Batch 98/98: LR=9.86e-04, Loss=2.15179e-01 LER=7.851e-02
***Loss=2.15179e-01
Epoch 24 Train Time 11.463086128234863s

No improvement. Patience: 2/40
Training epoch 25, Batch 49/98: LR=9.84e-04, Loss=2.13183e-01 LER=7.731e-02
***Loss=2.13183e-01
Training epoch 25, Batch 98/98: LR=9.84e-04, Loss=2.12655e-01 LER=7.696e-02
***Loss=2.12655e-01
Epoch 25 Train Time 11.492668628692627s

Model Saved - New best loss: 2.12655e-01
Training epoch 26, Batch 49/98: LR=9.83e-04, Loss=2.10688e-01 LER=7.669e-02
***Loss=2.10688e-01
Training epoch 26, Batch 98/98: LR=9.83e-04, Loss=2.08509e-01 LER=7.544e-02
***Loss=2.08509e-01
Epoch 26 Train Time 11.39461374282837s

Model Saved - New best loss: 2.08509e-01
Training epoch 27, Batch 49/98: LR=9.82e-04, Loss=2.11218e-01 LER=7.665e-02
***Loss=2.11218e-01
Training epoch 27, Batch 98/98: LR=9.82e-04, Loss=2.09369e-01 LER=7.632e-02
***Loss=2.09369e-01
Epoch 27 Train Time 11.621015787124634s

No improvement. Patience: 1/40
Training epoch 28, Batch 49/98: LR=9.80e-04, Loss=2.06583e-01 LER=7.593e-02
***Loss=2.06583e-01
Training epoch 28, Batch 98/98: LR=9.80e-04, Loss=2.08539e-01 LER=7.592e-02
***Loss=2.08539e-01
Epoch 28 Train Time 11.473851919174194s

No improvement. Patience: 2/40
Training epoch 29, Batch 49/98: LR=9.79e-04, Loss=2.09223e-01 LER=7.504e-02
***Loss=2.09223e-01
Training epoch 29, Batch 98/98: LR=9.79e-04, Loss=2.07857e-01 LER=7.483e-02
***Loss=2.07857e-01
Epoch 29 Train Time 11.209777355194092s

Model Saved - New best loss: 2.07857e-01
Training epoch 30, Batch 49/98: LR=9.77e-04, Loss=2.02534e-01 LER=7.388e-02
***Loss=2.02534e-01
Training epoch 30, Batch 98/98: LR=9.77e-04, Loss=2.02506e-01 LER=7.324e-02
***Loss=2.02506e-01
Epoch 30 Train Time 11.570550441741943s

Model Saved - New best loss: 2.02506e-01
Test LER  p=7.00e-02: 3.97e-02 p=8.00e-02: 5.48e-02 p=9.00e-02: 7.40e-02 p=1.00e-01: 9.77e-02 p=1.10e-01: 1.11e-01
Mean LER = 7.543e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.5944242477417 s

Training epoch 31, Batch 49/98: LR=9.76e-04, Loss=2.05599e-01 LER=7.520e-02
***Loss=2.05599e-01
Training epoch 31, Batch 98/98: LR=9.76e-04, Loss=2.06395e-01 LER=7.585e-02
***Loss=2.06395e-01
Epoch 31 Train Time 11.346788883209229s

No improvement. Patience: 1/40
Training epoch 32, Batch 49/98: LR=9.74e-04, Loss=2.06566e-01 LER=7.418e-02
***Loss=2.06566e-01
Training epoch 32, Batch 98/98: LR=9.74e-04, Loss=2.07490e-01 LER=7.509e-02
***Loss=2.07490e-01
Epoch 32 Train Time 11.342591762542725s

No improvement. Patience: 2/40
Training epoch 33, Batch 49/98: LR=9.72e-04, Loss=2.04461e-01 LER=7.444e-02
***Loss=2.04461e-01
Training epoch 33, Batch 98/98: LR=9.72e-04, Loss=2.04293e-01 LER=7.362e-02
***Loss=2.04293e-01
Epoch 33 Train Time 11.052058696746826s

No improvement. Patience: 3/40
Training epoch 34, Batch 49/98: LR=9.70e-04, Loss=2.03533e-01 LER=7.416e-02
***Loss=2.03533e-01
Training epoch 34, Batch 98/98: LR=9.70e-04, Loss=2.00824e-01 LER=7.277e-02
***Loss=2.00824e-01
Epoch 34 Train Time 11.422136068344116s

Model Saved - New best loss: 2.00824e-01
Training epoch 35, Batch 49/98: LR=9.69e-04, Loss=2.06524e-01 LER=7.526e-02
***Loss=2.06524e-01
Training epoch 35, Batch 98/98: LR=9.69e-04, Loss=2.05682e-01 LER=7.524e-02
***Loss=2.05682e-01
Epoch 35 Train Time 11.482175350189209s

No improvement. Patience: 1/40
Training epoch 36, Batch 49/98: LR=9.67e-04, Loss=2.04134e-01 LER=7.404e-02
***Loss=2.04134e-01
Training epoch 36, Batch 98/98: LR=9.67e-04, Loss=2.03958e-01 LER=7.392e-02
***Loss=2.03958e-01
Epoch 36 Train Time 11.373753070831299s

No improvement. Patience: 2/40
Training epoch 37, Batch 49/98: LR=9.65e-04, Loss=2.04516e-01 LER=7.500e-02
***Loss=2.04516e-01
Training epoch 37, Batch 98/98: LR=9.65e-04, Loss=2.04369e-01 LER=7.399e-02
***Loss=2.04369e-01
Epoch 37 Train Time 11.539151430130005s

No improvement. Patience: 3/40
Training epoch 38, Batch 49/98: LR=9.63e-04, Loss=2.00811e-01 LER=7.201e-02
***Loss=2.00811e-01
Training epoch 38, Batch 98/98: LR=9.63e-04, Loss=2.01607e-01 LER=7.266e-02
***Loss=2.01607e-01
Epoch 38 Train Time 11.603904962539673s

No improvement. Patience: 4/40
Training epoch 39, Batch 49/98: LR=9.61e-04, Loss=1.96566e-01 LER=7.069e-02
***Loss=1.96566e-01
Training epoch 39, Batch 98/98: LR=9.61e-04, Loss=1.98606e-01 LER=7.163e-02
***Loss=1.98606e-01
Epoch 39 Train Time 11.298343181610107s

Model Saved - New best loss: 1.98606e-01
Training epoch 40, Batch 49/98: LR=9.59e-04, Loss=2.02428e-01 LER=7.376e-02
***Loss=2.02428e-01
Training epoch 40, Batch 98/98: LR=9.59e-04, Loss=2.01504e-01 LER=7.345e-02
***Loss=2.01504e-01
Epoch 40 Train Time 11.82770037651062s

No improvement. Patience: 1/40
Test LER  p=7.00e-02: 3.96e-02 p=8.00e-02: 5.42e-02 p=9.00e-02: 7.03e-02 p=1.00e-01: 8.83e-02 p=1.10e-01: 1.08e-01
Mean LER = 7.213e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.442378520965576 s

Training epoch 41, Batch 49/98: LR=9.57e-04, Loss=2.00810e-01 LER=7.362e-02
***Loss=2.00810e-01
Training epoch 41, Batch 98/98: LR=9.57e-04, Loss=1.99772e-01 LER=7.261e-02
***Loss=1.99772e-01
Epoch 41 Train Time 11.372419357299805s

No improvement. Patience: 2/40
Training epoch 42, Batch 49/98: LR=9.55e-04, Loss=1.98956e-01 LER=7.298e-02
***Loss=1.98956e-01
Training epoch 42, Batch 98/98: LR=9.55e-04, Loss=1.98576e-01 LER=7.195e-02
***Loss=1.98576e-01
Epoch 42 Train Time 11.236861944198608s

Model Saved - New best loss: 1.98576e-01
Training epoch 43, Batch 49/98: LR=9.52e-04, Loss=1.96421e-01 LER=7.051e-02
***Loss=1.96421e-01
Training epoch 43, Batch 98/98: LR=9.52e-04, Loss=1.97035e-01 LER=7.047e-02
***Loss=1.97035e-01
Epoch 43 Train Time 11.356195449829102s

Model Saved - New best loss: 1.97035e-01
Training epoch 44, Batch 49/98: LR=9.50e-04, Loss=1.99073e-01 LER=7.189e-02
***Loss=1.99073e-01
Training epoch 44, Batch 98/98: LR=9.50e-04, Loss=1.98635e-01 LER=7.125e-02
***Loss=1.98635e-01
Epoch 44 Train Time 11.140907764434814s

No improvement. Patience: 1/40
Training epoch 45, Batch 49/98: LR=9.48e-04, Loss=1.97634e-01 LER=7.095e-02
***Loss=1.97634e-01
Training epoch 45, Batch 98/98: LR=9.48e-04, Loss=1.97660e-01 LER=7.058e-02
***Loss=1.97660e-01
Epoch 45 Train Time 11.298029899597168s

No improvement. Patience: 2/40
Training epoch 46, Batch 49/98: LR=9.46e-04, Loss=1.99997e-01 LER=7.171e-02
***Loss=1.99997e-01
Training epoch 46, Batch 98/98: LR=9.46e-04, Loss=1.97724e-01 LER=7.066e-02
***Loss=1.97724e-01
Epoch 46 Train Time 11.597991228103638s

No improvement. Patience: 3/40
Training epoch 47, Batch 49/98: LR=9.43e-04, Loss=1.94699e-01 LER=7.037e-02
***Loss=1.94699e-01
Training epoch 47, Batch 98/98: LR=9.43e-04, Loss=1.96327e-01 LER=7.062e-02
***Loss=1.96327e-01
Epoch 47 Train Time 11.585200786590576s

Model Saved - New best loss: 1.96327e-01
Training epoch 48, Batch 49/98: LR=9.41e-04, Loss=2.01310e-01 LER=7.235e-02
***Loss=2.01310e-01
Training epoch 48, Batch 98/98: LR=9.41e-04, Loss=1.99940e-01 LER=7.169e-02
***Loss=1.99940e-01
Epoch 48 Train Time 11.942234754562378s

No improvement. Patience: 1/40
Training epoch 49, Batch 49/98: LR=9.38e-04, Loss=2.02231e-01 LER=7.360e-02
***Loss=2.02231e-01
Training epoch 49, Batch 98/98: LR=9.38e-04, Loss=2.01883e-01 LER=7.267e-02
***Loss=2.01883e-01
Epoch 49 Train Time 11.694391250610352s

No improvement. Patience: 2/40
Training epoch 50, Batch 49/98: LR=9.36e-04, Loss=1.96000e-01 LER=7.119e-02
***Loss=1.96000e-01
Training epoch 50, Batch 98/98: LR=9.36e-04, Loss=1.96971e-01 LER=7.090e-02
***Loss=1.96971e-01
Epoch 50 Train Time 11.668992042541504s

No improvement. Patience: 3/40
Test LER  p=7.00e-02: 3.68e-02 p=8.00e-02: 5.29e-02 p=9.00e-02: 6.82e-02 p=1.00e-01: 9.25e-02 p=1.10e-01: 1.07e-01
Mean LER = 7.143e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.478868007659912 s

Training epoch 51, Batch 49/98: LR=9.33e-04, Loss=1.94810e-01 LER=7.027e-02
***Loss=1.94810e-01
Training epoch 51, Batch 98/98: LR=9.33e-04, Loss=1.95638e-01 LER=7.073e-02
***Loss=1.95638e-01
Epoch 51 Train Time 11.552810430526733s

Model Saved - New best loss: 1.95638e-01
Training epoch 52, Batch 49/98: LR=9.30e-04, Loss=1.94579e-01 LER=6.904e-02
***Loss=1.94579e-01
Training epoch 52, Batch 98/98: LR=9.30e-04, Loss=1.96027e-01 LER=6.987e-02
***Loss=1.96027e-01
Epoch 52 Train Time 11.527276515960693s

No improvement. Patience: 1/40
Training epoch 53, Batch 49/98: LR=9.28e-04, Loss=1.95945e-01 LER=6.989e-02
***Loss=1.95945e-01
Training epoch 53, Batch 98/98: LR=9.28e-04, Loss=1.97959e-01 LER=7.104e-02
***Loss=1.97959e-01
Epoch 53 Train Time 11.665854454040527s

No improvement. Patience: 2/40
Training epoch 54, Batch 49/98: LR=9.25e-04, Loss=1.95868e-01 LER=7.125e-02
***Loss=1.95868e-01
Training epoch 54, Batch 98/98: LR=9.25e-04, Loss=1.96422e-01 LER=7.166e-02
***Loss=1.96422e-01
Epoch 54 Train Time 11.65222692489624s

No improvement. Patience: 3/40
Training epoch 55, Batch 49/98: LR=9.22e-04, Loss=1.95571e-01 LER=6.975e-02
***Loss=1.95571e-01
Training epoch 55, Batch 98/98: LR=9.22e-04, Loss=1.95365e-01 LER=7.038e-02
***Loss=1.95365e-01
Epoch 55 Train Time 11.557891130447388s

Model Saved - New best loss: 1.95365e-01
Training epoch 56, Batch 49/98: LR=9.19e-04, Loss=1.95296e-01 LER=7.185e-02
***Loss=1.95296e-01
Training epoch 56, Batch 98/98: LR=9.19e-04, Loss=1.93593e-01 LER=7.070e-02
***Loss=1.93593e-01
Epoch 56 Train Time 11.443862199783325s

Model Saved - New best loss: 1.93593e-01
Training epoch 57, Batch 49/98: LR=9.17e-04, Loss=1.97481e-01 LER=7.117e-02
***Loss=1.97481e-01
Training epoch 57, Batch 98/98: LR=9.17e-04, Loss=1.93481e-01 LER=6.959e-02
***Loss=1.93481e-01
Epoch 57 Train Time 11.162309408187866s

Model Saved - New best loss: 1.93481e-01
Training epoch 58, Batch 49/98: LR=9.14e-04, Loss=1.88164e-01 LER=6.816e-02
***Loss=1.88164e-01
Training epoch 58, Batch 98/98: LR=9.14e-04, Loss=1.92597e-01 LER=6.945e-02
***Loss=1.92597e-01
Epoch 58 Train Time 11.65113377571106s

Model Saved - New best loss: 1.92597e-01
Training epoch 59, Batch 49/98: LR=9.11e-04, Loss=1.94450e-01 LER=7.033e-02
***Loss=1.94450e-01
Training epoch 59, Batch 98/98: LR=9.11e-04, Loss=1.93999e-01 LER=7.014e-02
***Loss=1.93999e-01
Epoch 59 Train Time 11.063226699829102s

No improvement. Patience: 1/40
Training epoch 60, Batch 49/98: LR=9.08e-04, Loss=1.95124e-01 LER=7.159e-02
***Loss=1.95124e-01
Training epoch 60, Batch 98/98: LR=9.08e-04, Loss=1.94070e-01 LER=7.081e-02
***Loss=1.94070e-01
Epoch 60 Train Time 11.449328899383545s

No improvement. Patience: 2/40
Test LER  p=7.00e-02: 3.87e-02 p=8.00e-02: 5.39e-02 p=9.00e-02: 6.74e-02 p=1.00e-01: 8.41e-02 p=1.10e-01: 1.08e-01
Mean LER = 7.035e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.620680809020996 s

Training epoch 61, Batch 49/98: LR=9.05e-04, Loss=1.92682e-01 LER=6.961e-02
***Loss=1.92682e-01
Training epoch 61, Batch 98/98: LR=9.05e-04, Loss=1.91985e-01 LER=6.988e-02
***Loss=1.91985e-01
Epoch 61 Train Time 11.586255073547363s

Model Saved - New best loss: 1.91985e-01
Training epoch 62, Batch 49/98: LR=9.02e-04, Loss=1.97642e-01 LER=7.037e-02
***Loss=1.97642e-01
Training epoch 62, Batch 98/98: LR=9.02e-04, Loss=1.92513e-01 LER=6.921e-02
***Loss=1.92513e-01
Epoch 62 Train Time 11.939223766326904s

No improvement. Patience: 1/40
Training epoch 63, Batch 49/98: LR=8.98e-04, Loss=1.94124e-01 LER=6.914e-02
***Loss=1.94124e-01
Training epoch 63, Batch 98/98: LR=8.98e-04, Loss=1.92029e-01 LER=6.906e-02
***Loss=1.92029e-01
Epoch 63 Train Time 11.014813899993896s

No improvement. Patience: 2/40
Training epoch 64, Batch 49/98: LR=8.95e-04, Loss=1.93050e-01 LER=6.954e-02
***Loss=1.93050e-01
Training epoch 64, Batch 98/98: LR=8.95e-04, Loss=1.92634e-01 LER=6.967e-02
***Loss=1.92634e-01
Epoch 64 Train Time 11.441928386688232s

No improvement. Patience: 3/40
Training epoch 65, Batch 49/98: LR=8.92e-04, Loss=1.92282e-01 LER=6.995e-02
***Loss=1.92282e-01
Training epoch 65, Batch 98/98: LR=8.92e-04, Loss=1.93802e-01 LER=7.037e-02
***Loss=1.93802e-01
Epoch 65 Train Time 11.826600313186646s

No improvement. Patience: 4/40
Training epoch 66, Batch 49/98: LR=8.89e-04, Loss=1.90126e-01 LER=6.794e-02
***Loss=1.90126e-01
Training epoch 66, Batch 98/98: LR=8.89e-04, Loss=1.91082e-01 LER=6.924e-02
***Loss=1.91082e-01
Epoch 66 Train Time 11.174649238586426s

Model Saved - New best loss: 1.91082e-01
Training epoch 67, Batch 49/98: LR=8.85e-04, Loss=1.94450e-01 LER=7.057e-02
***Loss=1.94450e-01
Training epoch 67, Batch 98/98: LR=8.85e-04, Loss=1.92349e-01 LER=6.998e-02
***Loss=1.92349e-01
Epoch 67 Train Time 11.207531452178955s

No improvement. Patience: 1/40
Training epoch 68, Batch 49/98: LR=8.82e-04, Loss=1.92457e-01 LER=6.995e-02
***Loss=1.92457e-01
Training epoch 68, Batch 98/98: LR=8.82e-04, Loss=1.92377e-01 LER=6.983e-02
***Loss=1.92377e-01
Epoch 68 Train Time 11.396258115768433s

No improvement. Patience: 2/40
Training epoch 69, Batch 49/98: LR=8.79e-04, Loss=1.90998e-01 LER=6.918e-02
***Loss=1.90998e-01
Training epoch 69, Batch 98/98: LR=8.79e-04, Loss=1.91743e-01 LER=6.996e-02
***Loss=1.91743e-01
Epoch 69 Train Time 11.497444152832031s

No improvement. Patience: 3/40
Training epoch 70, Batch 49/98: LR=8.75e-04, Loss=1.91025e-01 LER=6.912e-02
***Loss=1.91025e-01
Training epoch 70, Batch 98/98: LR=8.75e-04, Loss=1.90617e-01 LER=6.841e-02
***Loss=1.90617e-01
Epoch 70 Train Time 11.020451784133911s

Model Saved - New best loss: 1.90617e-01
Test LER  p=7.00e-02: 3.36e-02 p=8.00e-02: 5.10e-02 p=9.00e-02: 6.33e-02 p=1.00e-01: 7.96e-02 p=1.10e-01: 1.07e-01
Mean LER = 6.693e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.48317813873291 s

Training epoch 71, Batch 49/98: LR=8.72e-04, Loss=1.89169e-01 LER=6.914e-02
***Loss=1.89169e-01
Training epoch 71, Batch 98/98: LR=8.72e-04, Loss=1.90008e-01 LER=6.848e-02
***Loss=1.90008e-01
Epoch 71 Train Time 11.68207597732544s

Model Saved - New best loss: 1.90008e-01
Training epoch 72, Batch 49/98: LR=8.68e-04, Loss=1.87426e-01 LER=6.726e-02
***Loss=1.87426e-01
Training epoch 72, Batch 98/98: LR=8.68e-04, Loss=1.90247e-01 LER=6.875e-02
***Loss=1.90247e-01
Epoch 72 Train Time 11.448294878005981s

No improvement. Patience: 1/40
Training epoch 73, Batch 49/98: LR=8.65e-04, Loss=1.94229e-01 LER=7.061e-02
***Loss=1.94229e-01
Training epoch 73, Batch 98/98: LR=8.65e-04, Loss=1.92421e-01 LER=6.943e-02
***Loss=1.92421e-01
Epoch 73 Train Time 11.008853435516357s

No improvement. Patience: 2/40
Training epoch 74, Batch 49/98: LR=8.61e-04, Loss=1.88679e-01 LER=6.948e-02
***Loss=1.88679e-01
Training epoch 74, Batch 98/98: LR=8.61e-04, Loss=1.87896e-01 LER=6.803e-02
***Loss=1.87896e-01
Epoch 74 Train Time 11.553088665008545s

Model Saved - New best loss: 1.87896e-01
Training epoch 75, Batch 49/98: LR=8.57e-04, Loss=1.90307e-01 LER=6.882e-02
***Loss=1.90307e-01
Training epoch 75, Batch 98/98: LR=8.57e-04, Loss=1.88779e-01 LER=6.790e-02
***Loss=1.88779e-01
Epoch 75 Train Time 11.253601312637329s

No improvement. Patience: 1/40
Training epoch 76, Batch 49/98: LR=8.54e-04, Loss=1.88789e-01 LER=6.926e-02
***Loss=1.88789e-01
Training epoch 76, Batch 98/98: LR=8.54e-04, Loss=1.88211e-01 LER=6.823e-02
***Loss=1.88211e-01
Epoch 76 Train Time 11.516906976699829s

No improvement. Patience: 2/40
Training epoch 77, Batch 49/98: LR=8.50e-04, Loss=1.94586e-01 LER=7.035e-02
***Loss=1.94586e-01
Training epoch 77, Batch 98/98: LR=8.50e-04, Loss=1.92836e-01 LER=6.966e-02
***Loss=1.92836e-01
Epoch 77 Train Time 11.333661317825317s

No improvement. Patience: 3/40
Training epoch 78, Batch 49/98: LR=8.46e-04, Loss=1.90837e-01 LER=6.950e-02
***Loss=1.90837e-01
Training epoch 78, Batch 98/98: LR=8.46e-04, Loss=1.88339e-01 LER=6.844e-02
***Loss=1.88339e-01
Epoch 78 Train Time 11.836324453353882s

No improvement. Patience: 4/40
Training epoch 79, Batch 49/98: LR=8.42e-04, Loss=1.85942e-01 LER=6.509e-02
***Loss=1.85942e-01
Training epoch 79, Batch 98/98: LR=8.42e-04, Loss=1.86120e-01 LER=6.707e-02
***Loss=1.86120e-01
Epoch 79 Train Time 11.470178604125977s

Model Saved - New best loss: 1.86120e-01
Training epoch 80, Batch 49/98: LR=8.39e-04, Loss=1.90107e-01 LER=6.912e-02
***Loss=1.90107e-01
Training epoch 80, Batch 98/98: LR=8.39e-04, Loss=1.88476e-01 LER=6.782e-02
***Loss=1.88476e-01
Epoch 80 Train Time 11.443588495254517s

No improvement. Patience: 1/40
Test LER  p=7.00e-02: 3.82e-02 p=8.00e-02: 5.20e-02 p=9.00e-02: 6.88e-02 p=1.00e-01: 8.96e-02 p=1.10e-01: 1.06e-01
Mean LER = 7.088e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.875693798065186 s

Training epoch 81, Batch 49/98: LR=8.35e-04, Loss=1.87290e-01 LER=6.682e-02
***Loss=1.87290e-01
Training epoch 81, Batch 98/98: LR=8.35e-04, Loss=1.88956e-01 LER=6.811e-02
***Loss=1.88956e-01
Epoch 81 Train Time 11.209707736968994s

No improvement. Patience: 2/40
Training epoch 82, Batch 49/98: LR=8.31e-04, Loss=1.84934e-01 LER=6.607e-02
***Loss=1.84934e-01
Training epoch 82, Batch 98/98: LR=8.31e-04, Loss=1.86575e-01 LER=6.678e-02
***Loss=1.86575e-01
Epoch 82 Train Time 11.913445234298706s

No improvement. Patience: 3/40
Training epoch 83, Batch 49/98: LR=8.27e-04, Loss=1.85855e-01 LER=6.663e-02
***Loss=1.85855e-01
Training epoch 83, Batch 98/98: LR=8.27e-04, Loss=1.90099e-01 LER=6.871e-02
***Loss=1.90099e-01
Epoch 83 Train Time 11.678345680236816s

No improvement. Patience: 4/40
Training epoch 84, Batch 49/98: LR=8.23e-04, Loss=1.91812e-01 LER=6.965e-02
***Loss=1.91812e-01
Training epoch 84, Batch 98/98: LR=8.23e-04, Loss=1.89140e-01 LER=6.867e-02
***Loss=1.89140e-01
Epoch 84 Train Time 11.347400426864624s

No improvement. Patience: 5/40
Training epoch 85, Batch 49/98: LR=8.19e-04, Loss=1.88185e-01 LER=6.738e-02
***Loss=1.88185e-01
Training epoch 85, Batch 98/98: LR=8.19e-04, Loss=1.89907e-01 LER=6.825e-02
***Loss=1.89907e-01
Epoch 85 Train Time 11.326336145401001s

No improvement. Patience: 6/40
Training epoch 86, Batch 49/98: LR=8.15e-04, Loss=1.87072e-01 LER=6.736e-02
***Loss=1.87072e-01
Training epoch 86, Batch 98/98: LR=8.15e-04, Loss=1.87872e-01 LER=6.800e-02
***Loss=1.87872e-01
Epoch 86 Train Time 11.362491130828857s

No improvement. Patience: 7/40
Training epoch 87, Batch 49/98: LR=8.11e-04, Loss=1.86886e-01 LER=6.820e-02
***Loss=1.86886e-01
Training epoch 87, Batch 98/98: LR=8.11e-04, Loss=1.86561e-01 LER=6.740e-02
***Loss=1.86561e-01
Epoch 87 Train Time 11.2802255153656s

No improvement. Patience: 8/40
Training epoch 88, Batch 49/98: LR=8.07e-04, Loss=1.87492e-01 LER=6.868e-02
***Loss=1.87492e-01
Training epoch 88, Batch 98/98: LR=8.07e-04, Loss=1.87827e-01 LER=6.846e-02
***Loss=1.87827e-01
Epoch 88 Train Time 11.416439056396484s

No improvement. Patience: 9/40
Training epoch 89, Batch 49/98: LR=8.02e-04, Loss=1.87339e-01 LER=6.768e-02
***Loss=1.87339e-01
Training epoch 89, Batch 98/98: LR=8.02e-04, Loss=1.86051e-01 LER=6.689e-02
***Loss=1.86051e-01
Epoch 89 Train Time 11.484134912490845s

Model Saved - New best loss: 1.86051e-01
Training epoch 90, Batch 49/98: LR=7.98e-04, Loss=1.90143e-01 LER=6.908e-02
***Loss=1.90143e-01
Training epoch 90, Batch 98/98: LR=7.98e-04, Loss=1.89889e-01 LER=6.877e-02
***Loss=1.89889e-01
Epoch 90 Train Time 11.432734966278076s

No improvement. Patience: 1/40
Test LER  p=7.00e-02: 3.82e-02 p=8.00e-02: 4.80e-02 p=9.00e-02: 6.48e-02 p=1.00e-01: 8.54e-02 p=1.10e-01: 1.02e-01
Mean LER = 6.762e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.386106967926025 s

Training epoch 91, Batch 49/98: LR=7.94e-04, Loss=1.89878e-01 LER=6.804e-02
***Loss=1.89878e-01
Training epoch 91, Batch 98/98: LR=7.94e-04, Loss=1.87495e-01 LER=6.784e-02
***Loss=1.87495e-01
Epoch 91 Train Time 11.145551681518555s

No improvement. Patience: 2/40
Training epoch 92, Batch 49/98: LR=7.90e-04, Loss=1.85355e-01 LER=6.589e-02
***Loss=1.85355e-01
Training epoch 92, Batch 98/98: LR=7.90e-04, Loss=1.87915e-01 LER=6.718e-02
***Loss=1.87915e-01
Epoch 92 Train Time 11.573165655136108s

No improvement. Patience: 3/40
Training epoch 93, Batch 49/98: LR=7.86e-04, Loss=1.81471e-01 LER=6.441e-02
***Loss=1.81471e-01
Training epoch 93, Batch 98/98: LR=7.86e-04, Loss=1.84898e-01 LER=6.575e-02
***Loss=1.84898e-01
Epoch 93 Train Time 11.117372035980225s

Model Saved - New best loss: 1.84898e-01
Training epoch 94, Batch 49/98: LR=7.81e-04, Loss=1.85664e-01 LER=6.643e-02
***Loss=1.85664e-01
Training epoch 94, Batch 98/98: LR=7.81e-04, Loss=1.86804e-01 LER=6.751e-02
***Loss=1.86804e-01
Epoch 94 Train Time 11.654946565628052s

No improvement. Patience: 1/40
Training epoch 95, Batch 49/98: LR=7.77e-04, Loss=1.89433e-01 LER=6.730e-02
***Loss=1.89433e-01
Training epoch 95, Batch 98/98: LR=7.77e-04, Loss=1.88549e-01 LER=6.794e-02
***Loss=1.88549e-01
Epoch 95 Train Time 11.351741552352905s

No improvement. Patience: 2/40
Training epoch 96, Batch 49/98: LR=7.73e-04, Loss=1.87621e-01 LER=6.764e-02
***Loss=1.87621e-01
Training epoch 96, Batch 98/98: LR=7.73e-04, Loss=1.87327e-01 LER=6.728e-02
***Loss=1.87327e-01
Epoch 96 Train Time 11.218462944030762s

No improvement. Patience: 3/40
Training epoch 97, Batch 49/98: LR=7.68e-04, Loss=1.88926e-01 LER=6.856e-02
***Loss=1.88926e-01
Training epoch 97, Batch 98/98: LR=7.68e-04, Loss=1.86956e-01 LER=6.843e-02
***Loss=1.86956e-01
Epoch 97 Train Time 11.120009422302246s

No improvement. Patience: 4/40
Training epoch 98, Batch 49/98: LR=7.64e-04, Loss=1.82650e-01 LER=6.585e-02
***Loss=1.82650e-01
Training epoch 98, Batch 98/98: LR=7.64e-04, Loss=1.85437e-01 LER=6.667e-02
***Loss=1.85437e-01
Epoch 98 Train Time 11.43804121017456s

No improvement. Patience: 5/40
Training epoch 99, Batch 49/98: LR=7.59e-04, Loss=1.84678e-01 LER=6.742e-02
***Loss=1.84678e-01
Training epoch 99, Batch 98/98: LR=7.59e-04, Loss=1.84602e-01 LER=6.698e-02
***Loss=1.84602e-01
Epoch 99 Train Time 11.71640396118164s

Model Saved - New best loss: 1.84602e-01
Training epoch 100, Batch 49/98: LR=7.55e-04, Loss=1.83822e-01 LER=6.665e-02
***Loss=1.83822e-01
Training epoch 100, Batch 98/98: LR=7.55e-04, Loss=1.85439e-01 LER=6.732e-02
***Loss=1.85439e-01
Epoch 100 Train Time 11.427355289459229s

No improvement. Patience: 1/40
Test LER  p=7.00e-02: 3.67e-02 p=8.00e-02: 5.06e-02 p=9.00e-02: 6.63e-02 p=1.00e-01: 7.66e-02 p=1.10e-01: 1.06e-01
Mean LER = 6.719e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.47087597846985 s

Training epoch 101, Batch 49/98: LR=7.50e-04, Loss=1.85756e-01 LER=6.750e-02
***Loss=1.85756e-01
Training epoch 101, Batch 98/98: LR=7.50e-04, Loss=1.84075e-01 LER=6.651e-02
***Loss=1.84075e-01
Epoch 101 Train Time 11.802835464477539s

Model Saved - New best loss: 1.84075e-01
Training epoch 102, Batch 49/98: LR=7.46e-04, Loss=1.83397e-01 LER=6.601e-02
***Loss=1.83397e-01
Training epoch 102, Batch 98/98: LR=7.46e-04, Loss=1.86944e-01 LER=6.719e-02
***Loss=1.86944e-01
Epoch 102 Train Time 11.59418773651123s

No improvement. Patience: 1/40
Training epoch 103, Batch 49/98: LR=7.41e-04, Loss=1.85731e-01 LER=6.583e-02
***Loss=1.85731e-01
Training epoch 103, Batch 98/98: LR=7.41e-04, Loss=1.83540e-01 LER=6.572e-02
***Loss=1.83540e-01
Epoch 103 Train Time 11.501515865325928s

Model Saved - New best loss: 1.83540e-01
Training epoch 104, Batch 49/98: LR=7.37e-04, Loss=1.81845e-01 LER=6.607e-02
***Loss=1.81845e-01
Training epoch 104, Batch 98/98: LR=7.37e-04, Loss=1.81908e-01 LER=6.648e-02
***Loss=1.81908e-01
Epoch 104 Train Time 11.442450284957886s

Model Saved - New best loss: 1.81908e-01
Training epoch 105, Batch 49/98: LR=7.32e-04, Loss=1.82434e-01 LER=6.744e-02
***Loss=1.82434e-01
Training epoch 105, Batch 98/98: LR=7.32e-04, Loss=1.84532e-01 LER=6.679e-02
***Loss=1.84532e-01
Epoch 105 Train Time 11.645578861236572s

No improvement. Patience: 1/40
Training epoch 106, Batch 49/98: LR=7.27e-04, Loss=1.80977e-01 LER=6.465e-02
***Loss=1.80977e-01
Training epoch 106, Batch 98/98: LR=7.27e-04, Loss=1.82270e-01 LER=6.554e-02
***Loss=1.82270e-01
Epoch 106 Train Time 11.262581586837769s

No improvement. Patience: 2/40
Training epoch 107, Batch 49/98: LR=7.23e-04, Loss=1.83709e-01 LER=6.485e-02
***Loss=1.83709e-01
Training epoch 107, Batch 98/98: LR=7.23e-04, Loss=1.83239e-01 LER=6.583e-02
***Loss=1.83239e-01
Epoch 107 Train Time 11.483826160430908s

No improvement. Patience: 3/40
Training epoch 108, Batch 49/98: LR=7.18e-04, Loss=1.85684e-01 LER=6.675e-02
***Loss=1.85684e-01
Training epoch 108, Batch 98/98: LR=7.18e-04, Loss=1.85696e-01 LER=6.764e-02
***Loss=1.85696e-01
Epoch 108 Train Time 11.211692333221436s

No improvement. Patience: 4/40
Training epoch 109, Batch 49/98: LR=7.13e-04, Loss=1.89665e-01 LER=6.858e-02
***Loss=1.89665e-01
Training epoch 109, Batch 98/98: LR=7.13e-04, Loss=1.87317e-01 LER=6.788e-02
***Loss=1.87317e-01
Epoch 109 Train Time 11.379430055618286s

No improvement. Patience: 5/40
Training epoch 110, Batch 49/98: LR=7.08e-04, Loss=1.81071e-01 LER=6.497e-02
***Loss=1.81071e-01
Training epoch 110, Batch 98/98: LR=7.08e-04, Loss=1.82216e-01 LER=6.601e-02
***Loss=1.82216e-01
Epoch 110 Train Time 11.919375896453857s

No improvement. Patience: 6/40
Test LER  p=7.00e-02: 3.92e-02 p=8.00e-02: 5.01e-02 p=9.00e-02: 6.91e-02 p=1.00e-01: 8.06e-02 p=1.10e-01: 1.03e-01
Mean LER = 6.844e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.386930227279663 s

Training epoch 111, Batch 49/98: LR=7.04e-04, Loss=1.87275e-01 LER=6.838e-02
***Loss=1.87275e-01
Training epoch 111, Batch 98/98: LR=7.04e-04, Loss=1.83402e-01 LER=6.700e-02
***Loss=1.83402e-01
Epoch 111 Train Time 11.038678884506226s

No improvement. Patience: 7/40
Training epoch 112, Batch 49/98: LR=6.99e-04, Loss=1.79266e-01 LER=6.525e-02
***Loss=1.79266e-01
Training epoch 112, Batch 98/98: LR=6.99e-04, Loss=1.81699e-01 LER=6.569e-02
***Loss=1.81699e-01
Epoch 112 Train Time 11.256266355514526s

Model Saved - New best loss: 1.81699e-01
Training epoch 113, Batch 49/98: LR=6.94e-04, Loss=1.81353e-01 LER=6.433e-02
***Loss=1.81353e-01
Training epoch 113, Batch 98/98: LR=6.94e-04, Loss=1.81822e-01 LER=6.501e-02
***Loss=1.81822e-01
Epoch 113 Train Time 11.151009559631348s

No improvement. Patience: 1/40
Training epoch 114, Batch 49/98: LR=6.89e-04, Loss=1.84219e-01 LER=6.708e-02
***Loss=1.84219e-01
Training epoch 114, Batch 98/98: LR=6.89e-04, Loss=1.82979e-01 LER=6.606e-02
***Loss=1.82979e-01
Epoch 114 Train Time 11.729880571365356s

No improvement. Patience: 2/40
Training epoch 115, Batch 49/98: LR=6.84e-04, Loss=1.77703e-01 LER=6.306e-02
***Loss=1.77703e-01
Training epoch 115, Batch 98/98: LR=6.84e-04, Loss=1.80585e-01 LER=6.512e-02
***Loss=1.80585e-01
Epoch 115 Train Time 11.219234704971313s

Model Saved - New best loss: 1.80585e-01
Training epoch 116, Batch 49/98: LR=6.80e-04, Loss=1.82512e-01 LER=6.525e-02
***Loss=1.82512e-01
Training epoch 116, Batch 98/98: LR=6.80e-04, Loss=1.84464e-01 LER=6.670e-02
***Loss=1.84464e-01
Epoch 116 Train Time 11.592846632003784s

No improvement. Patience: 1/40
Training epoch 117, Batch 49/98: LR=6.75e-04, Loss=1.83788e-01 LER=6.599e-02
***Loss=1.83788e-01
Training epoch 117, Batch 98/98: LR=6.75e-04, Loss=1.82812e-01 LER=6.520e-02
***Loss=1.82812e-01
Epoch 117 Train Time 11.413742542266846s

No improvement. Patience: 2/40
Training epoch 118, Batch 49/98: LR=6.70e-04, Loss=1.81439e-01 LER=6.515e-02
***Loss=1.81439e-01
Training epoch 118, Batch 98/98: LR=6.70e-04, Loss=1.82732e-01 LER=6.522e-02
***Loss=1.82732e-01
Epoch 118 Train Time 11.361931085586548s

No improvement. Patience: 3/40
Training epoch 119, Batch 49/98: LR=6.65e-04, Loss=1.79331e-01 LER=6.521e-02
***Loss=1.79331e-01
Training epoch 119, Batch 98/98: LR=6.65e-04, Loss=1.82318e-01 LER=6.560e-02
***Loss=1.82318e-01
Epoch 119 Train Time 11.344234704971313s

No improvement. Patience: 4/40
Training epoch 120, Batch 49/98: LR=6.60e-04, Loss=1.82846e-01 LER=6.541e-02
***Loss=1.82846e-01
Training epoch 120, Batch 98/98: LR=6.60e-04, Loss=1.81222e-01 LER=6.475e-02
***Loss=1.81222e-01
Epoch 120 Train Time 11.349929094314575s

No improvement. Patience: 5/40
Test LER  p=7.00e-02: 3.45e-02 p=8.00e-02: 4.35e-02 p=9.00e-02: 6.50e-02 p=1.00e-01: 8.11e-02 p=1.10e-01: 1.04e-01
Mean LER = 6.566e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.674497842788696 s

Training epoch 121, Batch 49/98: LR=6.55e-04, Loss=1.86689e-01 LER=6.852e-02
***Loss=1.86689e-01
Training epoch 121, Batch 98/98: LR=6.55e-04, Loss=1.84896e-01 LER=6.742e-02
***Loss=1.84896e-01
Epoch 121 Train Time 11.421925067901611s

No improvement. Patience: 6/40
Training epoch 122, Batch 49/98: LR=6.50e-04, Loss=1.85874e-01 LER=6.696e-02
***Loss=1.85874e-01
Training epoch 122, Batch 98/98: LR=6.50e-04, Loss=1.83705e-01 LER=6.607e-02
***Loss=1.83705e-01
Epoch 122 Train Time 11.226822853088379s

No improvement. Patience: 7/40
Training epoch 123, Batch 49/98: LR=6.45e-04, Loss=1.89852e-01 LER=6.768e-02
***Loss=1.89852e-01
Training epoch 123, Batch 98/98: LR=6.45e-04, Loss=1.85240e-01 LER=6.685e-02
***Loss=1.85240e-01
Epoch 123 Train Time 11.359557867050171s

No improvement. Patience: 8/40
Training epoch 124, Batch 49/98: LR=6.40e-04, Loss=1.84255e-01 LER=6.577e-02
***Loss=1.84255e-01
Training epoch 124, Batch 98/98: LR=6.40e-04, Loss=1.83559e-01 LER=6.543e-02
***Loss=1.83559e-01
Epoch 124 Train Time 10.99891972541809s

No improvement. Patience: 9/40
Training epoch 125, Batch 49/98: LR=6.35e-04, Loss=1.80350e-01 LER=6.669e-02
***Loss=1.80350e-01
Training epoch 125, Batch 98/98: LR=6.35e-04, Loss=1.80491e-01 LER=6.628e-02
***Loss=1.80491e-01
Epoch 125 Train Time 11.50464677810669s

Model Saved - New best loss: 1.80491e-01
Training epoch 126, Batch 49/98: LR=6.30e-04, Loss=1.82432e-01 LER=6.575e-02
***Loss=1.82432e-01
Training epoch 126, Batch 98/98: LR=6.30e-04, Loss=1.83340e-01 LER=6.634e-02
***Loss=1.83340e-01
Epoch 126 Train Time 11.654435873031616s

No improvement. Patience: 1/40
Training epoch 127, Batch 49/98: LR=6.25e-04, Loss=1.83018e-01 LER=6.571e-02
***Loss=1.83018e-01
Training epoch 127, Batch 98/98: LR=6.25e-04, Loss=1.81064e-01 LER=6.517e-02
***Loss=1.81064e-01
Epoch 127 Train Time 11.633032321929932s

No improvement. Patience: 2/40
Training epoch 128, Batch 49/98: LR=6.20e-04, Loss=1.83878e-01 LER=6.571e-02
***Loss=1.83878e-01
Training epoch 128, Batch 98/98: LR=6.20e-04, Loss=1.82162e-01 LER=6.565e-02
***Loss=1.82162e-01
Epoch 128 Train Time 11.382593154907227s

No improvement. Patience: 3/40
Training epoch 129, Batch 49/98: LR=6.15e-04, Loss=1.82894e-01 LER=6.659e-02
***Loss=1.82894e-01
Training epoch 129, Batch 98/98: LR=6.15e-04, Loss=1.82615e-01 LER=6.597e-02
***Loss=1.82615e-01
Epoch 129 Train Time 11.365064144134521s

No improvement. Patience: 4/40
Training epoch 130, Batch 49/98: LR=6.09e-04, Loss=1.80001e-01 LER=6.595e-02
***Loss=1.80001e-01
Training epoch 130, Batch 98/98: LR=6.09e-04, Loss=1.79558e-01 LER=6.492e-02
***Loss=1.79558e-01
Epoch 130 Train Time 11.497743368148804s

Model Saved - New best loss: 1.79558e-01
Test LER  p=7.00e-02: 3.79e-02 p=8.00e-02: 4.53e-02 p=9.00e-02: 6.60e-02 p=1.00e-01: 8.23e-02 p=1.10e-01: 9.79e-02
Mean LER = 6.590e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.344620943069458 s

Training epoch 131, Batch 49/98: LR=6.04e-04, Loss=1.77446e-01 LER=6.487e-02
***Loss=1.77446e-01
Training epoch 131, Batch 98/98: LR=6.04e-04, Loss=1.81588e-01 LER=6.636e-02
***Loss=1.81588e-01
Epoch 131 Train Time 11.599963903427124s

No improvement. Patience: 1/40
Training epoch 132, Batch 49/98: LR=5.99e-04, Loss=1.79863e-01 LER=6.573e-02
***Loss=1.79863e-01
Training epoch 132, Batch 98/98: LR=5.99e-04, Loss=1.76909e-01 LER=6.408e-02
***Loss=1.76909e-01
Epoch 132 Train Time 11.388869285583496s

Model Saved - New best loss: 1.76909e-01
Training epoch 133, Batch 49/98: LR=5.94e-04, Loss=1.80551e-01 LER=6.641e-02
***Loss=1.80551e-01
Training epoch 133, Batch 98/98: LR=5.94e-04, Loss=1.80925e-01 LER=6.610e-02
***Loss=1.80925e-01
Epoch 133 Train Time 11.58354926109314s

No improvement. Patience: 1/40
Training epoch 134, Batch 49/98: LR=5.89e-04, Loss=1.78373e-01 LER=6.579e-02
***Loss=1.78373e-01
Training epoch 134, Batch 98/98: LR=5.89e-04, Loss=1.79434e-01 LER=6.560e-02
***Loss=1.79434e-01
Epoch 134 Train Time 11.49328088760376s

No improvement. Patience: 2/40
Training epoch 135, Batch 49/98: LR=5.84e-04, Loss=1.78795e-01 LER=6.459e-02
***Loss=1.78795e-01
Training epoch 135, Batch 98/98: LR=5.84e-04, Loss=1.79709e-01 LER=6.522e-02
***Loss=1.79709e-01
Epoch 135 Train Time 11.48921799659729s

No improvement. Patience: 3/40
Training epoch 136, Batch 49/98: LR=5.79e-04, Loss=1.82098e-01 LER=6.619e-02
***Loss=1.82098e-01
Training epoch 136, Batch 98/98: LR=5.79e-04, Loss=1.83272e-01 LER=6.629e-02
***Loss=1.83272e-01
Epoch 136 Train Time 11.499675035476685s

No improvement. Patience: 4/40
Training epoch 137, Batch 49/98: LR=5.73e-04, Loss=1.78510e-01 LER=6.427e-02
***Loss=1.78510e-01
Training epoch 137, Batch 98/98: LR=5.73e-04, Loss=1.78920e-01 LER=6.425e-02
***Loss=1.78920e-01
Epoch 137 Train Time 11.858166456222534s

No improvement. Patience: 5/40
Training epoch 138, Batch 49/98: LR=5.68e-04, Loss=1.79578e-01 LER=6.392e-02
***Loss=1.79578e-01
Training epoch 138, Batch 98/98: LR=5.68e-04, Loss=1.81156e-01 LER=6.497e-02
***Loss=1.81156e-01
Epoch 138 Train Time 11.685499668121338s

No improvement. Patience: 6/40
Training epoch 139, Batch 49/98: LR=5.63e-04, Loss=1.81415e-01 LER=6.523e-02
***Loss=1.81415e-01
Training epoch 139, Batch 98/98: LR=5.63e-04, Loss=1.81460e-01 LER=6.518e-02
***Loss=1.81460e-01
Epoch 139 Train Time 11.498510360717773s

No improvement. Patience: 7/40
Training epoch 140, Batch 49/98: LR=5.58e-04, Loss=1.78720e-01 LER=6.378e-02
***Loss=1.78720e-01
Training epoch 140, Batch 98/98: LR=5.58e-04, Loss=1.79989e-01 LER=6.472e-02
***Loss=1.79989e-01
Epoch 140 Train Time 11.33251142501831s

No improvement. Patience: 8/40
Test LER  p=7.00e-02: 3.18e-02 p=8.00e-02: 5.11e-02 p=9.00e-02: 6.28e-02 p=1.00e-01: 8.17e-02 p=1.10e-01: 9.76e-02
Mean LER = 6.500e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.51694631576538 s

Training epoch 141, Batch 49/98: LR=5.53e-04, Loss=1.82864e-01 LER=6.617e-02
***Loss=1.82864e-01
Training epoch 141, Batch 98/98: LR=5.53e-04, Loss=1.81908e-01 LER=6.564e-02
***Loss=1.81908e-01
Epoch 141 Train Time 11.290883541107178s

No improvement. Patience: 9/40
Training epoch 142, Batch 49/98: LR=5.48e-04, Loss=1.80017e-01 LER=6.559e-02
***Loss=1.80017e-01
Training epoch 142, Batch 98/98: LR=5.48e-04, Loss=1.79926e-01 LER=6.531e-02
***Loss=1.79926e-01
Epoch 142 Train Time 11.266177892684937s

No improvement. Patience: 10/40
Training epoch 143, Batch 49/98: LR=5.42e-04, Loss=1.78828e-01 LER=6.475e-02
***Loss=1.78828e-01
Training epoch 143, Batch 98/98: LR=5.42e-04, Loss=1.78790e-01 LER=6.476e-02
***Loss=1.78790e-01
Epoch 143 Train Time 11.58638048171997s

No improvement. Patience: 11/40
Training epoch 144, Batch 49/98: LR=5.37e-04, Loss=1.81077e-01 LER=6.501e-02
***Loss=1.81077e-01
Training epoch 144, Batch 98/98: LR=5.37e-04, Loss=1.80016e-01 LER=6.495e-02
***Loss=1.80016e-01
Epoch 144 Train Time 11.436667919158936s

No improvement. Patience: 12/40
Training epoch 145, Batch 49/98: LR=5.32e-04, Loss=1.78920e-01 LER=6.573e-02
***Loss=1.78920e-01
Training epoch 145, Batch 98/98: LR=5.32e-04, Loss=1.80736e-01 LER=6.588e-02
***Loss=1.80736e-01
Epoch 145 Train Time 11.144673824310303s

No improvement. Patience: 13/40
Training epoch 146, Batch 49/98: LR=5.27e-04, Loss=1.78817e-01 LER=6.397e-02
***Loss=1.78817e-01
Training epoch 146, Batch 98/98: LR=5.27e-04, Loss=1.79847e-01 LER=6.426e-02
***Loss=1.79847e-01
Epoch 146 Train Time 11.049736022949219s

No improvement. Patience: 14/40
Training epoch 147, Batch 49/98: LR=5.21e-04, Loss=1.76738e-01 LER=6.320e-02
***Loss=1.76738e-01
Training epoch 147, Batch 98/98: LR=5.21e-04, Loss=1.77198e-01 LER=6.362e-02
***Loss=1.77198e-01
Epoch 147 Train Time 10.86820912361145s

No improvement. Patience: 15/40
Training epoch 148, Batch 49/98: LR=5.16e-04, Loss=1.78322e-01 LER=6.477e-02
***Loss=1.78322e-01
Training epoch 148, Batch 98/98: LR=5.16e-04, Loss=1.80845e-01 LER=6.540e-02
***Loss=1.80845e-01
Epoch 148 Train Time 11.515893459320068s

No improvement. Patience: 16/40
Training epoch 149, Batch 49/98: LR=5.11e-04, Loss=1.80990e-01 LER=6.447e-02
***Loss=1.80990e-01
Training epoch 149, Batch 98/98: LR=5.11e-04, Loss=1.79404e-01 LER=6.438e-02
***Loss=1.79404e-01
Epoch 149 Train Time 11.356922149658203s

No improvement. Patience: 17/40
Training epoch 150, Batch 49/98: LR=5.06e-04, Loss=1.80968e-01 LER=6.525e-02
***Loss=1.80968e-01
Training epoch 150, Batch 98/98: LR=5.06e-04, Loss=1.80246e-01 LER=6.542e-02
***Loss=1.80246e-01
Epoch 150 Train Time 11.577794790267944s

No improvement. Patience: 18/40
Test LER  p=7.00e-02: 3.37e-02 p=8.00e-02: 4.52e-02 p=9.00e-02: 6.46e-02 p=1.00e-01: 8.30e-02 p=1.10e-01: 1.02e-01
Mean LER = 6.572e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.946173667907715 s

Training epoch 151, Batch 49/98: LR=5.01e-04, Loss=1.78904e-01 LER=6.463e-02
***Loss=1.78904e-01
Training epoch 151, Batch 98/98: LR=5.01e-04, Loss=1.78668e-01 LER=6.453e-02
***Loss=1.78668e-01
Epoch 151 Train Time 11.32593321800232s

No improvement. Patience: 19/40
Training epoch 152, Batch 49/98: LR=4.95e-04, Loss=1.77147e-01 LER=6.340e-02
***Loss=1.77147e-01
Training epoch 152, Batch 98/98: LR=4.95e-04, Loss=1.77074e-01 LER=6.412e-02
***Loss=1.77074e-01
Epoch 152 Train Time 11.262086153030396s

No improvement. Patience: 20/40
Training epoch 153, Batch 49/98: LR=4.90e-04, Loss=1.81072e-01 LER=6.585e-02
***Loss=1.81072e-01
Training epoch 153, Batch 98/98: LR=4.90e-04, Loss=1.79050e-01 LER=6.487e-02
***Loss=1.79050e-01
Epoch 153 Train Time 11.4818856716156s

No improvement. Patience: 21/40
Training epoch 154, Batch 49/98: LR=4.85e-04, Loss=1.82052e-01 LER=6.678e-02
***Loss=1.82052e-01
Training epoch 154, Batch 98/98: LR=4.85e-04, Loss=1.80847e-01 LER=6.591e-02
***Loss=1.80847e-01
Epoch 154 Train Time 12.219034433364868s

No improvement. Patience: 22/40
Training epoch 155, Batch 49/98: LR=4.80e-04, Loss=1.84390e-01 LER=6.742e-02
***Loss=1.84390e-01
Training epoch 155, Batch 98/98: LR=4.80e-04, Loss=1.80608e-01 LER=6.591e-02
***Loss=1.80608e-01
Epoch 155 Train Time 11.492823600769043s

No improvement. Patience: 23/40
Training epoch 156, Batch 49/98: LR=4.74e-04, Loss=1.79342e-01 LER=6.425e-02
***Loss=1.79342e-01
Training epoch 156, Batch 98/98: LR=4.74e-04, Loss=1.80093e-01 LER=6.491e-02
***Loss=1.80093e-01
Epoch 156 Train Time 11.507188081741333s

No improvement. Patience: 24/40
Training epoch 157, Batch 49/98: LR=4.69e-04, Loss=1.83643e-01 LER=6.678e-02
***Loss=1.83643e-01
Training epoch 157, Batch 98/98: LR=4.69e-04, Loss=1.81129e-01 LER=6.560e-02
***Loss=1.81129e-01
Epoch 157 Train Time 11.536736011505127s

No improvement. Patience: 25/40
Training epoch 158, Batch 49/98: LR=4.64e-04, Loss=1.77617e-01 LER=6.471e-02
***Loss=1.77617e-01
Training epoch 158, Batch 98/98: LR=4.64e-04, Loss=1.75953e-01 LER=6.373e-02
***Loss=1.75953e-01
Epoch 158 Train Time 11.267632246017456s

Model Saved - New best loss: 1.75953e-01
Training epoch 159, Batch 49/98: LR=4.59e-04, Loss=1.73473e-01 LER=6.278e-02
***Loss=1.73473e-01
Training epoch 159, Batch 98/98: LR=4.59e-04, Loss=1.75692e-01 LER=6.334e-02
***Loss=1.75692e-01
Epoch 159 Train Time 11.425821542739868s

Model Saved - New best loss: 1.75692e-01
Training epoch 160, Batch 49/98: LR=4.53e-04, Loss=1.72571e-01 LER=6.218e-02
***Loss=1.72571e-01
Training epoch 160, Batch 98/98: LR=4.53e-04, Loss=1.72911e-01 LER=6.218e-02
***Loss=1.72911e-01
Epoch 160 Train Time 11.55887484550476s

Model Saved - New best loss: 1.72911e-01
Test LER  p=7.00e-02: 3.40e-02 p=8.00e-02: 5.03e-02 p=9.00e-02: 6.44e-02 p=1.00e-01: 7.02e-02 p=1.10e-01: 1.04e-01
Mean LER = 6.455e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.448448419570923 s

Training epoch 161, Batch 49/98: LR=4.48e-04, Loss=1.79266e-01 LER=6.543e-02
***Loss=1.79266e-01
Training epoch 161, Batch 98/98: LR=4.48e-04, Loss=1.78006e-01 LER=6.429e-02
***Loss=1.78006e-01
Epoch 161 Train Time 11.29438066482544s

No improvement. Patience: 1/40
Training epoch 162, Batch 49/98: LR=4.43e-04, Loss=1.77858e-01 LER=6.409e-02
***Loss=1.77858e-01
Training epoch 162, Batch 98/98: LR=4.43e-04, Loss=1.77736e-01 LER=6.430e-02
***Loss=1.77736e-01
Epoch 162 Train Time 11.300082445144653s

No improvement. Patience: 2/40
Training epoch 163, Batch 49/98: LR=4.38e-04, Loss=1.76837e-01 LER=6.419e-02
***Loss=1.76837e-01
Training epoch 163, Batch 98/98: LR=4.38e-04, Loss=1.76692e-01 LER=6.430e-02
***Loss=1.76692e-01
Epoch 163 Train Time 11.666927814483643s

No improvement. Patience: 3/40
Training epoch 164, Batch 49/98: LR=4.33e-04, Loss=1.79064e-01 LER=6.503e-02
***Loss=1.79064e-01
Training epoch 164, Batch 98/98: LR=4.33e-04, Loss=1.79465e-01 LER=6.540e-02
***Loss=1.79465e-01
Epoch 164 Train Time 11.62965178489685s

No improvement. Patience: 4/40
Training epoch 165, Batch 49/98: LR=4.28e-04, Loss=1.76654e-01 LER=6.505e-02
***Loss=1.76654e-01
Training epoch 165, Batch 98/98: LR=4.28e-04, Loss=1.77197e-01 LER=6.470e-02
***Loss=1.77197e-01
Epoch 165 Train Time 11.050353765487671s

No improvement. Patience: 5/40
Training epoch 166, Batch 49/98: LR=4.22e-04, Loss=1.76158e-01 LER=6.409e-02
***Loss=1.76158e-01
Training epoch 166, Batch 98/98: LR=4.22e-04, Loss=1.75655e-01 LER=6.371e-02
***Loss=1.75655e-01
Epoch 166 Train Time 11.175543308258057s

No improvement. Patience: 6/40
Training epoch 167, Batch 49/98: LR=4.17e-04, Loss=1.75493e-01 LER=6.384e-02
***Loss=1.75493e-01
Training epoch 167, Batch 98/98: LR=4.17e-04, Loss=1.74273e-01 LER=6.287e-02
***Loss=1.74273e-01
Epoch 167 Train Time 11.577912092208862s

No improvement. Patience: 7/40
Training epoch 168, Batch 49/98: LR=4.12e-04, Loss=1.77599e-01 LER=6.302e-02
***Loss=1.77599e-01
Training epoch 168, Batch 98/98: LR=4.12e-04, Loss=1.75228e-01 LER=6.254e-02
***Loss=1.75228e-01
Epoch 168 Train Time 11.491084575653076s

No improvement. Patience: 8/40
Training epoch 169, Batch 49/98: LR=4.07e-04, Loss=1.78374e-01 LER=6.461e-02
***Loss=1.78374e-01
Training epoch 169, Batch 98/98: LR=4.07e-04, Loss=1.76321e-01 LER=6.358e-02
***Loss=1.76321e-01
Epoch 169 Train Time 11.500105381011963s

No improvement. Patience: 9/40
Training epoch 170, Batch 49/98: LR=4.02e-04, Loss=1.79687e-01 LER=6.647e-02
***Loss=1.79687e-01
Training epoch 170, Batch 98/98: LR=4.02e-04, Loss=1.77604e-01 LER=6.473e-02
***Loss=1.77604e-01
Epoch 170 Train Time 11.478079557418823s

No improvement. Patience: 10/40
Test LER  p=7.00e-02: 3.67e-02 p=8.00e-02: 4.58e-02 p=9.00e-02: 5.97e-02 p=1.00e-01: 7.98e-02 p=1.10e-01: 9.79e-02
Mean LER = 6.398e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.449474334716797 s

Training epoch 171, Batch 49/98: LR=3.97e-04, Loss=1.77293e-01 LER=6.360e-02
***Loss=1.77293e-01
Training epoch 171, Batch 98/98: LR=3.97e-04, Loss=1.74777e-01 LER=6.347e-02
***Loss=1.74777e-01
Epoch 171 Train Time 11.553644180297852s

No improvement. Patience: 11/40
Training epoch 172, Batch 49/98: LR=3.92e-04, Loss=1.75707e-01 LER=6.244e-02
***Loss=1.75707e-01
Training epoch 172, Batch 98/98: LR=3.92e-04, Loss=1.76402e-01 LER=6.371e-02
***Loss=1.76402e-01
Epoch 172 Train Time 11.441623210906982s

No improvement. Patience: 12/40
Training epoch 173, Batch 49/98: LR=3.86e-04, Loss=1.75157e-01 LER=6.393e-02
***Loss=1.75157e-01
Training epoch 173, Batch 98/98: LR=3.86e-04, Loss=1.75172e-01 LER=6.378e-02
***Loss=1.75172e-01
Epoch 173 Train Time 11.467297554016113s

No improvement. Patience: 13/40
Training epoch 174, Batch 49/98: LR=3.81e-04, Loss=1.71235e-01 LER=6.186e-02
***Loss=1.71235e-01
Training epoch 174, Batch 98/98: LR=3.81e-04, Loss=1.72812e-01 LER=6.228e-02
***Loss=1.72812e-01
Epoch 174 Train Time 11.306156396865845s

Model Saved - New best loss: 1.72812e-01
Training epoch 175, Batch 49/98: LR=3.76e-04, Loss=1.73940e-01 LER=6.236e-02
***Loss=1.73940e-01
Training epoch 175, Batch 98/98: LR=3.76e-04, Loss=1.76213e-01 LER=6.412e-02
***Loss=1.76213e-01
Epoch 175 Train Time 11.424715518951416s

No improvement. Patience: 1/40
Training epoch 176, Batch 49/98: LR=3.71e-04, Loss=1.76775e-01 LER=6.485e-02
***Loss=1.76775e-01
Training epoch 176, Batch 98/98: LR=3.71e-04, Loss=1.76772e-01 LER=6.483e-02
***Loss=1.76772e-01
Epoch 176 Train Time 11.525872468948364s

No improvement. Patience: 2/40
Training epoch 177, Batch 49/98: LR=3.66e-04, Loss=1.76291e-01 LER=6.332e-02
***Loss=1.76291e-01
Training epoch 177, Batch 98/98: LR=3.66e-04, Loss=1.74382e-01 LER=6.350e-02
***Loss=1.74382e-01
Epoch 177 Train Time 11.17723298072815s

No improvement. Patience: 3/40
Training epoch 178, Batch 49/98: LR=3.61e-04, Loss=1.69766e-01 LER=6.300e-02
***Loss=1.69766e-01
Training epoch 178, Batch 98/98: LR=3.61e-04, Loss=1.72414e-01 LER=6.315e-02
***Loss=1.72414e-01
Epoch 178 Train Time 11.581395387649536s

Model Saved - New best loss: 1.72414e-01
Training epoch 179, Batch 49/98: LR=3.56e-04, Loss=1.74212e-01 LER=6.393e-02
***Loss=1.74212e-01
Training epoch 179, Batch 98/98: LR=3.56e-04, Loss=1.74544e-01 LER=6.351e-02
***Loss=1.74544e-01
Epoch 179 Train Time 11.394729375839233s

No improvement. Patience: 1/40
Training epoch 180, Batch 49/98: LR=3.51e-04, Loss=1.77647e-01 LER=6.364e-02
***Loss=1.77647e-01
Training epoch 180, Batch 98/98: LR=3.51e-04, Loss=1.75467e-01 LER=6.284e-02
***Loss=1.75467e-01
Epoch 180 Train Time 11.108537673950195s

No improvement. Patience: 2/40
Test LER  p=7.00e-02: 3.21e-02 p=8.00e-02: 4.59e-02 p=9.00e-02: 6.29e-02 p=1.00e-01: 7.48e-02 p=1.10e-01: 9.39e-02
Mean LER = 6.193e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.57513117790222 s

Training epoch 181, Batch 49/98: LR=3.46e-04, Loss=1.76994e-01 LER=6.405e-02
***Loss=1.76994e-01
Training epoch 181, Batch 98/98: LR=3.46e-04, Loss=1.74975e-01 LER=6.351e-02
***Loss=1.74975e-01
Epoch 181 Train Time 11.328176975250244s

No improvement. Patience: 3/40
Training epoch 182, Batch 49/98: LR=3.41e-04, Loss=1.75962e-01 LER=6.280e-02
***Loss=1.75962e-01
Training epoch 182, Batch 98/98: LR=3.41e-04, Loss=1.74942e-01 LER=6.289e-02
***Loss=1.74942e-01
Epoch 182 Train Time 11.643569946289062s

No improvement. Patience: 4/40
Training epoch 183, Batch 49/98: LR=3.36e-04, Loss=1.78434e-01 LER=6.463e-02
***Loss=1.78434e-01
Training epoch 183, Batch 98/98: LR=3.36e-04, Loss=1.77428e-01 LER=6.401e-02
***Loss=1.77428e-01
Epoch 183 Train Time 11.254201412200928s

No improvement. Patience: 5/40
Training epoch 184, Batch 49/98: LR=3.31e-04, Loss=1.76389e-01 LER=6.431e-02
***Loss=1.76389e-01
Training epoch 184, Batch 98/98: LR=3.31e-04, Loss=1.75695e-01 LER=6.335e-02
***Loss=1.75695e-01
Epoch 184 Train Time 11.670684099197388s

No improvement. Patience: 6/40
Training epoch 185, Batch 49/98: LR=3.26e-04, Loss=1.75401e-01 LER=6.334e-02
***Loss=1.75401e-01
Training epoch 185, Batch 98/98: LR=3.26e-04, Loss=1.75314e-01 LER=6.302e-02
***Loss=1.75314e-01
Epoch 185 Train Time 11.497433423995972s

No improvement. Patience: 7/40
Training epoch 186, Batch 49/98: LR=3.21e-04, Loss=1.77384e-01 LER=6.417e-02
***Loss=1.77384e-01
Training epoch 186, Batch 98/98: LR=3.21e-04, Loss=1.78145e-01 LER=6.481e-02
***Loss=1.78145e-01
Epoch 186 Train Time 11.436115264892578s

No improvement. Patience: 8/40
Training epoch 187, Batch 49/98: LR=3.17e-04, Loss=1.75042e-01 LER=6.350e-02
***Loss=1.75042e-01
Training epoch 187, Batch 98/98: LR=3.17e-04, Loss=1.76705e-01 LER=6.381e-02
***Loss=1.76705e-01
Epoch 187 Train Time 11.531668663024902s

No improvement. Patience: 9/40
Training epoch 188, Batch 49/98: LR=3.12e-04, Loss=1.74016e-01 LER=6.290e-02
***Loss=1.74016e-01
Training epoch 188, Batch 98/98: LR=3.12e-04, Loss=1.74587e-01 LER=6.314e-02
***Loss=1.74587e-01
Epoch 188 Train Time 11.334500551223755s

No improvement. Patience: 10/40
Training epoch 189, Batch 49/98: LR=3.07e-04, Loss=1.72607e-01 LER=6.276e-02
***Loss=1.72607e-01
Training epoch 189, Batch 98/98: LR=3.07e-04, Loss=1.75429e-01 LER=6.346e-02
***Loss=1.75429e-01
Epoch 189 Train Time 11.22101354598999s

No improvement. Patience: 11/40
Training epoch 190, Batch 49/98: LR=3.02e-04, Loss=1.76322e-01 LER=6.453e-02
***Loss=1.76322e-01
Training epoch 190, Batch 98/98: LR=3.02e-04, Loss=1.75966e-01 LER=6.388e-02
***Loss=1.75966e-01
Epoch 190 Train Time 11.277416706085205s

No improvement. Patience: 12/40
Test LER  p=7.00e-02: 3.37e-02 p=8.00e-02: 4.88e-02 p=9.00e-02: 5.74e-02 p=1.00e-01: 7.52e-02 p=1.10e-01: 9.87e-02
Mean LER = 6.277e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.568591833114624 s

Training epoch 191, Batch 49/98: LR=2.97e-04, Loss=1.71962e-01 LER=6.188e-02
***Loss=1.71962e-01
Training epoch 191, Batch 98/98: LR=2.97e-04, Loss=1.75416e-01 LER=6.329e-02
***Loss=1.75416e-01
Epoch 191 Train Time 11.113921403884888s

No improvement. Patience: 13/40
Training epoch 192, Batch 49/98: LR=2.93e-04, Loss=1.74054e-01 LER=6.294e-02
***Loss=1.74054e-01
Training epoch 192, Batch 98/98: LR=2.93e-04, Loss=1.75122e-01 LER=6.374e-02
***Loss=1.75122e-01
Epoch 192 Train Time 11.555881261825562s

No improvement. Patience: 14/40
Training epoch 193, Batch 49/98: LR=2.88e-04, Loss=1.76357e-01 LER=6.248e-02
***Loss=1.76357e-01
Training epoch 193, Batch 98/98: LR=2.88e-04, Loss=1.74573e-01 LER=6.233e-02
***Loss=1.74573e-01
Epoch 193 Train Time 11.274603843688965s

No improvement. Patience: 15/40
Training epoch 194, Batch 49/98: LR=2.83e-04, Loss=1.71504e-01 LER=6.144e-02
***Loss=1.71504e-01
Training epoch 194, Batch 98/98: LR=2.83e-04, Loss=1.71321e-01 LER=6.168e-02
***Loss=1.71321e-01
Epoch 194 Train Time 11.520784378051758s

Model Saved - New best loss: 1.71321e-01
Training epoch 195, Batch 49/98: LR=2.78e-04, Loss=1.75630e-01 LER=6.401e-02
***Loss=1.75630e-01
Training epoch 195, Batch 98/98: LR=2.78e-04, Loss=1.74375e-01 LER=6.318e-02
***Loss=1.74375e-01
Epoch 195 Train Time 11.461471557617188s

No improvement. Patience: 1/40
Training epoch 196, Batch 49/98: LR=2.74e-04, Loss=1.75518e-01 LER=6.292e-02
***Loss=1.75518e-01
Training epoch 196, Batch 98/98: LR=2.74e-04, Loss=1.74196e-01 LER=6.291e-02
***Loss=1.74196e-01
Epoch 196 Train Time 11.20634651184082s

No improvement. Patience: 2/40
Training epoch 197, Batch 49/98: LR=2.69e-04, Loss=1.76811e-01 LER=6.401e-02
***Loss=1.76811e-01
Training epoch 197, Batch 98/98: LR=2.69e-04, Loss=1.75127e-01 LER=6.295e-02
***Loss=1.75127e-01
Epoch 197 Train Time 11.536942720413208s

No improvement. Patience: 3/40
Training epoch 198, Batch 49/98: LR=2.64e-04, Loss=1.72796e-01 LER=6.352e-02
***Loss=1.72796e-01
Training epoch 198, Batch 98/98: LR=2.64e-04, Loss=1.73570e-01 LER=6.248e-02
***Loss=1.73570e-01
Epoch 198 Train Time 11.285709857940674s

No improvement. Patience: 4/40
Training epoch 199, Batch 49/98: LR=2.60e-04, Loss=1.75120e-01 LER=6.314e-02
***Loss=1.75120e-01
Training epoch 199, Batch 98/98: LR=2.60e-04, Loss=1.74090e-01 LER=6.204e-02
***Loss=1.74090e-01
Epoch 199 Train Time 11.07153844833374s

No improvement. Patience: 5/40
Training epoch 200, Batch 49/98: LR=2.55e-04, Loss=1.73758e-01 LER=6.276e-02
***Loss=1.73758e-01
Training epoch 200, Batch 98/98: LR=2.55e-04, Loss=1.74231e-01 LER=6.293e-02
***Loss=1.74231e-01
Epoch 200 Train Time 11.539681196212769s

No improvement. Patience: 6/40
Test LER  p=7.00e-02: 3.28e-02 p=8.00e-02: 4.84e-02 p=9.00e-02: 6.20e-02 p=1.00e-01: 7.07e-02 p=1.10e-01: 9.58e-02
Mean LER = 6.195e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.69206213951111 s

Training epoch 201, Batch 49/98: LR=2.51e-04, Loss=1.73097e-01 LER=6.399e-02
***Loss=1.73097e-01
Training epoch 201, Batch 98/98: LR=2.51e-04, Loss=1.73177e-01 LER=6.312e-02
***Loss=1.73177e-01
Epoch 201 Train Time 11.301913499832153s

No improvement. Patience: 7/40
Training epoch 202, Batch 49/98: LR=2.46e-04, Loss=1.70155e-01 LER=6.085e-02
***Loss=1.70155e-01
Training epoch 202, Batch 98/98: LR=2.46e-04, Loss=1.72004e-01 LER=6.220e-02
***Loss=1.72004e-01
Epoch 202 Train Time 11.922067642211914s

No improvement. Patience: 8/40
Training epoch 203, Batch 49/98: LR=2.42e-04, Loss=1.76138e-01 LER=6.344e-02
***Loss=1.76138e-01
Training epoch 203, Batch 98/98: LR=2.42e-04, Loss=1.72390e-01 LER=6.268e-02
***Loss=1.72390e-01
Epoch 203 Train Time 11.165498495101929s

No improvement. Patience: 9/40
Training epoch 204, Batch 49/98: LR=2.37e-04, Loss=1.76292e-01 LER=6.429e-02
***Loss=1.76292e-01
Training epoch 204, Batch 98/98: LR=2.37e-04, Loss=1.75184e-01 LER=6.328e-02
***Loss=1.75184e-01
Epoch 204 Train Time 11.290945529937744s

No improvement. Patience: 10/40
Training epoch 205, Batch 49/98: LR=2.33e-04, Loss=1.74844e-01 LER=6.395e-02
***Loss=1.74844e-01
Training epoch 205, Batch 98/98: LR=2.33e-04, Loss=1.75817e-01 LER=6.465e-02
***Loss=1.75817e-01
Epoch 205 Train Time 11.240612030029297s

No improvement. Patience: 11/40
Training epoch 206, Batch 49/98: LR=2.28e-04, Loss=1.71660e-01 LER=6.214e-02
***Loss=1.71660e-01
Training epoch 206, Batch 98/98: LR=2.28e-04, Loss=1.72464e-01 LER=6.198e-02
***Loss=1.72464e-01
Epoch 206 Train Time 11.149454116821289s

No improvement. Patience: 12/40
Training epoch 207, Batch 49/98: LR=2.24e-04, Loss=1.76588e-01 LER=6.362e-02
***Loss=1.76588e-01
Training epoch 207, Batch 98/98: LR=2.24e-04, Loss=1.74046e-01 LER=6.266e-02
***Loss=1.74046e-01
Epoch 207 Train Time 11.445577144622803s

No improvement. Patience: 13/40
Training epoch 208, Batch 49/98: LR=2.20e-04, Loss=1.75529e-01 LER=6.292e-02
***Loss=1.75529e-01
Training epoch 208, Batch 98/98: LR=2.20e-04, Loss=1.74262e-01 LER=6.281e-02
***Loss=1.74262e-01
Epoch 208 Train Time 11.467599868774414s

No improvement. Patience: 14/40
Training epoch 209, Batch 49/98: LR=2.15e-04, Loss=1.70796e-01 LER=6.256e-02
***Loss=1.70796e-01
Training epoch 209, Batch 98/98: LR=2.15e-04, Loss=1.72444e-01 LER=6.301e-02
***Loss=1.72444e-01
Epoch 209 Train Time 11.500215530395508s

No improvement. Patience: 15/40
Training epoch 210, Batch 49/98: LR=2.11e-04, Loss=1.70653e-01 LER=6.232e-02
***Loss=1.70653e-01
Training epoch 210, Batch 98/98: LR=2.11e-04, Loss=1.69987e-01 LER=6.168e-02
***Loss=1.69987e-01
Epoch 210 Train Time 11.11250114440918s

Model Saved - New best loss: 1.69987e-01
Test LER  p=7.00e-02: 3.57e-02 p=8.00e-02: 4.22e-02 p=9.00e-02: 6.04e-02 p=1.00e-01: 7.12e-02 p=1.10e-01: 9.07e-02
Mean LER = 6.006e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.60135531425476 s

Training epoch 211, Batch 49/98: LR=2.07e-04, Loss=1.73645e-01 LER=6.316e-02
***Loss=1.73645e-01
Training epoch 211, Batch 98/98: LR=2.07e-04, Loss=1.73263e-01 LER=6.286e-02
***Loss=1.73263e-01
Epoch 211 Train Time 11.484267950057983s

No improvement. Patience: 1/40
Training epoch 212, Batch 49/98: LR=2.03e-04, Loss=1.76043e-01 LER=6.300e-02
***Loss=1.76043e-01
Training epoch 212, Batch 98/98: LR=2.03e-04, Loss=1.75027e-01 LER=6.322e-02
***Loss=1.75027e-01
Epoch 212 Train Time 11.633505821228027s

No improvement. Patience: 2/40
Training epoch 213, Batch 49/98: LR=1.99e-04, Loss=1.73578e-01 LER=6.284e-02
***Loss=1.73578e-01
Training epoch 213, Batch 98/98: LR=1.99e-04, Loss=1.73793e-01 LER=6.314e-02
***Loss=1.73793e-01
Epoch 213 Train Time 11.302254915237427s

No improvement. Patience: 3/40
Training epoch 214, Batch 49/98: LR=1.94e-04, Loss=1.74225e-01 LER=6.260e-02
***Loss=1.74225e-01
Training epoch 214, Batch 98/98: LR=1.94e-04, Loss=1.71920e-01 LER=6.230e-02
***Loss=1.71920e-01
Epoch 214 Train Time 11.03799295425415s

No improvement. Patience: 4/40
Training epoch 215, Batch 49/98: LR=1.90e-04, Loss=1.75950e-01 LER=6.445e-02
***Loss=1.75950e-01
Training epoch 215, Batch 98/98: LR=1.90e-04, Loss=1.75947e-01 LER=6.435e-02
***Loss=1.75947e-01
Epoch 215 Train Time 11.36101770401001s

No improvement. Patience: 5/40
Training epoch 216, Batch 49/98: LR=1.86e-04, Loss=1.71162e-01 LER=6.240e-02
***Loss=1.71162e-01
Training epoch 216, Batch 98/98: LR=1.86e-04, Loss=1.72682e-01 LER=6.275e-02
***Loss=1.72682e-01
Epoch 216 Train Time 11.646905660629272s

No improvement. Patience: 6/40
Training epoch 217, Batch 49/98: LR=1.82e-04, Loss=1.70279e-01 LER=6.130e-02
***Loss=1.70279e-01
Training epoch 217, Batch 98/98: LR=1.82e-04, Loss=1.69471e-01 LER=6.172e-02
***Loss=1.69471e-01
Epoch 217 Train Time 11.381447553634644s

Model Saved - New best loss: 1.69471e-01
Training epoch 218, Batch 49/98: LR=1.78e-04, Loss=1.73453e-01 LER=6.258e-02
***Loss=1.73453e-01
Training epoch 218, Batch 98/98: LR=1.78e-04, Loss=1.74468e-01 LER=6.320e-02
***Loss=1.74468e-01
Epoch 218 Train Time 11.449824571609497s

No improvement. Patience: 1/40
Training epoch 219, Batch 49/98: LR=1.74e-04, Loss=1.71400e-01 LER=6.168e-02
***Loss=1.71400e-01
Training epoch 219, Batch 98/98: LR=1.74e-04, Loss=1.72543e-01 LER=6.208e-02
***Loss=1.72543e-01
Epoch 219 Train Time 11.430728197097778s

No improvement. Patience: 2/40
Training epoch 220, Batch 49/98: LR=1.70e-04, Loss=1.71393e-01 LER=6.232e-02
***Loss=1.71393e-01
Training epoch 220, Batch 98/98: LR=1.70e-04, Loss=1.69203e-01 LER=6.104e-02
***Loss=1.69203e-01
Epoch 220 Train Time 11.59623670578003s

Model Saved - New best loss: 1.69203e-01
Test LER  p=7.00e-02: 3.20e-02 p=8.00e-02: 4.76e-02 p=9.00e-02: 6.24e-02 p=1.00e-01: 8.17e-02 p=1.10e-01: 9.81e-02
Mean LER = 6.438e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.473362684249878 s

Training epoch 221, Batch 49/98: LR=1.66e-04, Loss=1.73808e-01 LER=6.399e-02
***Loss=1.73808e-01
Training epoch 221, Batch 98/98: LR=1.66e-04, Loss=1.75734e-01 LER=6.433e-02
***Loss=1.75734e-01
Epoch 221 Train Time 11.375430822372437s

No improvement. Patience: 1/40
Training epoch 222, Batch 49/98: LR=1.62e-04, Loss=1.74296e-01 LER=6.254e-02
***Loss=1.74296e-01
Training epoch 222, Batch 98/98: LR=1.62e-04, Loss=1.73742e-01 LER=6.254e-02
***Loss=1.73742e-01
Epoch 222 Train Time 11.389517784118652s

No improvement. Patience: 2/40
Training epoch 223, Batch 49/98: LR=1.59e-04, Loss=1.76511e-01 LER=6.407e-02
***Loss=1.76511e-01
Training epoch 223, Batch 98/98: LR=1.59e-04, Loss=1.74639e-01 LER=6.305e-02
***Loss=1.74639e-01
Epoch 223 Train Time 11.720770597457886s

No improvement. Patience: 3/40
Training epoch 224, Batch 49/98: LR=1.55e-04, Loss=1.71069e-01 LER=6.099e-02
***Loss=1.71069e-01
Training epoch 224, Batch 98/98: LR=1.55e-04, Loss=1.72099e-01 LER=6.210e-02
***Loss=1.72099e-01
Epoch 224 Train Time 11.907432794570923s

No improvement. Patience: 4/40
Training epoch 225, Batch 49/98: LR=1.51e-04, Loss=1.70852e-01 LER=6.160e-02
***Loss=1.70852e-01
Training epoch 225, Batch 98/98: LR=1.51e-04, Loss=1.72373e-01 LER=6.207e-02
***Loss=1.72373e-01
Epoch 225 Train Time 11.451886177062988s

No improvement. Patience: 5/40
Training epoch 226, Batch 49/98: LR=1.47e-04, Loss=1.73957e-01 LER=6.306e-02
***Loss=1.73957e-01
Training epoch 226, Batch 98/98: LR=1.47e-04, Loss=1.73299e-01 LER=6.282e-02
***Loss=1.73299e-01
Epoch 226 Train Time 11.339624643325806s

No improvement. Patience: 6/40
Training epoch 227, Batch 49/98: LR=1.44e-04, Loss=1.74084e-01 LER=6.328e-02
***Loss=1.74084e-01
Training epoch 227, Batch 98/98: LR=1.44e-04, Loss=1.72157e-01 LER=6.322e-02
***Loss=1.72157e-01
Epoch 227 Train Time 11.301551103591919s

No improvement. Patience: 7/40
Training epoch 228, Batch 49/98: LR=1.40e-04, Loss=1.74563e-01 LER=6.350e-02
***Loss=1.74563e-01
Training epoch 228, Batch 98/98: LR=1.40e-04, Loss=1.73966e-01 LER=6.327e-02
***Loss=1.73966e-01
Epoch 228 Train Time 11.816711902618408s

No improvement. Patience: 8/40
Training epoch 229, Batch 49/98: LR=1.36e-04, Loss=1.73985e-01 LER=6.256e-02
***Loss=1.73985e-01
Training epoch 229, Batch 98/98: LR=1.36e-04, Loss=1.71948e-01 LER=6.202e-02
***Loss=1.71948e-01
Epoch 229 Train Time 11.102204084396362s

No improvement. Patience: 9/40
Training epoch 230, Batch 49/98: LR=1.33e-04, Loss=1.70656e-01 LER=6.172e-02
***Loss=1.70656e-01
Training epoch 230, Batch 98/98: LR=1.33e-04, Loss=1.71940e-01 LER=6.169e-02
***Loss=1.71940e-01
Epoch 230 Train Time 11.563671827316284s

No improvement. Patience: 10/40
Test LER  p=7.00e-02: 2.90e-02 p=8.00e-02: 4.90e-02 p=9.00e-02: 5.95e-02 p=1.00e-01: 7.03e-02 p=1.10e-01: 9.67e-02
Mean LER = 6.090e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.598527669906616 s

Training epoch 231, Batch 49/98: LR=1.29e-04, Loss=1.73044e-01 LER=6.282e-02
***Loss=1.73044e-01
Training epoch 231, Batch 98/98: LR=1.29e-04, Loss=1.73225e-01 LER=6.332e-02
***Loss=1.73225e-01
Epoch 231 Train Time 11.357383012771606s

No improvement. Patience: 11/40
Training epoch 232, Batch 49/98: LR=1.26e-04, Loss=1.66536e-01 LER=6.003e-02
***Loss=1.66536e-01
Training epoch 232, Batch 98/98: LR=1.26e-04, Loss=1.69844e-01 LER=6.124e-02
***Loss=1.69844e-01
Epoch 232 Train Time 11.557196855545044s

No improvement. Patience: 12/40
Training epoch 233, Batch 49/98: LR=1.22e-04, Loss=1.69514e-01 LER=6.170e-02
***Loss=1.69514e-01
Training epoch 233, Batch 98/98: LR=1.22e-04, Loss=1.69991e-01 LER=6.198e-02
***Loss=1.69991e-01
Epoch 233 Train Time 11.391005992889404s

No improvement. Patience: 13/40
Training epoch 234, Batch 49/98: LR=1.19e-04, Loss=1.77067e-01 LER=6.300e-02
***Loss=1.77067e-01
Training epoch 234, Batch 98/98: LR=1.19e-04, Loss=1.77092e-01 LER=6.318e-02
***Loss=1.77092e-01
Epoch 234 Train Time 11.725379705429077s

No improvement. Patience: 14/40
Training epoch 235, Batch 49/98: LR=1.16e-04, Loss=1.70025e-01 LER=6.198e-02
***Loss=1.70025e-01
Training epoch 235, Batch 98/98: LR=1.16e-04, Loss=1.70687e-01 LER=6.132e-02
***Loss=1.70687e-01
Epoch 235 Train Time 11.574504613876343s

No improvement. Patience: 15/40
Training epoch 236, Batch 49/98: LR=1.12e-04, Loss=1.71761e-01 LER=6.346e-02
***Loss=1.71761e-01
Training epoch 236, Batch 98/98: LR=1.12e-04, Loss=1.72277e-01 LER=6.308e-02
***Loss=1.72277e-01
Epoch 236 Train Time 11.268909215927124s

No improvement. Patience: 16/40
Training epoch 237, Batch 49/98: LR=1.09e-04, Loss=1.74503e-01 LER=6.300e-02
***Loss=1.74503e-01
Training epoch 237, Batch 98/98: LR=1.09e-04, Loss=1.73384e-01 LER=6.276e-02
***Loss=1.73384e-01
Epoch 237 Train Time 11.402992963790894s

No improvement. Patience: 17/40
Training epoch 238, Batch 49/98: LR=1.06e-04, Loss=1.71196e-01 LER=6.190e-02
***Loss=1.71196e-01
Training epoch 238, Batch 98/98: LR=1.06e-04, Loss=1.73700e-01 LER=6.283e-02
***Loss=1.73700e-01
Epoch 238 Train Time 11.406639814376831s

No improvement. Patience: 18/40
Training epoch 239, Batch 49/98: LR=1.03e-04, Loss=1.74926e-01 LER=6.360e-02
***Loss=1.74926e-01
Training epoch 239, Batch 98/98: LR=1.03e-04, Loss=1.71706e-01 LER=6.269e-02
***Loss=1.71706e-01
Epoch 239 Train Time 11.708495378494263s

No improvement. Patience: 19/40
Training epoch 240, Batch 49/98: LR=9.95e-05, Loss=1.71054e-01 LER=6.168e-02
***Loss=1.71054e-01
Training epoch 240, Batch 98/98: LR=9.95e-05, Loss=1.71695e-01 LER=6.254e-02
***Loss=1.71695e-01
Epoch 240 Train Time 11.158853530883789s

No improvement. Patience: 20/40
Test LER  p=7.00e-02: 3.51e-02 p=8.00e-02: 4.56e-02 p=9.00e-02: 5.97e-02 p=1.00e-01: 7.55e-02 p=1.10e-01: 9.91e-02
Mean LER = 6.299e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.615656852722168 s

Training epoch 241, Batch 49/98: LR=9.64e-05, Loss=1.71250e-01 LER=6.178e-02
***Loss=1.71250e-01
Training epoch 241, Batch 98/98: LR=9.64e-05, Loss=1.71881e-01 LER=6.245e-02
***Loss=1.71881e-01
Epoch 241 Train Time 11.062516689300537s

No improvement. Patience: 21/40
Training epoch 242, Batch 49/98: LR=9.33e-05, Loss=1.75943e-01 LER=6.238e-02
***Loss=1.75943e-01
Training epoch 242, Batch 98/98: LR=9.33e-05, Loss=1.74164e-01 LER=6.200e-02
***Loss=1.74164e-01
Epoch 242 Train Time 11.557718992233276s

No improvement. Patience: 22/40
Training epoch 243, Batch 49/98: LR=9.03e-05, Loss=1.70020e-01 LER=6.182e-02
***Loss=1.70020e-01
Training epoch 243, Batch 98/98: LR=9.03e-05, Loss=1.69866e-01 LER=6.165e-02
***Loss=1.69866e-01
Epoch 243 Train Time 11.568045854568481s

No improvement. Patience: 23/40
Training epoch 244, Batch 49/98: LR=8.74e-05, Loss=1.73260e-01 LER=6.380e-02
***Loss=1.73260e-01
Training epoch 244, Batch 98/98: LR=8.74e-05, Loss=1.73723e-01 LER=6.412e-02
***Loss=1.73723e-01
Epoch 244 Train Time 11.349535465240479s

No improvement. Patience: 24/40
Training epoch 245, Batch 49/98: LR=8.45e-05, Loss=1.69886e-01 LER=6.128e-02
***Loss=1.69886e-01
Training epoch 245, Batch 98/98: LR=8.45e-05, Loss=1.70629e-01 LER=6.174e-02
***Loss=1.70629e-01
Epoch 245 Train Time 11.06447982788086s

No improvement. Patience: 25/40
Training epoch 246, Batch 49/98: LR=8.16e-05, Loss=1.76185e-01 LER=6.334e-02
***Loss=1.76185e-01
Training epoch 246, Batch 98/98: LR=8.16e-05, Loss=1.74090e-01 LER=6.290e-02
***Loss=1.74090e-01
Epoch 246 Train Time 10.9746675491333s

No improvement. Patience: 26/40
Training epoch 247, Batch 49/98: LR=7.88e-05, Loss=1.67384e-01 LER=6.031e-02
***Loss=1.67384e-01
Training epoch 247, Batch 98/98: LR=7.88e-05, Loss=1.69882e-01 LER=6.110e-02
***Loss=1.69882e-01
Epoch 247 Train Time 11.332810163497925s

No improvement. Patience: 27/40
Training epoch 248, Batch 49/98: LR=7.60e-05, Loss=1.72364e-01 LER=6.230e-02
***Loss=1.72364e-01
Training epoch 248, Batch 98/98: LR=7.60e-05, Loss=1.71496e-01 LER=6.175e-02
***Loss=1.71496e-01
Epoch 248 Train Time 11.503623485565186s

No improvement. Patience: 28/40
Training epoch 249, Batch 49/98: LR=7.32e-05, Loss=1.71129e-01 LER=6.344e-02
***Loss=1.71129e-01
Training epoch 249, Batch 98/98: LR=7.32e-05, Loss=1.70735e-01 LER=6.276e-02
***Loss=1.70735e-01
Epoch 249 Train Time 11.566855192184448s

No improvement. Patience: 29/40
Training epoch 250, Batch 49/98: LR=7.06e-05, Loss=1.71847e-01 LER=6.282e-02
***Loss=1.71847e-01
Training epoch 250, Batch 98/98: LR=7.06e-05, Loss=1.70291e-01 LER=6.244e-02
***Loss=1.70291e-01
Epoch 250 Train Time 11.565141201019287s

No improvement. Patience: 30/40
Test LER  p=7.00e-02: 3.22e-02 p=8.00e-02: 4.43e-02 p=9.00e-02: 5.82e-02 p=1.00e-01: 7.71e-02 p=1.10e-01: 9.39e-02
Mean LER = 6.115e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.536251068115234 s

Training epoch 251, Batch 49/98: LR=6.79e-05, Loss=1.70826e-01 LER=6.148e-02
***Loss=1.70826e-01
Training epoch 251, Batch 98/98: LR=6.79e-05, Loss=1.68876e-01 LER=6.110e-02
***Loss=1.68876e-01
Epoch 251 Train Time 11.112754583358765s

Model Saved - New best loss: 1.68876e-01
Training epoch 252, Batch 49/98: LR=6.53e-05, Loss=1.71760e-01 LER=6.258e-02
***Loss=1.71760e-01
Training epoch 252, Batch 98/98: LR=6.53e-05, Loss=1.70808e-01 LER=6.224e-02
***Loss=1.70808e-01
Epoch 252 Train Time 11.549784183502197s

No improvement. Patience: 1/40
Training epoch 253, Batch 49/98: LR=6.28e-05, Loss=1.72851e-01 LER=6.208e-02
***Loss=1.72851e-01
Training epoch 253, Batch 98/98: LR=6.28e-05, Loss=1.72246e-01 LER=6.300e-02
***Loss=1.72246e-01
Epoch 253 Train Time 11.339859962463379s

No improvement. Patience: 2/40
Training epoch 254, Batch 49/98: LR=6.03e-05, Loss=1.71625e-01 LER=6.172e-02
***Loss=1.71625e-01
Training epoch 254, Batch 98/98: LR=6.03e-05, Loss=1.71921e-01 LER=6.193e-02
***Loss=1.71921e-01
Epoch 254 Train Time 11.626058340072632s

No improvement. Patience: 3/40
Training epoch 255, Batch 49/98: LR=5.78e-05, Loss=1.71939e-01 LER=6.152e-02
***Loss=1.71939e-01
Training epoch 255, Batch 98/98: LR=5.78e-05, Loss=1.72099e-01 LER=6.210e-02
***Loss=1.72099e-01
Epoch 255 Train Time 11.309630870819092s

No improvement. Patience: 4/40
Training epoch 256, Batch 49/98: LR=5.54e-05, Loss=1.69958e-01 LER=6.126e-02
***Loss=1.69958e-01
Training epoch 256, Batch 98/98: LR=5.54e-05, Loss=1.69079e-01 LER=6.103e-02
***Loss=1.69079e-01
Epoch 256 Train Time 11.411765813827515s

No improvement. Patience: 5/40
Training epoch 257, Batch 49/98: LR=5.31e-05, Loss=1.75277e-01 LER=6.274e-02
***Loss=1.75277e-01
Training epoch 257, Batch 98/98: LR=5.31e-05, Loss=1.73577e-01 LER=6.240e-02
***Loss=1.73577e-01
Epoch 257 Train Time 11.256950855255127s

No improvement. Patience: 6/40
Training epoch 258, Batch 49/98: LR=5.08e-05, Loss=1.71328e-01 LER=6.290e-02
***Loss=1.71328e-01
Training epoch 258, Batch 98/98: LR=5.08e-05, Loss=1.70895e-01 LER=6.267e-02
***Loss=1.70895e-01
Epoch 258 Train Time 11.27696704864502s

No improvement. Patience: 7/40
Training epoch 259, Batch 49/98: LR=4.85e-05, Loss=1.72167e-01 LER=6.284e-02
***Loss=1.72167e-01
Training epoch 259, Batch 98/98: LR=4.85e-05, Loss=1.70053e-01 LER=6.189e-02
***Loss=1.70053e-01
Epoch 259 Train Time 11.81898021697998s

No improvement. Patience: 8/40
Training epoch 260, Batch 49/98: LR=4.63e-05, Loss=1.69908e-01 LER=6.202e-02
***Loss=1.69908e-01
Training epoch 260, Batch 98/98: LR=4.63e-05, Loss=1.70835e-01 LER=6.273e-02
***Loss=1.70835e-01
Epoch 260 Train Time 11.285025358200073s

No improvement. Patience: 9/40
Test LER  p=7.00e-02: 3.66e-02 p=8.00e-02: 4.69e-02 p=9.00e-02: 6.13e-02 p=1.00e-01: 7.87e-02 p=1.10e-01: 1.00e-01
Mean LER = 6.479e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.47714900970459 s

Training epoch 261, Batch 49/98: LR=4.42e-05, Loss=1.67029e-01 LER=6.039e-02
***Loss=1.67029e-01
Training epoch 261, Batch 98/98: LR=4.42e-05, Loss=1.69712e-01 LER=6.183e-02
***Loss=1.69712e-01
Epoch 261 Train Time 11.11882209777832s

No improvement. Patience: 10/40
Training epoch 262, Batch 49/98: LR=4.21e-05, Loss=1.70109e-01 LER=6.152e-02
***Loss=1.70109e-01
Training epoch 262, Batch 98/98: LR=4.21e-05, Loss=1.71098e-01 LER=6.186e-02
***Loss=1.71098e-01
Epoch 262 Train Time 11.280088901519775s

No improvement. Patience: 11/40
Training epoch 263, Batch 49/98: LR=4.00e-05, Loss=1.70774e-01 LER=6.184e-02
***Loss=1.70774e-01
Training epoch 263, Batch 98/98: LR=4.00e-05, Loss=1.73360e-01 LER=6.324e-02
***Loss=1.73360e-01
Epoch 263 Train Time 11.560259580612183s

No improvement. Patience: 12/40
Training epoch 264, Batch 49/98: LR=3.80e-05, Loss=1.70482e-01 LER=6.218e-02
***Loss=1.70482e-01
Training epoch 264, Batch 98/98: LR=3.80e-05, Loss=1.71486e-01 LER=6.268e-02
***Loss=1.71486e-01
Epoch 264 Train Time 11.487213373184204s

No improvement. Patience: 13/40
Training epoch 265, Batch 49/98: LR=3.61e-05, Loss=1.71177e-01 LER=6.138e-02
***Loss=1.71177e-01
Training epoch 265, Batch 98/98: LR=3.61e-05, Loss=1.71943e-01 LER=6.163e-02
***Loss=1.71943e-01
Epoch 265 Train Time 11.552964925765991s

No improvement. Patience: 14/40
Training epoch 266, Batch 49/98: LR=3.42e-05, Loss=1.67078e-01 LER=6.134e-02
***Loss=1.67078e-01
Training epoch 266, Batch 98/98: LR=3.42e-05, Loss=1.71659e-01 LER=6.232e-02
***Loss=1.71659e-01
Epoch 266 Train Time 11.66452169418335s

No improvement. Patience: 15/40
Training epoch 267, Batch 49/98: LR=3.23e-05, Loss=1.71448e-01 LER=6.206e-02
***Loss=1.71448e-01
Training epoch 267, Batch 98/98: LR=3.23e-05, Loss=1.71091e-01 LER=6.204e-02
***Loss=1.71091e-01
Epoch 267 Train Time 11.793747663497925s

No improvement. Patience: 16/40
Training epoch 268, Batch 49/98: LR=3.05e-05, Loss=1.76639e-01 LER=6.423e-02
***Loss=1.76639e-01
Training epoch 268, Batch 98/98: LR=3.05e-05, Loss=1.73690e-01 LER=6.319e-02
***Loss=1.73690e-01
Epoch 268 Train Time 11.475871562957764s

No improvement. Patience: 17/40
Training epoch 269, Batch 49/98: LR=2.88e-05, Loss=1.71795e-01 LER=6.390e-02
***Loss=1.71795e-01
Training epoch 269, Batch 98/98: LR=2.88e-05, Loss=1.70670e-01 LER=6.272e-02
***Loss=1.70670e-01
Epoch 269 Train Time 11.32340955734253s

No improvement. Patience: 18/40
Training epoch 270, Batch 49/98: LR=2.71e-05, Loss=1.73698e-01 LER=6.328e-02
***Loss=1.73698e-01
Training epoch 270, Batch 98/98: LR=2.71e-05, Loss=1.73092e-01 LER=6.302e-02
***Loss=1.73092e-01
Epoch 270 Train Time 11.217597007751465s

No improvement. Patience: 19/40
Test LER  p=7.00e-02: 3.10e-02 p=8.00e-02: 4.62e-02 p=9.00e-02: 5.95e-02 p=1.00e-01: 7.56e-02 p=1.10e-01: 9.07e-02
Mean LER = 6.059e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.758554697036743 s

Training epoch 271, Batch 49/98: LR=2.54e-05, Loss=1.70713e-01 LER=6.166e-02
***Loss=1.70713e-01
Training epoch 271, Batch 98/98: LR=2.54e-05, Loss=1.69864e-01 LER=6.108e-02
***Loss=1.69864e-01
Epoch 271 Train Time 11.483454704284668s

No improvement. Patience: 20/40
Training epoch 272, Batch 49/98: LR=2.39e-05, Loss=1.72162e-01 LER=6.290e-02
***Loss=1.72162e-01
Training epoch 272, Batch 98/98: LR=2.39e-05, Loss=1.70952e-01 LER=6.191e-02
***Loss=1.70952e-01
Epoch 272 Train Time 11.05150055885315s

No improvement. Patience: 21/40
Training epoch 273, Batch 49/98: LR=2.23e-05, Loss=1.70346e-01 LER=6.228e-02
***Loss=1.70346e-01
Training epoch 273, Batch 98/98: LR=2.23e-05, Loss=1.68869e-01 LER=6.178e-02
***Loss=1.68869e-01
Epoch 273 Train Time 11.385901927947998s

Model Saved - New best loss: 1.68869e-01
Training epoch 274, Batch 49/98: LR=2.08e-05, Loss=1.70912e-01 LER=6.166e-02
***Loss=1.70912e-01
Training epoch 274, Batch 98/98: LR=2.08e-05, Loss=1.70404e-01 LER=6.203e-02
***Loss=1.70404e-01
Epoch 274 Train Time 11.516791343688965s

No improvement. Patience: 1/40
Training epoch 275, Batch 49/98: LR=1.94e-05, Loss=1.66218e-01 LER=5.991e-02
***Loss=1.66218e-01
Training epoch 275, Batch 98/98: LR=1.94e-05, Loss=1.69528e-01 LER=6.096e-02
***Loss=1.69528e-01
Epoch 275 Train Time 11.43768310546875s

No improvement. Patience: 2/40
Training epoch 276, Batch 49/98: LR=1.80e-05, Loss=1.66697e-01 LER=6.077e-02
***Loss=1.66697e-01
Training epoch 276, Batch 98/98: LR=1.80e-05, Loss=1.68592e-01 LER=6.218e-02
***Loss=1.68592e-01
Epoch 276 Train Time 11.611656188964844s

Model Saved - New best loss: 1.68592e-01
Training epoch 277, Batch 49/98: LR=1.67e-05, Loss=1.74029e-01 LER=6.380e-02
***Loss=1.74029e-01
Training epoch 277, Batch 98/98: LR=1.67e-05, Loss=1.73110e-01 LER=6.253e-02
***Loss=1.73110e-01
Epoch 277 Train Time 11.607629776000977s

No improvement. Patience: 1/40
Training epoch 278, Batch 49/98: LR=1.54e-05, Loss=1.71351e-01 LER=6.224e-02
***Loss=1.71351e-01
Training epoch 278, Batch 98/98: LR=1.54e-05, Loss=1.71042e-01 LER=6.198e-02
***Loss=1.71042e-01
Epoch 278 Train Time 11.865611553192139s

No improvement. Patience: 2/40
Training epoch 279, Batch 49/98: LR=1.42e-05, Loss=1.70976e-01 LER=6.190e-02
***Loss=1.70976e-01
Training epoch 279, Batch 98/98: LR=1.42e-05, Loss=1.73390e-01 LER=6.249e-02
***Loss=1.73390e-01
Epoch 279 Train Time 11.576493501663208s

No improvement. Patience: 3/40
Training epoch 280, Batch 49/98: LR=1.30e-05, Loss=1.67382e-01 LER=6.061e-02
***Loss=1.67382e-01
Training epoch 280, Batch 98/98: LR=1.30e-05, Loss=1.69478e-01 LER=6.174e-02
***Loss=1.69478e-01
Epoch 280 Train Time 11.545329809188843s

No improvement. Patience: 4/40
Test LER  p=7.00e-02: 3.11e-02 p=8.00e-02: 4.71e-02 p=9.00e-02: 5.80e-02 p=1.00e-01: 7.71e-02 p=1.10e-01: 9.56e-02
Mean LER = 6.176e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.513790369033813 s

Training epoch 281, Batch 49/98: LR=1.19e-05, Loss=1.72544e-01 LER=6.246e-02
***Loss=1.72544e-01
Training epoch 281, Batch 98/98: LR=1.19e-05, Loss=1.74138e-01 LER=6.324e-02
***Loss=1.74138e-01
Epoch 281 Train Time 11.304726362228394s

No improvement. Patience: 5/40
Training epoch 282, Batch 49/98: LR=1.09e-05, Loss=1.67443e-01 LER=5.977e-02
***Loss=1.67443e-01
Training epoch 282, Batch 98/98: LR=1.09e-05, Loss=1.67926e-01 LER=6.044e-02
***Loss=1.67926e-01
Epoch 282 Train Time 11.501771211624146s

Model Saved - New best loss: 1.67926e-01
Training epoch 283, Batch 49/98: LR=9.85e-06, Loss=1.68688e-01 LER=6.118e-02
***Loss=1.68688e-01
Training epoch 283, Batch 98/98: LR=9.85e-06, Loss=1.71478e-01 LER=6.208e-02
***Loss=1.71478e-01
Epoch 283 Train Time 11.509721517562866s

No improvement. Patience: 1/40
Training epoch 284, Batch 49/98: LR=8.89e-06, Loss=1.67455e-01 LER=6.112e-02
***Loss=1.67455e-01
Training epoch 284, Batch 98/98: LR=8.89e-06, Loss=1.67758e-01 LER=6.137e-02
***Loss=1.67758e-01
Epoch 284 Train Time 11.714974880218506s

Model Saved - New best loss: 1.67758e-01
Training epoch 285, Batch 49/98: LR=7.99e-06, Loss=1.68408e-01 LER=6.073e-02
***Loss=1.68408e-01
Training epoch 285, Batch 98/98: LR=7.99e-06, Loss=1.69827e-01 LER=6.177e-02
***Loss=1.69827e-01
Epoch 285 Train Time 11.366982698440552s

No improvement. Patience: 1/40
Training epoch 286, Batch 49/98: LR=7.15e-06, Loss=1.73215e-01 LER=6.232e-02
***Loss=1.73215e-01
Training epoch 286, Batch 98/98: LR=7.15e-06, Loss=1.71994e-01 LER=6.208e-02
***Loss=1.71994e-01
Epoch 286 Train Time 11.034069061279297s

No improvement. Patience: 2/40
Training epoch 287, Batch 49/98: LR=6.36e-06, Loss=1.70806e-01 LER=6.250e-02
***Loss=1.70806e-01
Training epoch 287, Batch 98/98: LR=6.36e-06, Loss=1.71393e-01 LER=6.269e-02
***Loss=1.71393e-01
Epoch 287 Train Time 11.240049362182617s

No improvement. Patience: 3/40
Training epoch 288, Batch 49/98: LR=5.62e-06, Loss=1.69465e-01 LER=6.146e-02
***Loss=1.69465e-01
Training epoch 288, Batch 98/98: LR=5.62e-06, Loss=1.71192e-01 LER=6.241e-02
***Loss=1.71192e-01
Epoch 288 Train Time 11.895708322525024s

No improvement. Patience: 4/40
Training epoch 289, Batch 49/98: LR=4.94e-06, Loss=1.72510e-01 LER=6.284e-02
***Loss=1.72510e-01
Training epoch 289, Batch 98/98: LR=4.94e-06, Loss=1.70001e-01 LER=6.169e-02
***Loss=1.70001e-01
Epoch 289 Train Time 11.490544080734253s

No improvement. Patience: 5/40
Training epoch 290, Batch 49/98: LR=4.31e-06, Loss=1.70699e-01 LER=6.166e-02
***Loss=1.70699e-01
Training epoch 290, Batch 98/98: LR=4.31e-06, Loss=1.71494e-01 LER=6.233e-02
***Loss=1.71494e-01
Epoch 290 Train Time 11.258875370025635s

No improvement. Patience: 6/40
Test LER  p=7.00e-02: 3.07e-02 p=8.00e-02: 4.81e-02 p=9.00e-02: 6.03e-02 p=1.00e-01: 7.46e-02 p=1.10e-01: 9.37e-02
Mean LER = 6.146e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.778361082077026 s

Training epoch 291, Batch 49/98: LR=3.74e-06, Loss=1.67880e-01 LER=6.154e-02
***Loss=1.67880e-01
Training epoch 291, Batch 98/98: LR=3.74e-06, Loss=1.68699e-01 LER=6.189e-02
***Loss=1.68699e-01
Epoch 291 Train Time 11.513524055480957s

No improvement. Patience: 7/40
Training epoch 292, Batch 49/98: LR=3.22e-06, Loss=1.69043e-01 LER=6.067e-02
***Loss=1.69043e-01
Training epoch 292, Batch 98/98: LR=3.22e-06, Loss=1.68895e-01 LER=6.064e-02
***Loss=1.68895e-01
Epoch 292 Train Time 11.268825769424438s

No improvement. Patience: 8/40
Training epoch 293, Batch 49/98: LR=2.75e-06, Loss=1.70937e-01 LER=6.144e-02
***Loss=1.70937e-01
Training epoch 293, Batch 98/98: LR=2.75e-06, Loss=1.68916e-01 LER=6.147e-02
***Loss=1.68916e-01
Epoch 293 Train Time 11.625405550003052s

No improvement. Patience: 9/40
Training epoch 294, Batch 49/98: LR=2.34e-06, Loss=1.69595e-01 LER=6.081e-02
***Loss=1.69595e-01
Training epoch 294, Batch 98/98: LR=2.34e-06, Loss=1.71722e-01 LER=6.252e-02
***Loss=1.71722e-01
Epoch 294 Train Time 11.506186485290527s

No improvement. Patience: 10/40
Training epoch 295, Batch 49/98: LR=1.99e-06, Loss=1.69602e-01 LER=6.128e-02
***Loss=1.69602e-01
Training epoch 295, Batch 98/98: LR=1.99e-06, Loss=1.68496e-01 LER=6.058e-02
***Loss=1.68496e-01
Epoch 295 Train Time 11.66388487815857s

No improvement. Patience: 11/40
Training epoch 296, Batch 49/98: LR=1.68e-06, Loss=1.68453e-01 LER=6.110e-02
***Loss=1.68453e-01
Training epoch 296, Batch 98/98: LR=1.68e-06, Loss=1.67984e-01 LER=6.138e-02
***Loss=1.67984e-01
Epoch 296 Train Time 11.169288873672485s

No improvement. Patience: 12/40
Training epoch 297, Batch 49/98: LR=1.44e-06, Loss=1.65119e-01 LER=5.975e-02
***Loss=1.65119e-01
Training epoch 297, Batch 98/98: LR=1.44e-06, Loss=1.65782e-01 LER=5.989e-02
***Loss=1.65782e-01
Epoch 297 Train Time 11.213441610336304s

Model Saved - New best loss: 1.65782e-01
Training epoch 298, Batch 49/98: LR=1.25e-06, Loss=1.73976e-01 LER=6.280e-02
***Loss=1.73976e-01
Training epoch 298, Batch 98/98: LR=1.25e-06, Loss=1.72692e-01 LER=6.268e-02
***Loss=1.72692e-01
Epoch 298 Train Time 11.337947607040405s

No improvement. Patience: 1/40
Training epoch 299, Batch 49/98: LR=1.11e-06, Loss=1.69207e-01 LER=6.045e-02
***Loss=1.69207e-01
Training epoch 299, Batch 98/98: LR=1.11e-06, Loss=1.70253e-01 LER=6.090e-02
***Loss=1.70253e-01
Epoch 299 Train Time 11.277308225631714s

No improvement. Patience: 2/40
Training epoch 300, Batch 49/98: LR=1.03e-06, Loss=1.70717e-01 LER=6.162e-02
***Loss=1.70717e-01
Training epoch 300, Batch 98/98: LR=1.03e-06, Loss=1.68948e-01 LER=6.110e-02
***Loss=1.68948e-01
Epoch 300 Train Time 11.830394983291626s

No improvement. Patience: 3/40
Test LER  p=7.00e-02: 3.14e-02 p=8.00e-02: 4.63e-02 p=9.00e-02: 6.03e-02 p=1.00e-01: 7.34e-02 p=1.10e-01: 1.03e-01
Mean LER = 6.295e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 31.53505301475525 s

Best model loaded
Test LER  p=7.00e-02: 3.47e-02 p=8.00e-02: 4.90e-02 p=9.00e-02: 5.51e-02 p=1.00e-01: 7.56e-02 p=1.10e-01: 9.55e-02
Mean LER = 6.197e-02
# of testing samples: [10240.0, 10240.0, 10240.0, 10240.0, 10240.0]
 Test Time 32.6970329284668 s

