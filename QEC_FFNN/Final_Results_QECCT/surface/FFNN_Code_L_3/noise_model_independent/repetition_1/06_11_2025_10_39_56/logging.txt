Path to model/logs: Final_Results_QECCT/surface/FFNN_Code_L_3/noise_model_independent/repetition_1/06_11_2025_10_39_56
Namespace(epochs=2, workers=4, lr=0.01, gpus='0', batch_size=64, test_batch_size=512, seed=42, code_type='surface', code_L=3, repetitions=1, noise_type='independent', N_dec=6, d_model=128, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x713aa2b67910>, path='Final_Results_QECCT/surface/FFNN_Code_L_3/noise_model_independent/repetition_1/06_11_2025_10_39_56')
PC matrix shape torch.Size([8, 18])
DataParallel(
  (module): ECC_Transformer(
    (fc1): Linear(in_features=8, out_features=128, bias=True)
    (fc2): Linear(in_features=128, out_features=4, bias=True)
    (activation): Sigmoid()
    (criterion): CrossEntropyLoss()
  )
)
# of Parameters: 1668
Training epoch 1, Batch 500/1000: LR=1.00e-02, Loss=7.17179e-01 LER=2.200e-01
***Loss=7.17179e-01
Training epoch 1, Batch 1000/1000: LR=1.00e-02, Loss=6.29406e-01 LER=1.991e-01
***Loss=6.29406e-01
Epoch 1 Train Time 10.406733989715576s

Model Saved
Training epoch 2, Batch 500/1000: LR=5.00e-03, Loss=4.96348e-01 LER=1.808e-01
***Loss=4.96348e-01
Training epoch 2, Batch 1000/1000: LR=5.00e-03, Loss=4.85767e-01 LER=1.787e-01
***Loss=4.85767e-01
Epoch 2 Train Time 8.75021481513977s

Model Saved
Test LER  p=7.00e-02: 1.34e-01 p=8.00e-02: 1.53e-01 p=9.00e-02: 1.70e-01 p=1.00e-01: 1.90e-01 p=1.10e-01: 2.12e-01
Mean LER = 1.716e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 197.79242420196533 s

Best model loaded
Test LER  p=7.00e-02: 1.34e-01 p=8.00e-02: 1.49e-01 p=9.00e-02: 1.73e-01 p=1.00e-01: 1.93e-01 p=1.10e-01: 2.10e-01
Mean LER = 1.720e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 158.6203155517578 s

