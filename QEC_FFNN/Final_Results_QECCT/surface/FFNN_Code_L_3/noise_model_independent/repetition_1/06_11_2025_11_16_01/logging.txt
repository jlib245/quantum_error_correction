Path to model/logs: Final_Results_QECCT/surface/FFNN_Code_L_3/noise_model_independent/repetition_1/06_11_2025_11_16_01
Namespace(epochs=10, workers=4, lr=0.01, gpus='0', batch_size=64, test_batch_size=512, seed=42, code_type='surface', code_L=3, repetitions=1, noise_type='independent', N_dec=6, d_model=128, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x7a06794cfb50>, path='Final_Results_QECCT/surface/FFNN_Code_L_3/noise_model_independent/repetition_1/06_11_2025_11_16_01')
Intel Arc GPU (XPU)를 사용합니다.
단일 Intel Arc GPU (XPU)를 사용합니다 (DataParallel 비활성화).
PC matrix shape torch.Size([8, 18])
ECC_Transformer(
  (fc1): Linear(in_features=8, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=4, bias=True)
  (activation): Sigmoid()
  (criterion): CrossEntropyLoss()
)
# of Parameters: 1668
Training epoch 1, Batch 500/1000: LR=1.00e-02, Loss=7.17179e-01 LER=2.200e-01
***Loss=7.17179e-01
Training epoch 1, Batch 1000/1000: LR=1.00e-02, Loss=6.29406e-01 LER=1.991e-01
***Loss=6.29406e-01
Epoch 1 Train Time 21.51472306251526s

Model Saved
Training epoch 2, Batch 500/1000: LR=9.76e-03, Loss=4.94183e-01 LER=1.795e-01
***Loss=4.94183e-01
Training epoch 2, Batch 1000/1000: LR=9.76e-03, Loss=4.80275e-01 LER=1.752e-01
***Loss=4.80275e-01
Epoch 2 Train Time 17.920536041259766s

Model Saved
Training epoch 3, Batch 500/1000: LR=9.05e-03, Loss=4.48882e-01 LER=1.591e-01
***Loss=4.48882e-01
Training epoch 3, Batch 1000/1000: LR=9.05e-03, Loss=4.46344e-01 LER=1.584e-01
***Loss=4.46344e-01
Epoch 3 Train Time 17.20837426185608s

Model Saved
Training epoch 4, Batch 500/1000: LR=7.94e-03, Loss=4.34019e-01 LER=1.541e-01
***Loss=4.34019e-01
Training epoch 4, Batch 1000/1000: LR=7.94e-03, Loss=4.31366e-01 LER=1.531e-01
***Loss=4.31366e-01
Epoch 4 Train Time 18.7623770236969s

Model Saved
Training epoch 5, Batch 500/1000: LR=6.55e-03, Loss=4.42577e-01 LER=1.574e-01
***Loss=4.42577e-01
Training epoch 5, Batch 1000/1000: LR=6.55e-03, Loss=4.39891e-01 LER=1.568e-01
***Loss=4.39891e-01
Epoch 5 Train Time 17.240586042404175s

Training epoch 6, Batch 500/1000: LR=5.00e-03, Loss=4.31807e-01 LER=1.518e-01
***Loss=4.31807e-01
Training epoch 6, Batch 1000/1000: LR=5.00e-03, Loss=4.28700e-01 LER=1.517e-01
***Loss=4.28700e-01
Epoch 6 Train Time 16.527313470840454s

Model Saved
Training epoch 7, Batch 500/1000: LR=3.46e-03, Loss=4.27508e-01 LER=1.503e-01
***Loss=4.27508e-01
Training epoch 7, Batch 1000/1000: LR=3.46e-03, Loss=4.23574e-01 LER=1.490e-01
***Loss=4.23574e-01
Epoch 7 Train Time 18.526973962783813s

Model Saved
Training epoch 8, Batch 500/1000: LR=2.06e-03, Loss=4.29207e-01 LER=1.526e-01
***Loss=4.29207e-01
Training epoch 8, Batch 1000/1000: LR=2.06e-03, Loss=4.21677e-01 LER=1.506e-01
***Loss=4.21677e-01
Epoch 8 Train Time 19.541577577590942s

Model Saved
Training epoch 9, Batch 500/1000: LR=9.56e-04, Loss=4.19518e-01 LER=1.492e-01
***Loss=4.19518e-01
Training epoch 9, Batch 1000/1000: LR=9.56e-04, Loss=4.20292e-01 LER=1.492e-01
***Loss=4.20292e-01
Epoch 9 Train Time 17.241086959838867s

Model Saved
Training epoch 10, Batch 500/1000: LR=2.46e-04, Loss=4.20261e-01 LER=1.507e-01
***Loss=4.20261e-01
Training epoch 10, Batch 1000/1000: LR=2.46e-04, Loss=4.19021e-01 LER=1.490e-01
***Loss=4.19021e-01
Epoch 10 Train Time 16.448069095611572s

Model Saved
Test LER  p=7.00e-02: 1.15e-01 p=8.00e-02: 1.31e-01 p=9.00e-02: 1.49e-01 p=1.00e-01: 1.67e-01 p=1.10e-01: 1.86e-01
Mean LER = 1.495e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 176.59142208099365 s

Best model loaded
Test LER  p=7.00e-02: 1.16e-01 p=8.00e-02: 1.30e-01 p=9.00e-02: 1.51e-01 p=1.00e-01: 1.70e-01 p=1.10e-01: 1.86e-01
Mean LER = 1.504e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 179.98876309394836 s

