Path to model/logs: Final_Results_QECCT/surface/FFNN_Code_L_3/noise_model_independent/repetition_1/10_11_2025_21_57_23
Namespace(epochs=200, workers=4, lr=0.01, gpus='0', batch_size=128, test_batch_size=512, seed=42, code_type='surface', code_L=3, repetitions=1, noise_type='independent', N_dec=6, d_model=128, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x752def58f940>, path='Final_Results_QECCT/surface/FFNN_Code_L_3/noise_model_independent/repetition_1/10_11_2025_21_57_23')
사용 가능한 GPU가 없어 CPU를 사용합니다.
PC matrix shape torch.Size([8, 18])
ECC_Transformer(
  (fc1): Linear(in_features=8, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=4, bias=True)
  (activation): Sigmoid()
  (criterion): CrossEntropyLoss()
)
# of Parameters: 1668
Training epoch 1, Batch 500/1000: LR=1.00e-02, Loss=6.93564e-01 LER=2.156e-01
***Loss=6.93564e-01
Training epoch 1, Batch 1000/1000: LR=1.00e-02, Loss=6.02316e-01 LER=1.973e-01
***Loss=6.02316e-01
Epoch 1 Train Time 14.404738664627075s

Model Saved
Training epoch 2, Batch 500/1000: LR=1.00e-02, Loss=4.67423e-01 LER=1.717e-01
***Loss=4.67423e-01
Training epoch 2, Batch 1000/1000: LR=1.00e-02, Loss=4.57192e-01 LER=1.646e-01
***Loss=4.57192e-01
Epoch 2 Train Time 12.906280517578125s

Model Saved
Training epoch 3, Batch 500/1000: LR=1.00e-02, Loss=4.35961e-01 LER=1.551e-01
***Loss=4.35961e-01
Training epoch 3, Batch 1000/1000: LR=1.00e-02, Loss=4.36624e-01 LER=1.556e-01
***Loss=4.36624e-01
Epoch 3 Train Time 11.921016216278076s

Model Saved
Training epoch 4, Batch 500/1000: LR=9.99e-03, Loss=4.25764e-01 LER=1.519e-01
***Loss=4.25764e-01
Training epoch 4, Batch 1000/1000: LR=9.99e-03, Loss=4.27493e-01 LER=1.525e-01
***Loss=4.27493e-01
Epoch 4 Train Time 19.579041719436646s

Model Saved
Training epoch 5, Batch 500/1000: LR=9.99e-03, Loss=4.35952e-01 LER=1.552e-01
***Loss=4.35952e-01
Training epoch 5, Batch 1000/1000: LR=9.99e-03, Loss=4.33038e-01 LER=1.529e-01
***Loss=4.33038e-01
Epoch 5 Train Time 17.367284774780273s

Training epoch 6, Batch 500/1000: LR=9.98e-03, Loss=4.26371e-01 LER=1.495e-01
***Loss=4.26371e-01
Training epoch 6, Batch 1000/1000: LR=9.98e-03, Loss=4.25369e-01 LER=1.497e-01
***Loss=4.25369e-01
Epoch 6 Train Time 12.151430130004883s

Model Saved
Training epoch 7, Batch 500/1000: LR=9.98e-03, Loss=4.22221e-01 LER=1.479e-01
***Loss=4.22221e-01
Training epoch 7, Batch 1000/1000: LR=9.98e-03, Loss=4.26204e-01 LER=1.499e-01
***Loss=4.26204e-01
Epoch 7 Train Time 13.852423191070557s

Training epoch 8, Batch 500/1000: LR=9.97e-03, Loss=4.21395e-01 LER=1.489e-01
***Loss=4.21395e-01
Training epoch 8, Batch 1000/1000: LR=9.97e-03, Loss=4.24357e-01 LER=1.497e-01
***Loss=4.24357e-01
Epoch 8 Train Time 11.575532913208008s

Model Saved
Training epoch 9, Batch 500/1000: LR=9.96e-03, Loss=4.20718e-01 LER=1.484e-01
***Loss=4.20718e-01
Training epoch 9, Batch 1000/1000: LR=9.96e-03, Loss=4.19736e-01 LER=1.490e-01
***Loss=4.19736e-01
Epoch 9 Train Time 11.545804977416992s

Model Saved
Training epoch 10, Batch 500/1000: LR=9.95e-03, Loss=4.20331e-01 LER=1.486e-01
***Loss=4.20331e-01
Training epoch 10, Batch 1000/1000: LR=9.95e-03, Loss=4.24116e-01 LER=1.497e-01
***Loss=4.24116e-01
Epoch 10 Train Time 16.019618034362793s

Test LER  p=7.00e-02: 1.14e-01 p=8.00e-02: 1.31e-01 p=9.00e-02: 1.49e-01 p=1.00e-01: 1.66e-01 p=1.10e-01: 1.85e-01
Mean LER = 1.490e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 378.797997713089 s

Training epoch 11, Batch 500/1000: LR=9.94e-03, Loss=4.24360e-01 LER=1.496e-01
***Loss=4.24360e-01
Training epoch 11, Batch 1000/1000: LR=9.94e-03, Loss=4.25537e-01 LER=1.499e-01
***Loss=4.25537e-01
Epoch 11 Train Time 45.322402238845825s

Training epoch 12, Batch 500/1000: LR=9.93e-03, Loss=4.29582e-01 LER=1.528e-01
***Loss=4.29582e-01
Training epoch 12, Batch 1000/1000: LR=9.93e-03, Loss=4.27043e-01 LER=1.513e-01
***Loss=4.27043e-01
Epoch 12 Train Time 41.26695370674133s

Training epoch 13, Batch 500/1000: LR=9.91e-03, Loss=4.17555e-01 LER=1.470e-01
***Loss=4.17555e-01
Training epoch 13, Batch 1000/1000: LR=9.91e-03, Loss=4.18054e-01 LER=1.470e-01
***Loss=4.18054e-01
Epoch 13 Train Time 45.80282783508301s

Model Saved
Training epoch 14, Batch 500/1000: LR=9.90e-03, Loss=4.19669e-01 LER=1.472e-01
***Loss=4.19669e-01
Training epoch 14, Batch 1000/1000: LR=9.90e-03, Loss=4.19576e-01 LER=1.479e-01
***Loss=4.19576e-01
Epoch 14 Train Time 41.14833474159241s

Training epoch 15, Batch 500/1000: LR=9.88e-03, Loss=4.20225e-01 LER=1.491e-01
***Loss=4.20225e-01
Training epoch 15, Batch 1000/1000: LR=9.88e-03, Loss=4.22115e-01 LER=1.499e-01
***Loss=4.22115e-01
Epoch 15 Train Time 38.80268216133118s

Training epoch 16, Batch 500/1000: LR=9.86e-03, Loss=4.21501e-01 LER=1.477e-01
***Loss=4.21501e-01
Training epoch 16, Batch 1000/1000: LR=9.86e-03, Loss=4.20822e-01 LER=1.484e-01
***Loss=4.20822e-01
Epoch 16 Train Time 40.6834933757782s

Training epoch 17, Batch 500/1000: LR=9.84e-03, Loss=4.22568e-01 LER=1.510e-01
***Loss=4.22568e-01
Training epoch 17, Batch 1000/1000: LR=9.84e-03, Loss=4.23871e-01 LER=1.508e-01
***Loss=4.23871e-01
Epoch 17 Train Time 43.721940994262695s

Training epoch 18, Batch 500/1000: LR=9.82e-03, Loss=4.21095e-01 LER=1.501e-01
***Loss=4.21095e-01
Training epoch 18, Batch 1000/1000: LR=9.82e-03, Loss=4.22291e-01 LER=1.497e-01
***Loss=4.22291e-01
Epoch 18 Train Time 38.960185289382935s

Training epoch 19, Batch 500/1000: LR=9.80e-03, Loss=4.21972e-01 LER=1.481e-01
***Loss=4.21972e-01
Training epoch 19, Batch 1000/1000: LR=9.80e-03, Loss=4.20846e-01 LER=1.482e-01
***Loss=4.20846e-01
Epoch 19 Train Time 41.38768291473389s

Training epoch 20, Batch 500/1000: LR=9.78e-03, Loss=4.22805e-01 LER=1.500e-01
***Loss=4.22805e-01
Training epoch 20, Batch 1000/1000: LR=9.78e-03, Loss=4.23657e-01 LER=1.498e-01
***Loss=4.23657e-01
Epoch 20 Train Time 39.44643235206604s

Test LER  p=7.00e-02: 1.14e-01 p=8.00e-02: 1.28e-01 p=9.00e-02: 1.50e-01 p=1.00e-01: 1.68e-01 p=1.10e-01: 1.84e-01
Mean LER = 1.490e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 240.50391578674316 s

Training epoch 21, Batch 500/1000: LR=9.76e-03, Loss=4.17776e-01 LER=1.484e-01
***Loss=4.17776e-01
Training epoch 21, Batch 1000/1000: LR=9.76e-03, Loss=4.18209e-01 LER=1.473e-01
***Loss=4.18209e-01
Epoch 21 Train Time 18.913784503936768s

Training epoch 22, Batch 500/1000: LR=9.73e-03, Loss=4.20581e-01 LER=1.483e-01
***Loss=4.20581e-01
Training epoch 22, Batch 1000/1000: LR=9.73e-03, Loss=4.20866e-01 LER=1.487e-01
***Loss=4.20866e-01
Epoch 22 Train Time 16.376312255859375s

Training epoch 23, Batch 500/1000: LR=9.70e-03, Loss=4.22391e-01 LER=1.499e-01
***Loss=4.22391e-01
Training epoch 23, Batch 1000/1000: LR=9.70e-03, Loss=4.19153e-01 LER=1.486e-01
***Loss=4.19153e-01
Epoch 23 Train Time 17.31386709213257s

Training epoch 24, Batch 500/1000: LR=9.68e-03, Loss=4.18396e-01 LER=1.482e-01
***Loss=4.18396e-01
Training epoch 24, Batch 1000/1000: LR=9.68e-03, Loss=4.22785e-01 LER=1.500e-01
***Loss=4.22785e-01
Epoch 24 Train Time 17.77824091911316s

Training epoch 25, Batch 500/1000: LR=9.65e-03, Loss=4.19545e-01 LER=1.479e-01
***Loss=4.19545e-01
Training epoch 25, Batch 1000/1000: LR=9.65e-03, Loss=4.20583e-01 LER=1.492e-01
***Loss=4.20583e-01
Epoch 25 Train Time 18.3341805934906s

Training epoch 26, Batch 500/1000: LR=9.62e-03, Loss=4.17846e-01 LER=1.489e-01
***Loss=4.17846e-01
Training epoch 26, Batch 1000/1000: LR=9.62e-03, Loss=4.18815e-01 LER=1.485e-01
***Loss=4.18815e-01
Epoch 26 Train Time 24.05699634552002s

Training epoch 27, Batch 500/1000: LR=9.59e-03, Loss=4.13745e-01 LER=1.457e-01
***Loss=4.13745e-01
Training epoch 27, Batch 1000/1000: LR=9.59e-03, Loss=4.17399e-01 LER=1.474e-01
***Loss=4.17399e-01
Epoch 27 Train Time 39.47425556182861s

Model Saved
Training epoch 28, Batch 500/1000: LR=9.56e-03, Loss=4.15737e-01 LER=1.484e-01
***Loss=4.15737e-01
Training epoch 28, Batch 1000/1000: LR=9.56e-03, Loss=4.19141e-01 LER=1.491e-01
***Loss=4.19141e-01
Epoch 28 Train Time 43.11119365692139s

Training epoch 29, Batch 500/1000: LR=9.52e-03, Loss=4.21711e-01 LER=1.507e-01
***Loss=4.21711e-01
Training epoch 29, Batch 1000/1000: LR=9.52e-03, Loss=4.20376e-01 LER=1.497e-01
***Loss=4.20376e-01
Epoch 29 Train Time 43.72101712226868s

Training epoch 30, Batch 500/1000: LR=9.49e-03, Loss=4.14663e-01 LER=1.462e-01
***Loss=4.14663e-01
Training epoch 30, Batch 1000/1000: LR=9.49e-03, Loss=4.16011e-01 LER=1.468e-01
***Loss=4.16011e-01
Epoch 30 Train Time 43.679784059524536s

Model Saved
Test LER  p=7.00e-02: 1.14e-01 p=8.00e-02: 1.31e-01 p=9.00e-02: 1.51e-01 p=1.00e-01: 1.68e-01 p=1.10e-01: 1.87e-01
Mean LER = 1.500e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 359.33977603912354 s

Training epoch 31, Batch 500/1000: LR=9.46e-03, Loss=4.15308e-01 LER=1.473e-01
***Loss=4.15308e-01
Training epoch 31, Batch 1000/1000: LR=9.46e-03, Loss=4.18030e-01 LER=1.483e-01
***Loss=4.18030e-01
Epoch 31 Train Time 38.85607409477234s

Training epoch 32, Batch 500/1000: LR=9.42e-03, Loss=4.15241e-01 LER=1.469e-01
***Loss=4.15241e-01
Training epoch 32, Batch 1000/1000: LR=9.42e-03, Loss=4.15153e-01 LER=1.467e-01
***Loss=4.15153e-01
Epoch 32 Train Time 39.42910671234131s

Model Saved
Training epoch 33, Batch 500/1000: LR=9.38e-03, Loss=4.18591e-01 LER=1.492e-01
***Loss=4.18591e-01
Training epoch 33, Batch 1000/1000: LR=9.38e-03, Loss=4.18100e-01 LER=1.494e-01
***Loss=4.18100e-01
Epoch 33 Train Time 38.9755380153656s

Training epoch 34, Batch 500/1000: LR=9.34e-03, Loss=4.19437e-01 LER=1.491e-01
***Loss=4.19437e-01
Training epoch 34, Batch 1000/1000: LR=9.34e-03, Loss=4.20607e-01 LER=1.497e-01
***Loss=4.20607e-01
Epoch 34 Train Time 38.80848979949951s

Training epoch 35, Batch 500/1000: LR=9.30e-03, Loss=4.16815e-01 LER=1.480e-01
***Loss=4.16815e-01
Training epoch 35, Batch 1000/1000: LR=9.30e-03, Loss=4.18203e-01 LER=1.480e-01
***Loss=4.18203e-01
Epoch 35 Train Time 37.49862599372864s

Training epoch 36, Batch 500/1000: LR=9.26e-03, Loss=4.23051e-01 LER=1.500e-01
***Loss=4.23051e-01
Training epoch 36, Batch 1000/1000: LR=9.26e-03, Loss=4.19295e-01 LER=1.491e-01
***Loss=4.19295e-01
Epoch 36 Train Time 39.218626976013184s

Training epoch 37, Batch 500/1000: LR=9.22e-03, Loss=4.18073e-01 LER=1.482e-01
***Loss=4.18073e-01
Training epoch 37, Batch 1000/1000: LR=9.22e-03, Loss=4.20096e-01 LER=1.490e-01
***Loss=4.20096e-01
Epoch 37 Train Time 46.63972020149231s

Training epoch 38, Batch 500/1000: LR=9.18e-03, Loss=4.18675e-01 LER=1.487e-01
***Loss=4.18675e-01
Training epoch 38, Batch 1000/1000: LR=9.18e-03, Loss=4.20301e-01 LER=1.497e-01
***Loss=4.20301e-01
Epoch 38 Train Time 39.89778280258179s

Training epoch 39, Batch 500/1000: LR=9.14e-03, Loss=4.16843e-01 LER=1.474e-01
***Loss=4.16843e-01
Training epoch 39, Batch 1000/1000: LR=9.14e-03, Loss=4.17246e-01 LER=1.478e-01
***Loss=4.17246e-01
Epoch 39 Train Time 39.70211410522461s

Training epoch 40, Batch 500/1000: LR=9.09e-03, Loss=4.20233e-01 LER=1.490e-01
***Loss=4.20233e-01
Training epoch 40, Batch 1000/1000: LR=9.09e-03, Loss=4.19850e-01 LER=1.493e-01
***Loss=4.19850e-01
Epoch 40 Train Time 40.90961790084839s

Test LER  p=7.00e-02: 1.13e-01 p=8.00e-02: 1.32e-01 p=9.00e-02: 1.49e-01 p=1.00e-01: 1.68e-01 p=1.10e-01: 1.88e-01
Mean LER = 1.499e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 166.2951877117157 s

Training epoch 41, Batch 500/1000: LR=9.05e-03, Loss=4.18632e-01 LER=1.494e-01
***Loss=4.18632e-01
Training epoch 41, Batch 1000/1000: LR=9.05e-03, Loss=4.19078e-01 LER=1.493e-01
***Loss=4.19078e-01
Epoch 41 Train Time 15.090261697769165s

Training epoch 42, Batch 500/1000: LR=9.00e-03, Loss=4.13925e-01 LER=1.474e-01
***Loss=4.13925e-01
Training epoch 42, Batch 1000/1000: LR=9.00e-03, Loss=4.15575e-01 LER=1.466e-01
***Loss=4.15575e-01
Epoch 42 Train Time 26.274160623550415s

Training epoch 43, Batch 500/1000: LR=8.95e-03, Loss=4.19225e-01 LER=1.475e-01
***Loss=4.19225e-01
Training epoch 43, Batch 1000/1000: LR=8.95e-03, Loss=4.19123e-01 LER=1.479e-01
***Loss=4.19123e-01
Epoch 43 Train Time 38.80034255981445s

Training epoch 44, Batch 500/1000: LR=8.90e-03, Loss=4.18968e-01 LER=1.503e-01
***Loss=4.18968e-01
Training epoch 44, Batch 1000/1000: LR=8.90e-03, Loss=4.18932e-01 LER=1.500e-01
***Loss=4.18932e-01
Epoch 44 Train Time 37.697697162628174s

Training epoch 45, Batch 500/1000: LR=8.85e-03, Loss=4.18912e-01 LER=1.499e-01
***Loss=4.18912e-01
Training epoch 45, Batch 1000/1000: LR=8.85e-03, Loss=4.15061e-01 LER=1.481e-01
***Loss=4.15061e-01
Epoch 45 Train Time 39.60689902305603s

Model Saved
Training epoch 46, Batch 500/1000: LR=8.80e-03, Loss=4.21330e-01 LER=1.505e-01
***Loss=4.21330e-01
Training epoch 46, Batch 1000/1000: LR=8.80e-03, Loss=4.19938e-01 LER=1.491e-01
***Loss=4.19938e-01
Epoch 46 Train Time 41.168567419052124s

Training epoch 47, Batch 500/1000: LR=8.75e-03, Loss=4.21593e-01 LER=1.508e-01
***Loss=4.21593e-01
Training epoch 47, Batch 1000/1000: LR=8.75e-03, Loss=4.21710e-01 LER=1.505e-01
***Loss=4.21710e-01
Epoch 47 Train Time 39.470584869384766s

Training epoch 48, Batch 500/1000: LR=8.70e-03, Loss=4.18742e-01 LER=1.497e-01
***Loss=4.18742e-01
Training epoch 48, Batch 1000/1000: LR=8.70e-03, Loss=4.19376e-01 LER=1.491e-01
***Loss=4.19376e-01
Epoch 48 Train Time 37.52491998672485s

Training epoch 49, Batch 500/1000: LR=8.64e-03, Loss=4.23343e-01 LER=1.507e-01
***Loss=4.23343e-01
Training epoch 49, Batch 1000/1000: LR=8.64e-03, Loss=4.17869e-01 LER=1.496e-01
***Loss=4.17869e-01
Epoch 49 Train Time 38.13827848434448s

Training epoch 50, Batch 500/1000: LR=8.59e-03, Loss=4.16601e-01 LER=1.494e-01
***Loss=4.16601e-01
Training epoch 50, Batch 1000/1000: LR=8.59e-03, Loss=4.14732e-01 LER=1.483e-01
***Loss=4.14732e-01
Epoch 50 Train Time 38.34713053703308s

Model Saved
Test LER  p=7.00e-02: 1.12e-01 p=8.00e-02: 1.31e-01 p=9.00e-02: 1.49e-01 p=1.00e-01: 1.67e-01 p=1.10e-01: 1.83e-01
Mean LER = 1.486e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 326.9477400779724 s

Training epoch 51, Batch 500/1000: LR=8.54e-03, Loss=4.21445e-01 LER=1.498e-01
***Loss=4.21445e-01
Training epoch 51, Batch 1000/1000: LR=8.54e-03, Loss=4.18466e-01 LER=1.487e-01
***Loss=4.18466e-01
Epoch 51 Train Time 34.82807207107544s

