Path to model/logs: Final_Results_QECCT/surface/Transformer_Code_L_3/noise_model_independent/repetition_1/06_11_2025_11_40_15
Namespace(epochs=10, workers=4, lr=0.01, gpus='0', batch_size=64, test_batch_size=512, seed=42, code_type='surface', code_L=3, repetitions=1, noise_type='independent', N_dec=6, d_model=128, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x7fc21f6eb940>, path='Final_Results_QECCT/surface/Transformer_Code_L_3/noise_model_independent/repetition_1/06_11_2025_11_40_15')
Intel Arc GPU (XPU)를 사용합니다.
단일 Intel Arc GPU (XPU)를 사용합니다 (DataParallel 비활성화).
PC matrix shape torch.Size([8, 18])
ECC_Transformer(
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder): Encoder(
    (layers): ModuleList(
      (0-5): 6 x EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0-1): 2 x SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (input_embedding): Linear(in_features=8, out_features=128, bias=True)
  (output_classifier): Linear(in_features=128, out_features=4, bias=True)
  (criterion): CrossEntropyLoss()
)
# of Parameters: 1191812
Training epoch 1, Batch 500/1000: LR=1.00e-02, Loss=1.22851e+00 LER=4.874e-01
***Loss=1.22851e+00
Training epoch 1, Batch 1000/1000: LR=1.00e-02, Loss=1.03317e+00 LER=3.963e-01
***Loss=1.03317e+00
Epoch 1 Train Time 67.08679866790771s

Model Saved
Training epoch 2, Batch 500/1000: LR=9.76e-03, Loss=6.22246e-01 LER=1.899e-01
***Loss=6.22246e-01
Training epoch 2, Batch 1000/1000: LR=9.76e-03, Loss=6.09891e-01 LER=1.889e-01
***Loss=6.09891e-01
Epoch 2 Train Time 84.12131142616272s

Model Saved
Training epoch 3, Batch 500/1000: LR=9.05e-03, Loss=5.82737e-01 LER=1.869e-01
***Loss=5.82737e-01
Training epoch 3, Batch 1000/1000: LR=9.05e-03, Loss=5.74382e-01 LER=1.817e-01
***Loss=5.74382e-01
Epoch 3 Train Time 73.00755572319031s

Model Saved
Training epoch 4, Batch 500/1000: LR=7.94e-03, Loss=5.36248e-01 LER=1.716e-01
***Loss=5.36248e-01
Training epoch 4, Batch 1000/1000: LR=7.94e-03, Loss=5.26096e-01 LER=1.693e-01
***Loss=5.26096e-01
Epoch 4 Train Time 70.89782404899597s

Model Saved
Training epoch 5, Batch 500/1000: LR=6.55e-03, Loss=4.95063e-01 LER=1.612e-01
***Loss=4.95063e-01
Training epoch 5, Batch 1000/1000: LR=6.55e-03, Loss=4.88305e-01 LER=1.617e-01
***Loss=4.88305e-01
Epoch 5 Train Time 72.021968126297s

Model Saved
Training epoch 6, Batch 500/1000: LR=5.00e-03, Loss=4.88272e-01 LER=1.653e-01
***Loss=4.88272e-01
Training epoch 6, Batch 1000/1000: LR=5.00e-03, Loss=5.00271e-01 LER=1.685e-01
***Loss=5.00271e-01
Epoch 6 Train Time 78.25036764144897s

Training epoch 7, Batch 500/1000: LR=3.46e-03, Loss=4.87980e-01 LER=1.625e-01
***Loss=4.87980e-01
Training epoch 7, Batch 1000/1000: LR=3.46e-03, Loss=4.89052e-01 LER=1.661e-01
***Loss=4.89052e-01
Epoch 7 Train Time 73.63176012039185s

Training epoch 8, Batch 500/1000: LR=2.06e-03, Loss=4.52262e-01 LER=1.571e-01
***Loss=4.52262e-01
Training epoch 8, Batch 1000/1000: LR=2.06e-03, Loss=4.58218e-01 LER=1.602e-01
***Loss=4.58218e-01
Epoch 8 Train Time 71.3265151977539s

Model Saved
Training epoch 9, Batch 500/1000: LR=9.56e-04, Loss=4.50299e-01 LER=1.563e-01
***Loss=4.50299e-01
Training epoch 9, Batch 1000/1000: LR=9.56e-04, Loss=4.46261e-01 LER=1.551e-01
***Loss=4.46261e-01
Epoch 9 Train Time 72.31445598602295s

Model Saved
Training epoch 10, Batch 500/1000: LR=2.46e-04, Loss=4.47138e-01 LER=1.565e-01
***Loss=4.47138e-01
Training epoch 10, Batch 1000/1000: LR=2.46e-04, Loss=4.46394e-01 LER=1.560e-01
***Loss=4.46394e-01
Epoch 10 Train Time 72.60263061523438s

Test LER  p=7.00e-02: 1.15e-01 p=8.00e-02: 1.34e-01 p=9.00e-02: 1.55e-01 p=1.00e-01: 1.70e-01 p=1.10e-01: 1.88e-01
Mean LER = 1.527e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 192.242014169693 s

Best model loaded
Test LER  p=7.00e-02: 1.18e-01 p=8.00e-02: 1.35e-01 p=9.00e-02: 1.53e-01 p=1.00e-01: 1.72e-01 p=1.10e-01: 1.89e-01
Mean LER = 1.532e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 185.05095648765564 s

