Path to model/logs: Final_Results_QECCT/surface/Transformer_Code_L_3/noise_model_independent/repetition_1/06_11_2025_12_20_47
Namespace(epochs=200, workers=4, lr=0.01, gpus='0', batch_size=128, test_batch_size=512, seed=42, code_type='surface', code_L=3, repetitions=1, noise_type='independent', N_dec=6, d_model=128, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x70ad8bd9a410>, path='Final_Results_QECCT/surface/Transformer_Code_L_3/noise_model_independent/repetition_1/06_11_2025_12_20_47')
사용 가능한 GPU가 없어 CPU를 사용합니다.
PC matrix shape torch.Size([8, 18])
ECC_Transformer(
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder): Encoder(
    (layers): ModuleList(
      (0-5): 6 x EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0-1): 2 x SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (input_embedding): Linear(in_features=8, out_features=128, bias=True)
  (output_classifier): Linear(in_features=128, out_features=4, bias=True)
  (criterion): CrossEntropyLoss()
)
# of Parameters: 1191812
Training epoch 1, Batch 500/1000: LR=1.00e-02, Loss=1.21488e+00 LER=4.888e-01
***Loss=1.21488e+00
Training epoch 1, Batch 1000/1000: LR=1.00e-02, Loss=1.04981e+00 LER=4.088e-01
***Loss=1.04981e+00
Epoch 1 Train Time 38.85066032409668s

Model Saved
Training epoch 2, Batch 500/1000: LR=1.00e-02, Loss=5.74703e-01 LER=1.792e-01
***Loss=5.74703e-01
Training epoch 2, Batch 1000/1000: LR=1.00e-02, Loss=5.55750e-01 LER=1.774e-01
***Loss=5.55750e-01
Epoch 2 Train Time 47.20878028869629s

Model Saved
Training epoch 3, Batch 500/1000: LR=1.00e-02, Loss=5.19924e-01 LER=1.762e-01
***Loss=5.19924e-01
Training epoch 3, Batch 1000/1000: LR=1.00e-02, Loss=5.10404e-01 LER=1.723e-01
***Loss=5.10404e-01
Epoch 3 Train Time 44.52433204650879s

Model Saved
Training epoch 4, Batch 500/1000: LR=9.99e-03, Loss=4.91268e-01 LER=1.675e-01
***Loss=4.91268e-01
Training epoch 4, Batch 1000/1000: LR=9.99e-03, Loss=4.89182e-01 LER=1.683e-01
***Loss=4.89182e-01
Epoch 4 Train Time 38.21529841423035s

Model Saved
Training epoch 5, Batch 500/1000: LR=9.99e-03, Loss=4.78012e-01 LER=1.663e-01
***Loss=4.78012e-01
Training epoch 5, Batch 1000/1000: LR=9.99e-03, Loss=4.71083e-01 LER=1.652e-01
***Loss=4.71083e-01
Epoch 5 Train Time 44.24186086654663s

Model Saved
Training epoch 6, Batch 500/1000: LR=9.98e-03, Loss=4.67320e-01 LER=1.627e-01
***Loss=4.67320e-01
Training epoch 6, Batch 1000/1000: LR=9.98e-03, Loss=4.63879e-01 LER=1.633e-01
***Loss=4.63879e-01
Epoch 6 Train Time 53.48629808425903s

Model Saved
Training epoch 7, Batch 500/1000: LR=9.98e-03, Loss=4.53423e-01 LER=1.610e-01
***Loss=4.53423e-01
Training epoch 7, Batch 1000/1000: LR=9.98e-03, Loss=4.50649e-01 LER=1.597e-01
***Loss=4.50649e-01
Epoch 7 Train Time 42.95609426498413s

Model Saved
