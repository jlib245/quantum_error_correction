Path to model/logs: Final_Results_QECCT/surface/Transformer_Code_L_3/noise_model_independent/repetition_1/10_11_2025_21_59_39
Namespace(epochs=200, workers=4, lr=0.01, gpus='0', batch_size=128, test_batch_size=512, seed=42, code_type='surface', code_L=3, repetitions=1, noise_type='independent', N_dec=6, d_model=128, h=16, lambda_loss_ber=0.3, lambda_loss_ler=1.0, lambda_loss_n_pred=0.3, lambda_loss_log_pred=1, no_g=0, no_mask=0, code=<__main__.Code object at 0x7caefc29a380>, path='Final_Results_QECCT/surface/Transformer_Code_L_3/noise_model_independent/repetition_1/10_11_2025_21_59_39')
PC matrix shape torch.Size([8, 18])
ECC_Transformer(
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder): Encoder(
    (layers): ModuleList(
      (0-5): 6 x EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=128, out_features=512, bias=True)
          (w_2): Linear(in_features=512, out_features=128, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0-1): 2 x SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (input_embedding): Linear(in_features=8, out_features=128, bias=True)
  (output_classifier): Linear(in_features=128, out_features=4, bias=True)
  (criterion): CrossEntropyLoss()
)
# of Parameters: 1191812
Training epoch 1, Batch 500/1000: LR=1.00e-02, Loss=1.21488e+00 LER=4.888e-01
***Loss=1.21488e+00
Training epoch 1, Batch 1000/1000: LR=1.00e-02, Loss=1.04981e+00 LER=4.088e-01
***Loss=1.04981e+00
Epoch 1 Train Time 77.51234412193298s

Model Saved
Training epoch 2, Batch 500/1000: LR=1.00e-02, Loss=5.83968e-01 LER=1.843e-01
***Loss=5.83968e-01
Training epoch 2, Batch 1000/1000: LR=1.00e-02, Loss=5.69319e-01 LER=1.816e-01
***Loss=5.69319e-01
Epoch 2 Train Time 49.320778369903564s

Model Saved
Training epoch 3, Batch 500/1000: LR=1.00e-02, Loss=5.14975e-01 LER=1.734e-01
***Loss=5.14975e-01
Training epoch 3, Batch 1000/1000: LR=1.00e-02, Loss=5.03543e-01 LER=1.696e-01
***Loss=5.03543e-01
Epoch 3 Train Time 84.74362349510193s

Model Saved
Training epoch 4, Batch 500/1000: LR=9.99e-03, Loss=4.79014e-01 LER=1.634e-01
***Loss=4.79014e-01
Training epoch 4, Batch 1000/1000: LR=9.99e-03, Loss=4.77516e-01 LER=1.640e-01
***Loss=4.77516e-01
Epoch 4 Train Time 61.79269623756409s

Model Saved
Training epoch 5, Batch 500/1000: LR=9.99e-03, Loss=4.60519e-01 LER=1.583e-01
***Loss=4.60519e-01
Training epoch 5, Batch 1000/1000: LR=9.99e-03, Loss=4.62455e-01 LER=1.605e-01
***Loss=4.62455e-01
Epoch 5 Train Time 44.63370943069458s

Model Saved
Training epoch 6, Batch 500/1000: LR=9.98e-03, Loss=4.59701e-01 LER=1.611e-01
***Loss=4.59701e-01
Training epoch 6, Batch 1000/1000: LR=9.98e-03, Loss=4.55191e-01 LER=1.605e-01
***Loss=4.55191e-01
Epoch 6 Train Time 47.221964836120605s

Model Saved
Training epoch 7, Batch 500/1000: LR=9.98e-03, Loss=4.48938e-01 LER=1.600e-01
***Loss=4.48938e-01
Training epoch 7, Batch 1000/1000: LR=9.98e-03, Loss=4.45733e-01 LER=1.576e-01
***Loss=4.45733e-01
Epoch 7 Train Time 167.4196469783783s

Model Saved
Training epoch 8, Batch 500/1000: LR=9.97e-03, Loss=4.37696e-01 LER=1.559e-01
***Loss=4.37696e-01
Training epoch 8, Batch 1000/1000: LR=9.97e-03, Loss=4.36879e-01 LER=1.538e-01
***Loss=4.36879e-01
Epoch 8 Train Time 222.03268027305603s

Model Saved
Training epoch 9, Batch 500/1000: LR=9.96e-03, Loss=4.30882e-01 LER=1.521e-01
***Loss=4.30882e-01
Training epoch 9, Batch 1000/1000: LR=9.96e-03, Loss=4.30744e-01 LER=1.522e-01
***Loss=4.30744e-01
Epoch 9 Train Time 84.1599760055542s

Model Saved
Training epoch 10, Batch 500/1000: LR=9.95e-03, Loss=4.37334e-01 LER=1.551e-01
***Loss=4.37334e-01
Training epoch 10, Batch 1000/1000: LR=9.95e-03, Loss=4.34116e-01 LER=1.536e-01
***Loss=4.34116e-01
Epoch 10 Train Time 63.378220558166504s

Test LER  p=7.00e-02: 1.15e-01 p=8.00e-02: 1.32e-01 p=9.00e-02: 1.55e-01 p=1.00e-01: 1.70e-01 p=1.10e-01: 1.86e-01
Mean LER = 1.518e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 256.19731307029724 s

Training epoch 11, Batch 500/1000: LR=9.94e-03, Loss=4.33370e-01 LER=1.540e-01
***Loss=4.33370e-01
Training epoch 11, Batch 1000/1000: LR=9.94e-03, Loss=4.32098e-01 LER=1.536e-01
***Loss=4.32098e-01
Epoch 11 Train Time 176.29175567626953s

Training epoch 12, Batch 500/1000: LR=9.93e-03, Loss=4.33600e-01 LER=1.546e-01
***Loss=4.33600e-01
Training epoch 12, Batch 1000/1000: LR=9.93e-03, Loss=4.33877e-01 LER=1.538e-01
***Loss=4.33877e-01
Epoch 12 Train Time 66.61867260932922s

Training epoch 13, Batch 500/1000: LR=9.91e-03, Loss=4.22809e-01 LER=1.500e-01
***Loss=4.22809e-01
Training epoch 13, Batch 1000/1000: LR=9.91e-03, Loss=4.21347e-01 LER=1.494e-01
***Loss=4.21347e-01
Epoch 13 Train Time 48.45167946815491s

Model Saved
Training epoch 14, Batch 500/1000: LR=9.90e-03, Loss=4.25668e-01 LER=1.493e-01
***Loss=4.25668e-01
Training epoch 14, Batch 1000/1000: LR=9.90e-03, Loss=4.25956e-01 LER=1.500e-01
***Loss=4.25956e-01
Epoch 14 Train Time 49.29300093650818s

Training epoch 15, Batch 500/1000: LR=9.88e-03, Loss=4.21511e-01 LER=1.486e-01
***Loss=4.21511e-01
Training epoch 15, Batch 1000/1000: LR=9.88e-03, Loss=4.23279e-01 LER=1.496e-01
***Loss=4.23279e-01
Epoch 15 Train Time 46.675618171691895s

Training epoch 16, Batch 500/1000: LR=9.86e-03, Loss=4.31984e-01 LER=1.533e-01
***Loss=4.31984e-01
Training epoch 16, Batch 1000/1000: LR=9.86e-03, Loss=4.29546e-01 LER=1.519e-01
***Loss=4.29546e-01
Epoch 16 Train Time 54.93057322502136s

Training epoch 17, Batch 500/1000: LR=9.84e-03, Loss=4.27676e-01 LER=1.516e-01
***Loss=4.27676e-01
Training epoch 17, Batch 1000/1000: LR=9.84e-03, Loss=4.28818e-01 LER=1.514e-01
***Loss=4.28818e-01
Epoch 17 Train Time 49.44563961029053s

Training epoch 18, Batch 500/1000: LR=9.82e-03, Loss=4.22294e-01 LER=1.497e-01
***Loss=4.22294e-01
Training epoch 18, Batch 1000/1000: LR=9.82e-03, Loss=4.23589e-01 LER=1.503e-01
***Loss=4.23589e-01
Epoch 18 Train Time 96.37674283981323s

Training epoch 19, Batch 500/1000: LR=9.80e-03, Loss=4.27020e-01 LER=1.521e-01
***Loss=4.27020e-01
Training epoch 19, Batch 1000/1000: LR=9.80e-03, Loss=4.24293e-01 LER=1.502e-01
***Loss=4.24293e-01
Epoch 19 Train Time 160.0748529434204s

Training epoch 20, Batch 500/1000: LR=9.78e-03, Loss=4.19883e-01 LER=1.499e-01
***Loss=4.19883e-01
Training epoch 20, Batch 1000/1000: LR=9.78e-03, Loss=4.19550e-01 LER=1.498e-01
***Loss=4.19550e-01
Epoch 20 Train Time 177.86239528656006s

Model Saved
Test LER  p=7.00e-02: 1.14e-01 p=8.00e-02: 1.31e-01 p=9.00e-02: 1.48e-01 p=1.00e-01: 1.68e-01 p=1.10e-01: 1.85e-01
Mean LER = 1.494e-01
# of testing samples: [100352.0, 100352.0, 100352.0, 100352.0, 100352.0]
 Test Time 187.06144857406616 s

Training epoch 21, Batch 500/1000: LR=9.76e-03, Loss=4.24288e-01 LER=1.508e-01
***Loss=4.24288e-01
Training epoch 21, Batch 1000/1000: LR=9.76e-03, Loss=4.19610e-01 LER=1.489e-01
***Loss=4.19610e-01
Epoch 21 Train Time 159.8670699596405s

Training epoch 22, Batch 500/1000: LR=9.73e-03, Loss=4.22257e-01 LER=1.503e-01
***Loss=4.22257e-01
Training epoch 22, Batch 1000/1000: LR=9.73e-03, Loss=4.24245e-01 LER=1.509e-01
***Loss=4.24245e-01
Epoch 22 Train Time 148.1142520904541s

Training epoch 23, Batch 500/1000: LR=9.70e-03, Loss=4.20988e-01 LER=1.481e-01
***Loss=4.20988e-01
Training epoch 23, Batch 1000/1000: LR=9.70e-03, Loss=4.18991e-01 LER=1.491e-01
***Loss=4.18991e-01
Epoch 23 Train Time 61.42927813529968s

Model Saved
Training epoch 24, Batch 500/1000: LR=9.68e-03, Loss=4.17905e-01 LER=1.479e-01
***Loss=4.17905e-01
Training epoch 24, Batch 1000/1000: LR=9.68e-03, Loss=4.19340e-01 LER=1.488e-01
***Loss=4.19340e-01
Epoch 24 Train Time 46.77248668670654s

Training epoch 25, Batch 500/1000: LR=9.65e-03, Loss=4.15612e-01 LER=1.467e-01
***Loss=4.15612e-01
Training epoch 25, Batch 1000/1000: LR=9.65e-03, Loss=4.21302e-01 LER=1.498e-01
***Loss=4.21302e-01
Epoch 25 Train Time 46.42960047721863s

Training epoch 26, Batch 500/1000: LR=9.62e-03, Loss=4.19423e-01 LER=1.492e-01
***Loss=4.19423e-01
Training epoch 26, Batch 1000/1000: LR=9.62e-03, Loss=4.19252e-01 LER=1.492e-01
***Loss=4.19252e-01
Epoch 26 Train Time 48.242703437805176s

Training epoch 27, Batch 500/1000: LR=9.59e-03, Loss=4.25278e-01 LER=1.522e-01
***Loss=4.25278e-01
Training epoch 27, Batch 1000/1000: LR=9.59e-03, Loss=4.18681e-01 LER=1.494e-01
***Loss=4.18681e-01
Epoch 27 Train Time 46.261144399642944s

Model Saved
Training epoch 28, Batch 500/1000: LR=9.56e-03, Loss=4.23493e-01 LER=1.512e-01
***Loss=4.23493e-01
Training epoch 28, Batch 1000/1000: LR=9.56e-03, Loss=4.21164e-01 LER=1.501e-01
***Loss=4.21164e-01
Epoch 28 Train Time 45.13242721557617s

Training epoch 29, Batch 500/1000: LR=9.52e-03, Loss=4.19854e-01 LER=1.496e-01
***Loss=4.19854e-01
Training epoch 29, Batch 1000/1000: LR=9.52e-03, Loss=4.17618e-01 LER=1.488e-01
***Loss=4.17618e-01
Epoch 29 Train Time 46.68962097167969s

Model Saved
